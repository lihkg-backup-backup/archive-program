{"tid":257808,"cid":18,"subCid":0,"title":"簡單了解AlphaGo 同今日既AI","createTime":"2017-05-25T14:06:38.000Z","updateTime":"2018-09-14T12:44:36.000Z","uid":58358,"like":615,"dislike":13,"uniUserReply":0,"replies":[{"pid":"cda8464a4ffa6b73d1639d9b1f14a2af3aedb47d","tid":257808,"uid":58358,"like":11,"dislike":1,"score":10,"citedBy":0,"replyTime":"2017-05-25T14:06:38.000Z","msg":"AI 發展至今日因為AlphaGo 既出現而被視為有重大突破，而呢種AI 既核心係一種叫Convolutional Neural Network (CNN) 既技術。<br />\n<br />\n由於見到唔少人對今時今日既AI 有誤解，所以想係呢度用好淺白方式同大家好簡單咁介紹下。<br />\n<br />\n待續<br />\n<br />\n利申：一間IT 公司既一名前員工"},{"pid":"4313e6b4d985fc1525741e345f3a2f1737ff3e0b","tid":257808,"uid":53794,"like":1,"dislike":15,"score":-14,"citedBy":0,"replyTime":"2017-05-25T14:19:15.000Z","msg":"前員工？咁依家做緊乜"},{"pid":"1e650078bbd874953aa956c0b21a47f5e92d6d1e","tid":257808,"uid":51307,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T14:29:58.000Z","msg":"lm"},{"pid":"e92fb07d2e7f3cd0504e596d3ca0a451f39abf55","tid":257808,"uid":58358,"like":1,"dislike":1,"score":0,"citedBy":0,"replyTime":"2017-05-25T14:38:04.000Z","msg":"AlphaGo 之父，Deepmind 始創人 Demis Hassabiss 話佢地所研發既係叫做 Artificial General Intelligence (AGI)，以此與之前既 AI 作區分。佢話研發 AGI 既目的係希望最終可以用黎解決世上所有既問題。<br />\n<br />\n所以，AlphaGo 係一種 AGI，而 CNN 就係 AGI 既核心。<br />\n<br />\n咁咩係CNN？首先了解下人腦係點運作先 ..."},{"pid":"4f193363bb70f7039eba34ed1ca233f569da71ac","tid":257808,"uid":36724,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T14:40:52.000Z","msg":"正評等學野"},{"pid":"04790cd7b95ce38187ed49e52e6403b500cf5a63","tid":257808,"uid":46452,"like":7,"dislike":302,"score":-295,"citedBy":0,"replyTime":"2017-05-25T14:42:12.000Z","msg":"<blockquote>AI 發展至今日因為AlphaGo 既出現而被視為有重大突破，而呢種AI 既核心係一種叫Convolutional Neural Network (CNN) 既技術。<br />\n<br />\n由於見到唔少人對今時今日既AI 有誤解，所以想係呢度用好淺白方式同大家好簡單咁介紹下。<br />\n<br />\n待續<br />\n<br />\n利申：一間IT 公司既一名前員工</blockquote><br />\n收皮啦屌你老母"},{"pid":"22dc9f4372ecbcca261275a6c91b097a2c614490","tid":257808,"uid":21699,"like":5,"dislike":0,"score":5,"citedBy":0,"replyTime":"2017-05-25T14:44:53.000Z","msg":"<blockquote><blockquote>AI 發展至今日因為AlphaGo 既出現而被視為有重大突破，而呢種AI 既核心係一種叫Convolutional Neural Network (CNN) 既技術。<br />\n<br />\n由於見到唔少人對今時今日既AI 有誤解，所以想係呢度用好淺白方式同大家好簡單咁介紹下。<br />\n<br />\n待續<br />\n<br />\n利申：一間IT 公司既一名前員工</blockquote><br />\n收皮啦屌你老母</blockquote><br />\n<img src=\"/assets/faces/normal/wonder.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/wonder.gif\" class=\"hkgmoji\" />你做咩？<br />\n<br />\n樓主 go on <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"93e6a2c9303a76a7dc8f9cb3b218db1205923cbb","tid":257808,"uid":29474,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T14:58:47.000Z","msg":"<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" />"},{"pid":"a6270fa8198455ab5b2a5d60749c77a186692613","tid":257808,"uid":103983,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T14:59:17.000Z","msg":"留名<img src=\"/assets/faces/normal/hoho.gif\" class=\"hkgmoji\" />"},{"pid":"37d6aa8d8e906535e3964411d01114f15122c7cc","tid":257808,"uid":36445,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:15:15.000Z","msg":"成日覺得呢d研究好熱血 <br />\nlm"},{"pid":"c4650fbfa95d50f2abd4547957ca40401d4c2838","tid":257808,"uid":66654,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:30:35.000Z","msg":"留名"},{"pid":"f6398d7f5606085552153e8607aa9c52cbcbc544","tid":257808,"uid":52522,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:34:53.000Z","msg":"留明"},{"pid":"e3a0d49b6533a6303a3d85e59743efe8a6a68937","tid":257808,"uid":58358,"like":2,"dislike":0,"score":2,"citedBy":0,"replyTime":"2017-05-25T15:36:34.000Z","msg":"大家見到腦部個啲彎彎曲曲既野，放大唔知幾多倍黎睇，其實就係一個個神經元(Neuron)細胞。<br />\n<br />\n其中一種神經元既樣<br />\n<img src=\"http://doctorsandhu.com/Neuron/images/image04.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fdoctorsandhu.com%2FNeuron%2Fimages%2Fimage04.png&h=1ca65a3f&s={SIZE}\" /><br />\n<br />\n腦部之所以可以運作（思考，呼吸，睇野，聽野等等），基於神經元既基本功能：接收訊號，同發出訊號（神經脈衝 / Synapse）。而人腦平均有1000億個神經元。佢地組成唔同既網絡（Neural Network)，每分每秒於不同網絡之間都有訊號傳遞。<br />\n<br />\n一個神經元是否發出訊號，要發出幾強既訊號，要視乎佢所接收之訊號既強弱是否高過該神經元既門檻(Threshold) 。<br />\n<br />\n假設一粒神經元 S 既門檻係 T, 而 S 所接收之訊號係 N：如果 N &gt; T, 該神經元就會根據本身既一啲物質(Neurotransmitter) 調節 N 既值(增強或減弱) 然後傳畀其他神經元。<br />\n<br />\nBTW 如果 Neurotransmitter 出現問題，就算 N &gt; T，S 都傳唔到訊號出去，好多時個啲同腦神經有關既病都係因為 Neurotransmitter 出現問題。<br />\n<br />\n待續 ..."},{"pid":"a7aeecb8c040a297a06a2c70833ec1e526048d0b","tid":257808,"uid":63126,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:36:44.000Z","msg":"lm"},{"pid":"0f820a701c05de40397d9d7a607616ecee9fd41d","tid":257808,"uid":78571,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:38:44.000Z","msg":"lm"},{"pid":"0363edacf6946d064f0d0b985d60d798215009b8","tid":257808,"uid":90072,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:38:53.000Z","msg":"Lm"},{"pid":"f5fbf40265a8f348abadb4d8d21adb779a98adda","tid":257808,"uid":54733,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:39:39.000Z","msg":"lm"},{"pid":"29ed1f85fb7643ca48436b3c2d5c9790bb2c9018","tid":257808,"uid":11208,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:41:56.000Z","msg":"樓主可唔可以解釋下CNN 同GIST既分別？<br />\n利申：岩岩做完份machine learning coursework, 要分辨張相係manmade object 定係non-manmade object<br />\n<br />\n用過SVM做 Accuracy得71%<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" />"},{"pid":"330a6be44577e15edf793e479d00eaf51b4aef67","tid":257808,"uid":10080,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:44:09.000Z","msg":"難得真學術post,樓主加油"},{"pid":"b85cf2352da131a9fe77bff63a48aec9019837ed","tid":257808,"uid":43825,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:46:39.000Z","msg":"正經post<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"cd15e1857f583bf2a04a7c9b9979e5fee3962d19","tid":257808,"uid":3657,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:51:24.000Z","msg":"<blockquote>樓主可唔可以解釋下CNN 同GIST既分別？<br />\n利申：岩岩做完份machine learning coursework, 要分辨張相係manmade object 定係non-manmade object<br />\n<br />\n用過SVM做 Accuracy得71%<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\nSvm 做呢d 一定唔work"},{"pid":"805c9a6e740516da35e91710a8152f556713f2c9","tid":257808,"uid":10908,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:51:37.000Z","msg":"留名<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"6fc8d16818fcaad42d01cf5a8f60dc84949a36ad","tid":257808,"uid":51321,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:52:23.000Z","msg":"lm"},{"pid":"3731fae4b38aca83976817462c70403a05418221","tid":257808,"uid":45208,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:54:49.000Z","msg":"前排留名<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"6a7143e1ae5f75c27a3bd9e324c3a2b5a4e638a5","tid":257808,"uid":26610,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:55:18.000Z","msg":"留名學野"},{"pid":"d6fcc821458becf8ed68a3b64e4aa05b5ab83259","tid":257808,"uid":63846,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:57:01.000Z","msg":"<blockquote>樓主可唔可以解釋下CNN 同GIST既分別？<br />\n利申：岩岩做完份machine learning coursework, 要分辨張相係manmade object 定係non-manmade object<br />\n<br />\n用過SVM做 Accuracy得71%<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /></blockquote><br />\nwtf is GIST<br />\n<br />\n夠data, 五分鐘用CNN做到90%+ accuracy <img src=\"/assets/faces/normal/bye.gif\" class=\"hkgmoji\" /> <br />\n<br />\n你svm用咩feature, linear svm?"},{"pid":"2467cd60c7dcf3f397c54e201164d0512bd9bdb1","tid":257808,"uid":56976,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:58:11.000Z","msg":"<img src=\"/assets/faces/normal/smile.gif\" class=\"hkgmoji\" />"},{"pid":"cba03a8b113fb82d8ad25cb58243290fa4fec7ab","tid":257808,"uid":56976,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:58:39.000Z","msg":"<img src=\"/assets/faces/normal/smile.gif\" class=\"hkgmoji\" />"},{"pid":"5d22183cd1d9a4c6f6de89e799347de5d2d7af83","tid":257808,"uid":86117,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:59:32.000Z","msg":"咁快live"},{"pid":"20929333306447771dad611c93684a145978a856","tid":257808,"uid":96826,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:59:38.000Z","msg":"lm"},{"pid":"b0ce439d42e17c2eb8b2f427b3fc0ead5bc0ab15","tid":257808,"uid":41684,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:59:45.000Z","msg":"嘩屌好似上bio咁"},{"pid":"def3495f311b1842b1c7f395a78d0ffa4b388d5f","tid":257808,"uid":59898,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:59:49.000Z","msg":"<img src=\"/assets/faces/normal/surprise.gif\" class=\"hkgmoji\" />"},{"pid":"9ef56fc4f9a82be23a152833e77953c8a070128e","tid":257808,"uid":63846,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T15:59:57.000Z","msg":"有冇network architecture可以係手機gpu run 到60fps<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" />"},{"pid":"062be348c50a4b6545b5b0e522d1686e1d1299a6","tid":257808,"uid":57088,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:00:20.000Z","msg":"lm 學野"},{"pid":"c400966b0326936abbeda99f822637f3ac9aaa99","tid":257808,"uid":61210,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:02:19.000Z","msg":"<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"197bee4fda810529978c230e9a0328e3ac53cddd","tid":257808,"uid":28229,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:03:11.000Z","msg":"lm"},{"pid":"90916a35574558f9c0b039046d22c0e6160eab8a","tid":257808,"uid":57911,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:03:33.000Z","msg":"lm"},{"pid":"d4e75201d9d9da55a83b77d7f9f17946de266132","tid":257808,"uid":12337,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:07:27.000Z","msg":"Lau ming"},{"pid":"e8d5ac0c7d9f0de68d18e1840117eb7aab58d8f5","tid":257808,"uid":14470,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:07:42.000Z","msg":"快"},{"pid":"edd5e0782787374a41fccfbf690ec2ae6b96982e","tid":257808,"uid":31696,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:08:09.000Z","msg":"留名 皮已正<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"e969e04cd0d1e2cd912f0b702c2b50ada92975a3","tid":257808,"uid":69140,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:08:17.000Z","msg":"屌快"},{"pid":"cc61500320f7e2e68df2f55aedd0c7d15b54ffbc","tid":257808,"uid":10373,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:08:40.000Z","msg":"lm"},{"pid":"5c3efc74f0510262e5071d892e2feb5c33adf4e1","tid":257808,"uid":63846,"like":2,"dislike":3,"score":-1,"citedBy":0,"replyTime":"2017-05-25T16:09:06.000Z","msg":"<blockquote>AlphaGo 之父，Deepmind 始創人 Demis Hassabiss 話佢地所研發既係叫做 Artificial General Intelligence (AGI)，以此與之前既 AI 作區分。佢話研發 AGI 既目的係希望最終可以用黎解決世上所有既問題。<br />\n<br />\n所以，<span style=\"color: red;\">AlphaGo 係一種 AGI，而 CNN 就係 AGI 既核心</span>。<br />\n<br />\n咁咩係CNN？首先了解下人腦係點運作先 ...</blockquote><br />\n此post已完<img src=\"/assets/faces/normal/bye.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/bye.gif\" class=\"hkgmoji\" />"},{"pid":"ae4f73d0486f73d4f97b34f1ca1c01ba05aec133","tid":257808,"uid":58358,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:11:19.000Z","msg":"我地見到一個人既樣既舜間，無意中某啲神經網絡就會有所活動，而當中所牽涉既神經元都有機會修改自身對所接收訊號之強弱既調節。即係話 S 對 N 既加減唔一定係每次都一樣，有機會變。呢個性質引致到如果我地每日都見一次同一個人，久而久之訊號既傳遞係呢啲網絡之間就會形成一種 Pattern，即是我地既記憶。<br />\n<br />\n今日寫住咁多先 ..."},{"pid":"c413b750589e9321f1752e76218628a2e31fa840","tid":257808,"uid":89931,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:13:30.000Z","msg":"lm"},{"pid":"1879b33633066012ce1dc1456599886b7a96cb2d","tid":257808,"uid":114040,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:14:01.000Z","msg":"LM"},{"pid":"8304d4e2503a3c72e919801e405c83bdb3964d35","tid":257808,"uid":58358,"like":3,"dislike":1,"score":2,"citedBy":0,"replyTime":"2017-05-25T16:15:57.000Z","msg":"<blockquote><blockquote>AlphaGo 之父，Deepmind 始創人 Demis Hassabiss 話佢地所研發既係叫做 Artificial General Intelligence (AGI)，以此與之前既 AI 作區分。佢話研發 AGI 既目的係希望最終可以用黎解決世上所有既問題。<br />\n<br />\n所以，<span style=\"color: red;\">AlphaGo 係一種 AGI，而 CNN 就係 AGI 既核心</span>。<br />\n<br />\n咁咩係CNN？首先了解下人腦係點運作先 ...</blockquote><br />\n此post已完<img src=\"/assets/faces/normal/bye.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/bye.gif\" class=\"hkgmoji\" /></blockquote><br />\nsor, 查返資料，AlphaGo 未係 AGI，應該話係作為研發 AGI 既基礎<br />\n多謝指正<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"4075146f31606428d8441977b3249300c19bf606","tid":257808,"uid":66054,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:16:41.000Z","msg":"lm"},{"pid":"67a40386237209f01d931e6a444c4072d8d2fe12","tid":257808,"uid":30830,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:17:18.000Z","msg":"留名"},{"pid":"b9fde701c4cfd56ae89f99e14670dee7cc8ffa0d","tid":257808,"uid":11417,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:17:36.000Z","msg":"此回覆已被刪除"},{"pid":"d48d12e7bdd7f03214881fa1256fe620da26fb4f","tid":257808,"uid":19174,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:18:15.000Z","msg":"高汁po留名"},{"pid":"a3b98aca92cb9f91fc003e9a30fa8cd708d67092","tid":257808,"uid":73741,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:20:46.000Z","msg":"LM 小學雞死撚開"},{"pid":"7e526eb7bd008f7c7e6474ae65c4d9e82b260d90","tid":257808,"uid":99583,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:21:10.000Z","msg":"Lm"},{"pid":"7a91445b9045f81131d9dcba019b62896422611b","tid":257808,"uid":87860,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:22:34.000Z","msg":"此回覆已被刪除"},{"pid":"aa713c12651c3c55afeb246df6234099512606c9","tid":257808,"uid":58358,"like":2,"dislike":1,"score":1,"citedBy":0,"replyTime":"2017-05-25T16:23:01.000Z","msg":"<blockquote><blockquote><blockquote>AlphaGo 之父，Deepmind 始創人 Demis Hassabiss 話佢地所研發既係叫做 Artificial General Intelligence (AGI)，以此與之前既 AI 作區分。佢話研發 AGI 既目的係希望最終可以用黎解決世上所有既問題。<br />\n<br />\n所以，<span style=\"color: red;\">AlphaGo 係一種 AGI，而 CNN 就係 AGI 既核心</span>。<br />\n<br />\n咁咩係CNN？首先了解下人腦係點運作先 ...</blockquote><br />\n此post已完<img src=\"/assets/faces/normal/bye.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/bye.gif\" class=\"hkgmoji\" /></blockquote><br />\nsor, 查返資料，AlphaGo 未係 AGI，應該話係作為研發 AGI 既基礎<br />\n多謝指正<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\nBTW 大家請放心，我係最後會列出參考資料<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"90f4bdb61bcce3640d317d7ce98594c28d5f297d","tid":257808,"uid":22131,"like":7,"dislike":2,"score":5,"citedBy":0,"replyTime":"2017-05-25T16:25:12.000Z","msg":"見今日d alpha go post真係覆都廢事<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> <br />\n一時話咩人類同架賽車鬥快實輸<br />\n一時就咩alphago會自我進化殺人類<br />\n<br />\n真係難得有高質post<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"5194e2aac9bc4bd9713a31cd688f12465de1fb49","tid":257808,"uid":64889,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:30:52.000Z","msg":"lm"},{"pid":"002b6ce225a017be625079b4be65f39b76515d68","tid":257808,"uid":116688,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:33:01.000Z","msg":"留精"},{"pid":"484d95232e3cd485ca4308b64f32f5e1e710dfef","tid":257808,"uid":16827,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-25T16:33:35.000Z","msg":"樓主你寫多d，難得高質post，唔想睇得兩句又要等一日，咁大家好快cool down 左個感覺...依家個感覺好似連序都未完咁，唔好話正文<img src=\"/assets/faces/normal/cry.gif\" class=\"hkgmoji\" />"},{"pid":"189923a2ac78c14fed2b1e349eaedd5adca4e421","tid":257808,"uid":110933,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:34:49.000Z","msg":"lm<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"6859fce239d1fa3a367a32286a102488565fed8d","tid":257808,"uid":60376,"like":2,"dislike":1,"score":1,"citedBy":0,"replyTime":"2017-05-25T16:36:15.000Z","msg":"<blockquote>見今日d alpha go post真係覆都廢事<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> <br />\n一時話咩人類同架賽車鬥快實輸<br />\n一時就咩alphago會自我進化殺人類<br />\n<br />\n真係難得有高質post<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n係 真係好痛苦<br />\n<br />\n見住佢地討論alphago仲focus係計算速度同用database記晒所有步數真係血都嘔埋出黎<br />\n<br />\n唔好話呢d CNN RNN, 我諗各種regression呢度都唔知有幾多人識。。。"},{"pid":"afe8bfb506dcfbca92ebb5645f9aa212880ac115","tid":257808,"uid":8336,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:39:32.000Z","msg":"留名"},{"pid":"d41aea06a50f189fc6722c870af895ac83f816e5","tid":257808,"uid":19979,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:39:40.000Z","msg":"lm"},{"pid":"c6492d50b5c79ac5108c2c15d1810a80abb4b09a","tid":257808,"uid":87860,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:41:49.000Z","msg":"此回覆已被刪除"},{"pid":"aa2a3a8f317839aafc38f878cc5b263a06cd4293","tid":257808,"uid":15199,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-25T16:42:11.000Z","msg":"<blockquote><blockquote>AlphaGo 之父，Deepmind 始創人 Demis Hassabiss 話佢地所研發既係叫做 Artificial General Intelligence (AGI)，以此與之前既 AI 作區分。佢話研發 AGI 既目的係希望最終可以用黎解決世上所有既問題。<br />\n<br />\n所以，<span style=\"color: red;\">AlphaGo 係一種 AGI，而 CNN 就係 AGI 既核心</span>。<br />\n<br />\n咁咩係CNN？首先了解下人腦係點運作先 ...</blockquote><br />\n此post已完<img src=\"/assets/faces/normal/bye.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/bye.gif\" class=\"hkgmoji\" /></blockquote><br />\n<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"c8e72ed9b2a7bab785f5843d033635269478fe2b","tid":257808,"uid":92110,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:43:06.000Z","msg":"LM"},{"pid":"7fd1528e2503438012d07f956f3a8c0e32359ad0","tid":257808,"uid":93793,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:46:46.000Z","msg":"正皮加留皮<br />\n諗起ex machina"},{"pid":"f0f5190fe5827e68a73a1682a5d4bd4f03198594","tid":257808,"uid":11447,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:49:37.000Z","msg":"LM"},{"pid":"5e3841e092176a68d4b74a9b4805c7399d222af6","tid":257808,"uid":114193,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:49:53.000Z","msg":"<blockquote>LM</blockquote><br />\nLm"},{"pid":"c5dbd604b4a75ff9f8631deb5d543325d700b907","tid":257808,"uid":48679,"like":10,"dislike":0,"score":10,"citedBy":0,"replyTime":"2017-05-25T16:51:53.000Z","msg":"<blockquote><blockquote>見今日d alpha go post真係覆都廢事<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> <br />\n一時話咩人類同架賽車鬥快實輸<br />\n一時就咩alphago會自我進化殺人類<br />\n<br />\n真係難得有高質post<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n係 真係好痛苦<br />\n<br />\n見住佢地討論alphago仲focus係計算速度同用database記晒所有步數真係血都嘔埋出黎<br />\n<br />\n唔好話呢d CNN RNN, 我諗各種regression呢度都唔知有幾多人識。。。</blockquote><br />\n明白左呢個世界係蠢人佔大多數就唔會咁激氣"},{"pid":"6342da90fe92bb8ac91c6cfe2e7bb45eacb7b907","tid":257808,"uid":49722,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T16:53:20.000Z","msg":"lm"},{"pid":"7ddf6579e57b2fba160943011baaf7fdb77ae21e","tid":257808,"uid":13309,"like":2,"dislike":1,"score":1,"citedBy":0,"replyTime":"2017-05-25T16:56:56.000Z","msg":"<blockquote><blockquote><blockquote>見今日d alpha go post真係覆都廢事<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> <br />\n一時話咩人類同架賽車鬥快實輸<br />\n一時就咩alphago會自我進化殺人類<br />\n<br />\n真係難得有高質post<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n係 真係好痛苦<br />\n<br />\n見住佢地討論alphago仲focus係計算速度同用database記晒所有步數真係血都嘔埋出黎<br />\n<br />\n唔好話呢d CNN RNN, 我諗各種regression呢度都唔知有幾多人識。。。</blockquote><br />\n明白左呢個世界係蠢人佔大多數就唔會咁激氣</blockquote><br />\n<br />\n唔識又唔虛心學習懶叻先煩"},{"pid":"3ae1f9b9fb307d16e61cae69b72ef457d0d2e3c6","tid":257808,"uid":33843,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T17:02:22.000Z","msg":"lm"},{"pid":"4f73b217dfc9a5a2e71548b9464f8e992729efc0","tid":257808,"uid":102554,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T17:05:35.000Z","msg":"lm"},{"pid":"5b67ca2ce261f6dbe5c772ba715f706706948419","tid":257808,"uid":6386,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T17:06:03.000Z","msg":"Lm"},{"pid":"6cdeeed5091845b50ca575aa691e11ba4f996fdf","tid":257808,"uid":96303,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T17:07:08.000Z","msg":"lm"},{"pid":"f548380c978d062bef74b003ae3a3fd45c56b687","tid":257808,"uid":78443,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T17:07:27.000Z","msg":"Lau ming"},{"pid":"bcfd2470f67251dd318bbf52f178760e84b0a13d","tid":257808,"uid":62935,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T17:08:09.000Z","msg":"快d"},{"pid":"f1799bb659cb19787a2269c30f3c9ef22eea5dbc","tid":257808,"uid":67201,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T17:08:43.000Z","msg":"推爆佢"},{"pid":"6032c9a5197d87b0b37205e6741798ffe01d6b55","tid":257808,"uid":2507,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T17:12:01.000Z","msg":"留名推一推<img src=\"/assets/faces/normal/hoho.gif\" class=\"hkgmoji\" />"},{"pid":"32d903b2f3f123cd2ea684acaeb83a87df26686c","tid":257808,"uid":39552,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T17:15:20.000Z","msg":"lm"},{"pid":"71ed150bfaf28c68e155cf85497d3a2c07148f25","tid":257808,"uid":7237,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T17:17:12.000Z","msg":"留名學嘢<br />\n樓主講嘅嘢係咪類似genetic algorithm?<br />\n利申冇讀ict，純粹有興趣"},{"pid":"40baddc6c4e32b0ccf4fab45339214863f3e5229","tid":257808,"uid":48724,"like":3,"dislike":0,"score":3,"citedBy":0,"replyTime":"2017-05-25T17:17:38.000Z","msg":"此回覆已被刪除"},{"pid":"00a04c84c22bc885d6e74aaf66a675a1ce3705cf","tid":257808,"uid":10317,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T17:18:40.000Z","msg":"Lm"},{"pid":"b4f884063f994116fe0222937b2705464ab8a2e4","tid":257808,"uid":48724,"like":2,"dislike":0,"score":2,"citedBy":0,"replyTime":"2017-05-25T17:24:47.000Z","msg":"此回覆已被刪除"},{"pid":"a766b81c0e8f44dea71cc89c4bf6562f2c5ed9b0","tid":257808,"uid":34301,"like":1,"dislike":6,"score":-5,"citedBy":0,"replyTime":"2017-05-25T17:35:54.000Z","msg":"樓主介唔介意用英文寫<br />\n你啲中文有少少難明<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"988e0ad958fdeac18067564862bc169462152d57","tid":257808,"uid":59933,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T17:36:21.000Z","msg":"lm"},{"pid":"a601c99183e93f174906537b540d218c30ddadef","tid":257808,"uid":40614,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T17:37:19.000Z","msg":"lm"},{"pid":"523fe543a5ef7067b920a23f067c218e80863a09","tid":257808,"uid":45897,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T17:37:22.000Z","msg":"高質post"},{"pid":"baafc56cd4ab3083f1b9c903e4741bb3eb0f7013","tid":257808,"uid":25598,"like":0,"dislike":1,"score":-1,"citedBy":0,"replyTime":"2017-05-25T17:39:12.000Z","msg":"<blockquote>AlphaGo 之父，Deepmind 始創人 Demis Hassabiss 話佢地所研發既係叫做 Artificial General Intelligence (AGI)，以此與之前既 AI 作區分。佢話研發 AGI 既目的係希望最終可以用黎解決世上所有既問題。<br />\n<br />\n所以，AlphaGo 係一種 AGI，而 CNN 就係 AGI 既核心。<br />\n<br />\n咁咩係CNN？首先了解下人腦係點運作先 ...</blockquote><br />\n人類咪世界最大嘅問題<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"300d2c28e1a50d19785442f979226b72289b8ff5","tid":257808,"uid":63846,"like":0,"dislike":1,"score":-1,"citedBy":0,"replyTime":"2017-05-25T17:39:57.000Z","msg":"<blockquote><blockquote>我地見到一個人既樣既舜間，無意中某啲神經網絡就會有所活動，而當中所牽涉既神經元都有機會修改自身對所接收訊號之強弱既調節。即係話 S 對 N 既加減唔一定係每次都一樣，有機會變。呢個性質引致到如果我地每日都見一次同一個人，久而久之訊號既傳遞係呢啲網絡之間就會形成一種 Pattern，即是我地既記憶。<br />\n<br />\n今日寫住咁多先 ...</blockquote><br />\n一開始已經concept錯<img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /></blockquote><br />\nCognitive science experiments shows face recognition capability is hard-wire into human brain"},{"pid":"0c9440a12241411a3029aa74da221c070ab0cc66","tid":257808,"uid":10009,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-25T17:40:54.000Z","msg":"<blockquote><blockquote>見今日d alpha go post真係覆都廢事<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> <br />\n一時話咩人類同架賽車鬥快實輸<br />\n一時就咩alphago會自我進化殺人類<br />\n<br />\n真係難得有高質post<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n係 真係好痛苦<br />\n<br />\n見住佢地討論alphago仲focus係計算速度同用database記晒所有步數真係血都嘔埋出黎<br />\n<br />\n唔好話呢d CNN RNN, 我諗各種regression呢度都唔知有幾多人識。。。</blockquote><br />\n世上如果人人都好似你咁醒 你就無得撈啦<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"a15de974799363d4e2885890abac5c2ff1c8cce4","tid":257808,"uid":14717,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T17:41:53.000Z","msg":"<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"76af5cb68652279c454a9c8513f4e481f7790a0c","tid":257808,"uid":36520,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T17:43:24.000Z","msg":"LM"},{"pid":"0d4cd74017a7e72a23cc1de865eac4ba7a25849a","tid":257808,"uid":21623,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T17:43:25.000Z","msg":"LM<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"5db0c5bd454360f164cccb7b5887d243adac0358","tid":257808,"uid":28960,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T17:43:36.000Z","msg":"Lm"},{"pid":"977f5b125c797570024cc08b1a234ccd02c27c22","tid":257808,"uid":63053,"like":3,"dislike":0,"score":3,"citedBy":0,"replyTime":"2017-05-25T17:47:56.000Z","msg":"見100正評 入黎一睇<br />\nAlphaGo 係一種 AGI <img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> <br />\nbtw係都用perceptron黎講解啦<br />\n又S又T講到1999<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" />"},{"pid":"7fcbf95957fd625ca0f823dd63a2d77df8b14c52","tid":257808,"uid":45788,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T17:55:59.000Z","msg":"Lm學野"},{"pid":"d8c3cf9e839167b70a8541cb09ed525aac3ff1f7","tid":257808,"uid":5386,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T18:10:12.000Z","msg":"Lm瞓醒睇"},{"pid":"118ed2a49c2aec4a99cc4b90f0e8b67558faf5b0","tid":257808,"uid":110071,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T18:21:18.000Z","msg":"Lm"},{"pid":"89a39699be18e8e21b6a51563b41ea40b7c288f4","tid":257808,"uid":48796,"like":0,"dislike":1,"score":-1,"citedBy":0,"replyTime":"2017-05-25T18:25:57.000Z","msg":"以前都見過呢啲模擬人類神經網絡嘅程式嘅書（咦咩嚟，拎起，嘩好深，放返低），又聽過人話AI發展停滯不前，點解去到呢期突飛猛進？外星人通水？<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"db0af760ea6fc17c8bd04726bb1684aa59540ae0","tid":257808,"uid":58358,"like":2,"dislike":0,"score":2,"citedBy":0,"replyTime":"2017-05-25T18:26:10.000Z","msg":"<blockquote><blockquote>我地見到一個人既樣既舜間，無意中某啲神經網絡就會有所活動，而當中所牽涉既神經元都有機會修改自身對所接收訊號之強弱既調節。即係話 S 對 N 既加減唔一定係每次都一樣，有機會變。呢個性質引致到如果我地每日都見一次同一個人，久而久之訊號既傳遞係呢啲網絡之間就會形成一種 Pattern，即是我地既記憶。<br />\n<br />\n今日寫住咁多先 ...</blockquote><br />\n一開始已經concept錯<img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /></blockquote><br />\nthat is based on findings of Neuroscience<br />\n<br />\n&quot;When neuroscientists use functional magnetic resonance imaging to see how a monkey&rsquo;s brain responds to familiar faces, something odd happens. When shown a familiar face, a monkey&rsquo;s brain lights up, not in a specific area, but in nine different ones. <strong>Neuroscientists call these areas &ldquo;face patches&rdquo; and think they are neural networks with the specialised functions associated with face recognition.</strong>&quot;<br />\n<br />\n<a href=\"https://www.technologyreview.com/s/535176/human-face-recognition-found-in-neural-network-based-on-monkey-brains/\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.technologyreview.com%2Fs%2F535176%2Fhuman-face-recognition-found-in-neural-network-based-on-monkey-brains%2F&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=68e68ad2\" target=\"_blank\">https://www.technologyreview.com/s/535176/human-face-recognition-found-in-neural-network-based-on-monkey-brains/</a>"},{"pid":"a2786597f9b8c6074f960596eb5aa708b8eb8475","tid":257808,"uid":92017,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T18:35:03.000Z","msg":"可唔可以打多d一次過出<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n對AI個學習能力最有興趣<br />\nbtw, thx樓主<img src=\"/assets/faces/normal/hoho.gif\" class=\"hkgmoji\" />"},{"pid":"ca48c2022008df0d17e33b614fd8b3fad090361d","tid":257808,"uid":70989,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T18:37:46.000Z","msg":"對個post有少少失望 不過樓主可以繼續寫 支持你 <img src=\"/assets/faces/normal/hoho.gif\" class=\"hkgmoji\" /><br />\n<br />\n<br />\n利申：出年9月出國讀Machine Learning PhD"},{"pid":"d1ac8f277c22b7220c72dc1d2fbd95e215c8dcf4","tid":257808,"uid":22906,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T18:40:25.000Z","msg":"Lm"},{"pid":"64266e2d4baf4fb5cad5afc6847fe6018482206f","tid":257808,"uid":58358,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-25T18:43:09.000Z","msg":"好，我打多啲再出，但提提新user 可以用追故模式去睇<br />\n<br />\n同埋歡迎提出質疑，錯左我會認，否則我會提出更多理據或者參考資料去反駁，多謝指教先<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"a17ba4c00edbaa873a74619675252d15b6f0bc10","tid":257808,"uid":3646,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T18:50:34.000Z","msg":"多謝你至真<br />\n想認識多d社會既將來<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <br />\n&amp; LM"},{"pid":"52654759c9b6996e2f5f5d5bedc00768060832e3","tid":257808,"uid":34147,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T19:01:44.000Z","msg":"lm"},{"pid":"ea8753415e182eaf4def2e1119963a216be05648","tid":257808,"uid":111253,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T19:34:41.000Z","msg":"如果要由零複製一個模擬人腦，咪要逐個設定成1000億個擬神經位元"},{"pid":"22ca0f57ad39d2aace046234adec4869f038eb65","tid":257808,"uid":104474,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T19:40:35.000Z","msg":"lm"},{"pid":"a47b7d5c219d9b32f8cf71a4ba406d26b1387e45","tid":257808,"uid":7359,"like":0,"dislike":1,"score":-1,"citedBy":0,"replyTime":"2017-05-25T20:09:41.000Z","msg":"留名, 好pro 咁<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"79297a79d545623f1e2cc398dd1afc535eb1d355","tid":257808,"uid":60858,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T20:12:57.000Z","msg":"正呀，謝謝樓主<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"d5bd94e8af6492d7709a1a8897d1fb96a8a71770","tid":257808,"uid":105553,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T20:21:36.000Z","msg":"LM <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <br />\nMachine learning 一D都唔熟。"},{"pid":"aa63acd4f38e9d9e15b947d18710db2e42d16c5a","tid":257808,"uid":59516,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T20:23:16.000Z","msg":"留名學野"},{"pid":"0551343b22e305c4637ebbead54d409ab5150a95","tid":257808,"uid":55192,"like":15,"dislike":1,"score":14,"citedBy":0,"replyTime":"2017-05-25T20:28:03.000Z","msg":"留名<br />\n<br />\n喂樓上指出錯係可以<br />\n但紅字加句咩此post已完/留舊<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> 就算<br />\n真係冇乜建設性<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> <br />\n<br />\n可否都麻煩大家 指錯處都留低一兩句 錯咩、正確係咩？<br />\n<br />\n難得有認真學術post 唔好一野打沉人啦<br />\n喺連登開呢啲post好大壓力<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" />"},{"pid":"0d34d6fd9d11688859ff109a1024e36ba9d67562","tid":257808,"uid":58799,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T20:34:40.000Z","msg":"Lm"},{"pid":"656c9fc220ffe2c8f5861271a97e19f2220e5b2f","tid":257808,"uid":68583,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T20:42:05.000Z","msg":"高質post Lm"},{"pid":"e1aceaade2b7190407761e831daa8015c5f21904","tid":257808,"uid":51978,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T20:54:08.000Z","msg":"<blockquote>留名<br />\n<br />\n喂樓上指出錯係可以<br />\n但紅字加句咩此post已完/留舊<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> 就算<br />\n真係冇乜建設性<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> <br />\n<br />\n可否都麻煩大家 指錯處都留低一兩句 錯咩、正確係咩？<br />\n<br />\n難得有認真學術post 唔好一野打沉人啦<br />\n喺連登開呢啲post好大壓力<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /></blockquote>樓主幾好脾氣 <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"127de5fa6f0cd47ff62d7591f0567207c6fe53b5","tid":257808,"uid":11950,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T20:55:39.000Z","msg":"<blockquote><blockquote>留名<br />\n<br />\n喂樓上指出錯係可以<br />\n但紅字加句咩此post已完/留舊<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> 就算<br />\n真係冇乜建設性<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> <br />\n<br />\n可否都麻煩大家 指錯處都留低一兩句 錯咩、正確係咩？<br />\n<br />\n難得有認真學術post 唔好一野打沉人啦<br />\n喺連登開呢啲post好大壓力<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /></blockquote>樓主幾好脾氣 <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /></blockquote><br />\n加油<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"6ecbce5090e4c11e9c2e9ab15c6bc6f63155689f","tid":257808,"uid":36210,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T20:56:19.000Z","msg":"高質post lm<br />\n支持樓主<img src=\"/assets/faces/normal/hoho.gif\" class=\"hkgmoji\" />"},{"pid":"a1f3bf38cd1a892cadfca7d0f5e8c256f51ecb83","tid":257808,"uid":52477,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T20:57:53.000Z","msg":"lm"},{"pid":"677e763cf5d2f4777e6af4ba207a1dee744b95ea","tid":257808,"uid":40628,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T21:05:10.000Z","msg":"留名 <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"6b0577020a50c1d9c00495532eb3b929481663c5","tid":257808,"uid":43570,"like":0,"dislike":2,"score":-2,"citedBy":0,"replyTime":"2017-05-25T21:11:52.000Z","msg":"見樓主好有心寫，值得正評。但睇得出樓主強項並唔係neuroscience，可能會講多錯多，都係集中講番電腦野啦。<br />\n<br />\n利申：讀緊neuro 相關PhD"},{"pid":"fbc6916d754adc252acaf9ce79ed6c9c9d43f8b6","tid":257808,"uid":58909,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T21:13:27.000Z","msg":"lm"},{"pid":"6b90534ad8d71bb0bf0fcb7223e60922ab61dc21","tid":257808,"uid":16827,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T21:17:13.000Z","msg":"<blockquote>見樓主好有心寫，值得正評。但睇得出樓主強項並唔係neuroscience，可能會講多錯多，都係集中講番電腦野啦。<br />\n<br />\n利申：讀緊neuro 相關PhD</blockquote><br />\n請巴打指教下，可唔可以簡單講下咩叫neroscience<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"2536f562dc2cba166aae45ac3793c11e67b6bf30","tid":257808,"uid":50042,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T21:29:34.000Z","msg":"midterm冇考lambda，final會唔會考<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" />"},{"pid":"24551b5d87985f25e530c85503732cc3a39c3795","tid":257808,"uid":68583,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T21:36:19.000Z","msg":"我想了解一下 alpha go 係根據highest possibility to win去捉棋，咁同人腦係咪一樣？算唔算人工智能？ 定只係一台可以自我進行最高成功率行動既機器？"},{"pid":"47a021b40c2a9388f12cdd2be0838d34365b81c2","tid":257808,"uid":43570,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T21:36:28.000Z","msg":"<blockquote><blockquote>見樓主好有心寫，值得正評。但睇得出樓主強項並唔係neuroscience，可能會講多錯多，都係集中講番電腦野啦。<br />\n<br />\n利申：讀緊neuro 相關PhD</blockquote><br />\n請巴打指教下，可唔可以簡單講下咩叫neroscience<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\nneuroscience 就係研究神經系統嘅學科。細至神經細胞機理，大至大腦結構運作，都算係neuroscience 範圍。如果話心理學係研究心靈嘅軟件部份，咁neuroscience就係研究心靈嘅硬件部份。"},{"pid":"f0a2f1abaa12f6247ae1f2ecb3df5dd3a5a7b097","tid":257808,"uid":43570,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T21:55:25.000Z","msg":"<blockquote>我想了解一下 alpha go 係根據highest possibility to win去捉棋，咁同人腦係咪一樣？算唔算人工智能？ 定只係一台可以自我進行最高成功率行動既機器？</blockquote><br />\n多嘴代樓主答. 嚴格黎講所有人為製造出黎嘅智能系統都可能叫人工智能, 同佢用嘅方法係咪同人腦一樣冇關係. <br />\n<br />\n如果你問alpha go 係咪好似人咁思考, 係一台strong AI, 咁答案係否定的. 原因好簡單, 佢淨係識捉圍棋, 唔識其它."},{"pid":"2808b38b35368372f8a830ce9b87e7a13e93fc38","tid":257808,"uid":51906,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T21:57:18.000Z","msg":"lm"},{"pid":"cf187eebacdf72c17db0aa8b1955620d32dd09b4","tid":257808,"uid":70989,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-25T22:02:08.000Z","msg":"<blockquote>留名<br />\n<br />\n喂樓上指出錯係可以<br />\n但紅字加句咩此post已完/留舊<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> 就算<br />\n真係冇乜建設性<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> <br />\n<br />\n可否都麻煩大家 指錯處都留低一兩句 錯咩、正確係咩？<br />\n<br />\n難得有認真學術post 唔好一野打沉人啦<br />\n喺連登開呢啲post好大壓力<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /></blockquote><br />\n其實冇辦法，因為想簡單了解其實係好難既一件事"},{"pid":"a24ae934bbe739f544949603f860e6e407ae8850","tid":257808,"uid":32721,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T22:07:27.000Z","msg":"留名學野"},{"pid":"e787c9557d856f2368196c6389bad7fc1d5ad78b","tid":257808,"uid":79863,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T22:10:44.000Z","msg":"留名學野"},{"pid":"a1704816419fbd4679069d2428fd292efd95ea67","tid":257808,"uid":68583,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T22:15:02.000Z","msg":"<blockquote><blockquote>我想了解一下 alpha go 係根據highest possibility to win去捉棋，咁同人腦係咪一樣？算唔算人工智能？ 定只係一台可以自我進行最高成功率行動既機器？</blockquote><br />\n多嘴代樓主答. 嚴格黎講所有人為製造出黎嘅智能系統都可能叫人工智能, 同佢用嘅方法係咪同人腦一樣冇關係. <br />\n<br />\n如果你問alpha go 係咪好似人咁思考, 係一台strong AI, 咁答案係否定的. 原因好簡單, 佢淨係識捉圍棋, 唔識其它.</blockquote><br />\n<br />\n人類作出決定係經過係stimulus and response + memory，而alpha go係根據對手落子+ 經驗(or say possibility) 去決定每一步棋，所以我想知現今最前線bio study 對人腦既理解係咪都係同中學/degree level 差唔多？<br />\n另外，雖然alphago 只係識捉圍棋，但唔排除可以上升到一個高度可以根據possibility 決定行動，咁同人腦分析比較，係咪已經好接近 ？<br />\n<br />\n利申 Chem底 但間唔中睇paper 睇到類似既野 所以有呢個諗法"},{"pid":"2d1afe4c038230ebb335c3a61f2e007722e7b3da","tid":257808,"uid":26098,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T22:21:18.000Z","msg":"LM"},{"pid":"1c5d40bf167b6d6c8ed4f9f6dd15a6e2e451d975","tid":257808,"uid":69238,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T22:27:44.000Z","msg":"不得不留名"},{"pid":"982e640927c819b37ee2c796f25b94e0aebf7036","tid":257808,"uid":43570,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-25T22:57:20.000Z","msg":"<blockquote><blockquote><blockquote>我想了解一下 alpha go 係根據highest possibility to win去捉棋，咁同人腦係咪一樣？算唔算人工智能？ 定只係一台可以自我進行最高成功率行動既機器？</blockquote><br />\n多嘴代樓主答. 嚴格黎講所有人為製造出黎嘅智能系統都可能叫人工智能, 同佢用嘅方法係咪同人腦一樣冇關係. <br />\n<br />\n如果你問alpha go 係咪好似人咁思考, 係一台strong AI, 咁答案係否定的. 原因好簡單, 佢淨係識捉圍棋, 唔識其它.</blockquote><br />\n<br />\n人類作出決定係經過係stimulus and response + memory，而alpha go係根據對手落子+ 經驗(or say possibility) 去決定每一步棋，所以我想知現今最前線bio study 對人腦既理解係咪都係同中學/degree level 差唔多？<br />\n另外，雖然alphago 只係識捉圍棋，但唔排除可以上升到一個高度可以根據possibility 決定行動，咁同人腦分析比較，係咪已經好接近 ？<br />\n<br />\n利申 Chem底 但間唔中睇paper 睇到類似既野 所以有呢個諗法</blockquote><br />\n我估你係講緊probability, 而家學術界對人腦嘅理論多到數唔晒. 係有理論講人腦其實只係一部機率機器, 呢個叫bayesian brain theory. 而係最底層神經細胞層面, 呢個理論可能亦都有根據. neuron同neuron之間通訊, 有樣野叫long term potentiation. 簡單黎講, 如果兩個neurons 同時fire, 佢地之間嘅連結會變強. 我地可以理解成兩個事件同時出現, 會增強佢地之間嘅連結. 呢個其實就係累積計算緊兩件事同時發生的機率. <br />\n<br />\nalphago唔係strong AI 唔係因為佢係咪用probability, 用probability 嘅智能系統幾十年前都有. 可能咁講, 機率類人工智能根本就係machine learning 嘅基本 (e.g naive bayesian). alphago 個架構仲未可以令佢可以解決一般問題. 一個BB可能智力唔高, 但佢可以講下野, 又可以食野, 又識分辨父母個樣, 呢啲就係general intelligence. 我地暫時都未知點樣先可以做到咁. 我個人認為generall intelligence除左純綷經驗外, 仲要創意同分析.  人唔係單單 stimulus-response + memory 嘅動物. 我地仲識得自省, 透過經驗了解問題, 同埋提出創新的解決辦法. 而呢個能力係可以應用係任何方面, 就好似聰明人做大部份事都成功. 唔一定因為佢地天生樣樣掂,而後佢地掌握左一套可以應該用任何事情嘅方法."},{"pid":"3aa17bb18254a5651980845c3ec9cbe89bf51205","tid":257808,"uid":91664,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T23:01:29.000Z","msg":"繼續"},{"pid":"253454c87c6ef2689c04a693debd62137023450a","tid":257808,"uid":43570,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-25T23:13:09.000Z","msg":"<blockquote><blockquote><blockquote>我地見到一個人既樣既舜間，無意中某啲神經網絡就會有所活動，而當中所牽涉既神經元都有機會修改自身對所接收訊號之強弱既調節。即係話 S 對 N 既加減唔一定係每次都一樣，有機會變。呢個性質引致到如果我地每日都見一次同一個人，久而久之訊號既傳遞係呢啲網絡之間就會形成一種 Pattern，即是我地既記憶。<br />\n<br />\n今日寫住咁多先 ...</blockquote><br />\n一開始已經concept錯<img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /></blockquote><br />\nthat is based on findings of Neuroscience<br />\n<br />\n&quot;When neuroscientists use functional magnetic resonance imaging to see how a monkey&rsquo;s brain responds to familiar faces, something odd happens. When shown a familiar face, a monkey&rsquo;s brain lights up, not in a specific area, but in nine different ones. <strong>Neuroscientists call these areas &ldquo;face patches&rdquo; and think they are neural networks with the specialised functions associated with face recognition.</strong>&quot;<br />\n<br />\n<a href=\"https://www.technologyreview.com/s/535176/human-face-recognition-found-in-neural-network-based-on-monkey-brains/\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.technologyreview.com%2Fs%2F535176%2Fhuman-face-recognition-found-in-neural-network-based-on-monkey-brains%2F&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=68e68ad2\" target=\"_blank\">https://www.technologyreview.com/s/535176/human-face-recognition-found-in-neural-network-based-on-monkey-brains/</a></blockquote><br />\n嚴格黎講呢個只係discriminate 唔同樣, 唔係直接memory recall. 同埋記憶有好多種, 見得你今朝早餐食乜係一種, 記得點開車又係另一種. 暫時對記憶為何物都未有一個確定說法."},{"pid":"5569da575cc7acea23d2577ca5d9ec62b599fa28","tid":257808,"uid":35746,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T23:22:09.000Z","msg":"推"},{"pid":"e7dc05c61413abff1df1b3e0d29ccbaa82afe9e6","tid":257808,"uid":22121,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T23:36:28.000Z","msg":"留名"},{"pid":"eadc569b194cb2309256e8517f63b16aff64028d","tid":257808,"uid":7212,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T23:43:16.000Z","msg":"正皮支持<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"d4ce27905a791a1178ef89e0c1a918dc95747435","tid":257808,"uid":2337,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T23:50:40.000Z","msg":"<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" />"},{"pid":"a466cf682dd67a4eb22c8c2153efd3ed5912d620","tid":257808,"uid":12712,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T23:54:35.000Z","msg":"留名"},{"pid":"3dead10cacd428c0e6fb38e887e504c8b1ee0df2","tid":257808,"uid":62841,"like":2,"dislike":7,"score":-5,"citedBy":0,"replyTime":"2017-05-25T23:54:39.000Z","msg":"<blockquote>樓主介唔介意用英文寫<br />\n你啲中文有少少難明<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n樓主介唔介意用俄文寫<br />\n你啲中文有少少難明"},{"pid":"9b324e9ac6a9e249c54ff42f664c4d935b695412","tid":257808,"uid":216,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-25T23:58:03.000Z","msg":"咁快live 既， 好似未開始講alphago 咁 仲講緊bio"},{"pid":"55338fc157d8c28635f579bd228045dbfd31ed42","tid":257808,"uid":48679,"like":3,"dislike":1,"score":2,"citedBy":0,"replyTime":"2017-05-25T23:58:44.000Z","msg":"<blockquote>留名<br />\n<br />\n喂樓上指出錯係可以<br />\n但紅字加句咩此post已完/留舊<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> 就算<br />\n真係冇乜建設性<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> <br />\n<br />\n可否都麻煩大家 指錯處都留低一兩句 錯咩、正確係咩？<br />\n<br />\n難得有認真學術post 唔好一野打沉人啦<br />\n喺連登開呢啲post好大壓力<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /></blockquote><br />\nd 人要扮勁無計<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n具體野唔會講得出，剩係話自己讀乜鳩PhD, 係邊到邊到做咁"},{"pid":"a2af8c8b5c160e4b60ed1cea1d7b2a4848e7dc30","tid":257808,"uid":28279,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T00:00:38.000Z","msg":"<blockquote>以前都見過呢啲模擬人類神經網絡嘅程式嘅書（咦咩嚟，拎起，嘩好深，放返低），又聽過人話AI發展停滯不前，點解去到呢期突飛猛進？外星人通水？<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n個人電腦勁咗，所以普及咗"},{"pid":"54815b82981a5a8a3f1ecbb53a0f9b280a9b892f","tid":257808,"uid":47426,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T00:02:59.000Z","msg":"留名"},{"pid":"5c038cb58990d8f3cc328b2eab2f983ae54b0617","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T00:04:58.000Z","msg":"<blockquote><blockquote>以前都見過呢啲模擬人類神經網絡嘅程式嘅書（咦咩嚟，拎起，嘩好深，放返低），又聽過人話AI發展停滯不前，點解去到呢期突飛猛進？外星人通水？<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n個人電腦勁咗，所以普及咗</blockquote><br />\n<img src=\"/assets/faces/normal/bye.gif\" class=\"hkgmoji\" />"},{"pid":"825140e896eed4055c7def385f5cc155ad3a796d","tid":257808,"uid":42046,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-26T00:05:22.000Z","msg":"<blockquote><blockquote>留名<br />\n<br />\n喂樓上指出錯係可以<br />\n但紅字加句咩此post已完/留舊<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> 就算<br />\n真係冇乜建設性<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> <br />\n<br />\n可否都麻煩大家 指錯處都留低一兩句 錯咩、正確係咩？<br />\n<br />\n難得有認真學術post 唔好一野打沉人啦<br />\n喺連登開呢啲post好大壓力<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /></blockquote><br />\nd 人要扮勁無計<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n具體野唔會講得出，剩係話自己讀乜鳩PhD, 係邊到邊到做咁</blockquote><br />\n話人錯貼個<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" />就走 低質到。。"},{"pid":"7c068a99e41c62762d8c9ad084b37a4ba28674ea","tid":257808,"uid":29080,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T00:05:26.000Z","msg":"<img src=\"/assets/faces/lomoji/26.png\" class=\"hkgmoji\" />"},{"pid":"c41c2e71884f9dda98842eefccd0b6a18f8d98d2","tid":257808,"uid":60376,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T00:06:11.000Z","msg":"<blockquote>以前都見過呢啲模擬人類神經網絡嘅程式嘅書（咦咩嚟，拎起，嘩好深，放返低），又聽過人話AI發展停滯不前，點解去到呢期突飛猛進？外星人通水？<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n<br />\nd theory 真係唔係咩新鮮野<br />\n至於樽頸位我覺得係training speed<br />\n而家受惠於distributed技術成熟好多野都可以多工進行<br />\n而家家用電腦都4core8thread, 用埋gpu成千上萬core都不是夢<br />\nrun model可以縮短數以幾十倍計既時間<br />\n咁帶出黎相關既多工agorithm, database 制式自然都加速快展<br />\n其次big data個term太響引入資金投資都有唔少幫助<br />\n<br />\n利申9吹"},{"pid":"b428f2e97d18c5b6aa4b6bcabde876f9f7f3a13e","tid":257808,"uid":113467,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T00:09:07.000Z","msg":"留名學野<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" />"},{"pid":"7c08832f8a33244f183c413017c7035eb6c1e1eb","tid":257808,"uid":2034,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T00:11:30.000Z","msg":"LM"},{"pid":"ba1fd82d07c4e5941e235d8866a4d9f06d7cb0e9","tid":257808,"uid":17825,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T00:12:22.000Z","msg":"Lm"},{"pid":"344eff95857792c868f762552c3b99d859587e6c","tid":257808,"uid":28279,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T00:14:59.000Z","msg":"<blockquote><blockquote><blockquote>以前都見過呢啲模擬人類神經網絡嘅程式嘅書（咦咩嚟，拎起，嘩好深，放返低），又聽過人話AI發展停滯不前，點解去到呢期突飛猛進？外星人通水？<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n個人電腦勁咗，所以普及咗</blockquote><br />\n<img src=\"/assets/faces/normal/bye.gif\" class=\"hkgmoji\" /></blockquote><br />\n上網快咗或過data既harddisk/usb或網上掛既size大咗都有關，下下裝落floppy或者用56k，搞全日可能得幾廿粒data，蚊都瞓<br />\n不過大家上網大file size通常係用嚟打丁"},{"pid":"a5043498c368247a152f97d0e61ad44efbf6c837","tid":257808,"uid":70989,"like":2,"dislike":0,"score":2,"citedBy":0,"replyTime":"2017-05-26T00:17:11.000Z","msg":"<blockquote><blockquote>留名<br />\n<br />\n喂樓上指出錯係可以<br />\n但紅字加句咩此post已完/留舊<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> 就算<br />\n真係冇乜建設性<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> <br />\n<br />\n可否都麻煩大家 指錯處都留低一兩句 錯咩、正確係咩？<br />\n<br />\n難得有認真學術post 唔好一野打沉人啦<br />\n喺連登開呢啲post好大壓力<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /></blockquote><br />\nd 人要扮勁無計<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n具體野唔會講得出，剩係話自己讀乜鳩PhD, 係邊到邊到做咁</blockquote><br />\n讀乜鳩PhD都叫扮勁，話人講唔出 <img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /><br />\n你睇就有份，下下都係要人由頭到尾講俾你講到明，其實網上都有唔少資訊有人寫關於佢既文章，一定講得比我好，而且我都冇咁既時候去寫長文解釋<br />\n<br />\nAlphaGo用既主要idea係deep reinforcement learning with Monte Carlo tree search, 結果同成個methodology係Nature上面發表左超過一年，如果你係大學生可以免費download嚟睇<br />\n<br />\n你想學最多既野，一定係自己主動去搵嚟睇，慢慢理解，你可以選擇速食，但之後你好快就唔記得<br />\n<br />\n如果有咩得罪，係度講句唔好意思"},{"pid":"7fd8e4de049475584495cdb06ff681a0a3a6ca29","tid":257808,"uid":37489,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-26T00:18:30.000Z","msg":"<blockquote><blockquote><blockquote><blockquote>以前都見過呢啲模擬人類神經網絡嘅程式嘅書（咦咩嚟，拎起，嘩好深，放返低），又聽過人話AI發展停滯不前，點解去到呢期突飛猛進？外星人通水？<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n個人電腦勁咗，所以普及咗</blockquote><br />\n<img src=\"/assets/faces/normal/bye.gif\" class=\"hkgmoji\" /></blockquote><br />\n上網快咗或過data既harddisk/usb或網上掛既size大咗都有關，下下裝落floppy或者用56k，搞全日可能得幾廿粒data，蚊都瞓<br />\n不過大家上網大file size通常係用嚟打丁</blockquote><br />\nNeural network本身係已經提出左好耐既野 只係training時間要好長(single thread)<br />\n而gpu既parallellism同埋本身design計既graphics(matrix/grid)同neural net既設計好似 所以先突然間推得咁快<br />\n<br />\n如果上網就係關distributed system事"},{"pid":"0931d05bd386ba32c58a00ef46df46310843976c","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T00:22:01.000Z","msg":"<blockquote><blockquote>以前都見過呢啲模擬人類神經網絡嘅程式嘅書（咦咩嚟，拎起，嘩好深，放返低），又聽過人話AI發展停滯不前，點解去到呢期突飛猛進？外星人通水？<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n<br />\nd theory 真係唔係咩新鮮野<br />\n至於樽頸位我覺得係training speed<br />\n而家受惠於distributed技術成熟好多野都可以多工進行<br />\n而家家用電腦都4core8thread, 用埋gpu成千上萬core都不是夢<br />\nrun model可以縮短數以幾十倍計既時間<br />\n咁帶出黎相關既多工agorithm, database 制式自然都加速快展<br />\n其次big data個term太響引入資金投資都有唔少幫助<br />\n<br />\n利申9吹</blockquote><br />\nN記掉左好多錢落去 學術界好多都要買gtx1080/titanX<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n利申寫過下CUDA, 真係好撚爽<img src=\"/assets/faces/normal/tongue.gif\" class=\"hkgmoji\" />"},{"pid":"cb25af61e83424b873bc16060482c0e2a3db445c","tid":257808,"uid":1303,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T00:25:32.000Z","msg":"點解我會見到林達<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" />"},{"pid":"351e8c1447ef1ccf31efd2c8b12d9e80e6b835bc","tid":257808,"uid":86775,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T00:26:36.000Z","msg":"lm"},{"pid":"a43bcd3d2f0f4625227f33d519a43d23aca9b88d","tid":257808,"uid":48679,"like":2,"dislike":0,"score":2,"citedBy":0,"replyTime":"2017-05-26T00:35:12.000Z","msg":"<blockquote><blockquote><blockquote><blockquote>我想了解一下 alpha go 係根據highest possibility to win去捉棋，咁同人腦係咪一樣？算唔算人工智能？ 定只係一台可以自我進行最高成功率行動既機器？</blockquote><br />\n多嘴代樓主答. 嚴格黎講所有人為製造出黎嘅智能系統都可能叫人工智能, 同佢用嘅方法係咪同人腦一樣冇關係. <br />\n<br />\n如果你問alpha go 係咪好似人咁思考, 係一台strong AI, 咁答案係否定的. 原因好簡單, 佢淨係識捉圍棋, 唔識其它.</blockquote><br />\n<br />\n人類作出決定係經過係stimulus and response + memory，而alpha go係根據對手落子+ 經驗(or say possibility) 去決定每一步棋，所以我想知現今最前線bio study 對人腦既理解係咪都係同中學/degree level 差唔多？<br />\n另外，雖然alphago 只係識捉圍棋，但唔排除可以上升到一個高度可以根據possibility 決定行動，咁同人腦分析比較，係咪已經好接近 ？<br />\n<br />\n利申 Chem底 但間唔中睇paper 睇到類似既野 所以有呢個諗法</blockquote><br />\n我估你係講緊probability, 而家學術界對人腦嘅理論多到數唔晒. 係有理論講人腦其實只係一部機率機器, 呢個叫bayesian brain theory. 而係最底層神經細胞層面, 呢個理論可能亦都有根據. neuron同neuron之間通訊, 有樣野叫long term potentiation. 簡單黎講, 如果兩個neurons 同時fire, 佢地之間嘅連結會變強. 我地可以理解成兩個事件同時出現, 會增強佢地之間嘅連結. 呢個其實就係累積計算緊兩件事同時發生的機率. <br />\n<br />\nalphago唔係strong AI 唔係因為佢係咪用probability, 用probability 嘅智能系統幾十年前都有. 可能咁講, 機率類人工智能根本就係machine learning 嘅基本 (e.g naive bayesian). alphago 個架構仲未可以令佢可以解決一般問題. 一個BB可能智力唔高, 但佢可以講下野, 又可以食野, 又識分辨父母個樣, 呢啲就係general intelligence. 我地暫時都未知點樣先可以做到咁. 我個人認為generall intelligence除左純綷經驗外, 仲要創意同分析.  人唔係單單 stimulus-response + memory 嘅動物. 我地仲識得<span style=\"color: red;\">自省, 透過經驗了解問題, 同埋提出創新的解決辦法.</span> 而呢個能力係可以應用係任何方面, 就好似聰明人做大部份事都成功. 唔一定因為佢地天生樣樣掂,而後佢地掌握左一套可以應該用任何事情嘅方法.</blockquote><br />\n前者只係簡單既regression, machine learning一早已經做緊<br />\n而後者，其實都只係multiple of multilayer neural network， 將所有方面既知識串連埋一齊再得出答案<br />\n你會話，唔係喎，人不斷重覆諗同一個問題，會有時可以創新到方法，而ai 每次都會得出同一個答案。但其實只係人會每次都evaluate 自己個答案既滿意度再feedback 番去自己個neural network 到做training。 而machine learning 一般唔會咁做只係想個result controllable, 同埋無咁易俾d poison (有意/無意)data 影響，而留待下一次做batch training 先version upgrade <br />\n而且其實呢個係人既弱點，因為好易俾人洗腦，只要不斷重覆講一個大話一百次，一千次，人就會不知不覺間信左<br />\n<br />\n人之所以好似好強大，創新力比ai 強係因為人真係有個非常大既neural network,每個cluster of nodes 都掌管唔同知識，但卻可以同時對所有input 作出反應，而且係每次input 都自動調整每個cluster of nodes 既weight<br />\n目前黎講machine learning 仲未有咁既computing power 去模擬呢樣野<br />\n或者咁講，其實可能已經有咁既computing power, 但一般由於效率比單一用途既machine learning低，暫時未有人倒足夠既錢落海去做呢樣野<br />\n其實見到google 搞cloud 就好明顯係想做呢樣野，因為佢想偷d 用家用剩既computation time 黎用， 成本基本上係cloud 既用家俾，但只要cloud 用家越黎越多，google 賺到既computing power 就越多"},{"pid":"e3b723e598e396d55f48520d0ebb7518e82e2d91","tid":257808,"uid":37489,"like":2,"dislike":0,"score":2,"citedBy":0,"replyTime":"2017-05-26T00:35:35.000Z","msg":"9up下reinforcement learning係乜鳩<br />\n<br />\n類似就係你而家呢個state(t0)有a0,b0,c0呢3個action可以做<br />\n<br />\n咁做每樣野都有個reward,當係食粒糖你食(positive reward)/做完會比人屌(negative reward)<br />\ngiven reward(a0) &gt; reward(b0) &gt; reward(c0)<br />\n<br />\n佢就緊係會揀食糖啦(最好果個option: a0)<br />\n<br />\n咁問題就係到下一個state(t1)就有幾個option<br />\n<br />\n揀完a0有, a01, a02, a03, (reward a01 &gt; a02 &gt; a03)<br />\n揀完b0有, b01, b02, b03, (reward b01 &gt; b02 &gt; b03)<br />\n揀完c0有, c01, c02, c03, (reward c01 &gt; c02 &gt; c03)<br />\n<br />\n咁佢又要再揀，但係佢發現原來上一個state(t0)冇咁好果個option(b0)係呢一個state(t1)有個好d既option揀 (b01 &gt; a01)，甚至加埋係 (b01 +b0&gt; a0 + a01)<br />\n<br />\n所以睇多一個state就發現自己應該揀b0-&gt;b01，而唔係揀a0-&gt;a01<br />\n<br />\n而google班IT狗就係呢一part就係要諗方法搵最optimal既option<br />\n<br />\n而另一樣野就係點定義個reward, 應該就係要靠monte carlo(好似係<br />\n<br />\n利申冇詳細睇過nature果份paper 有錯請指出<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"85f67bd5a360d036930ed2b4bf61982213211555","tid":257808,"uid":54380,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T00:36:32.000Z","msg":"Lm"},{"pid":"a87ddcc966d81affcf4d8ad5bed6e7a26eca42d1","tid":257808,"uid":95510,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T00:36:44.000Z","msg":"Lm"},{"pid":"fc24cfdd78120451637fac4157c0ce973374af87","tid":257808,"uid":37489,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-26T00:39:43.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote>我想了解一下 alpha go 係根據highest possibility to win去捉棋，咁同人腦係咪一樣？算唔算人工智能？ 定只係一台可以自我進行最高成功率行動既機器？</blockquote><br />\n多嘴代樓主答. 嚴格黎講所有人為製造出黎嘅智能系統都可能叫人工智能, 同佢用嘅方法係咪同人腦一樣冇關係. <br />\n<br />\n如果你問alpha go 係咪好似人咁思考, 係一台strong AI, 咁答案係否定的. 原因好簡單, 佢淨係識捉圍棋, 唔識其它.</blockquote><br />\n<br />\n人類作出決定係經過係stimulus and response + memory，而alpha go係根據對手落子+ 經驗(or say possibility) 去決定每一步棋，所以我想知現今最前線bio study 對人腦既理解係咪都係同中學/degree level 差唔多？<br />\n另外，雖然alphago 只係識捉圍棋，但唔排除可以上升到一個高度可以根據possibility 決定行動，咁同人腦分析比較，係咪已經好接近 ？<br />\n<br />\n利申 Chem底 但間唔中睇paper 睇到類似既野 所以有呢個諗法</blockquote><br />\n我估你係講緊probability, 而家學術界對人腦嘅理論多到數唔晒. 係有理論講人腦其實只係一部機率機器, 呢個叫bayesian brain theory. 而係最底層神經細胞層面, 呢個理論可能亦都有根據. neuron同neuron之間通訊, 有樣野叫long term potentiation. 簡單黎講, 如果兩個neurons 同時fire, 佢地之間嘅連結會變強. 我地可以理解成兩個事件同時出現, 會增強佢地之間嘅連結. 呢個其實就係累積計算緊兩件事同時發生的機率. <br />\n<br />\nalphago唔係strong AI 唔係因為佢係咪用probability, 用probability 嘅智能系統幾十年前都有. 可能咁講, 機率類人工智能根本就係machine learning 嘅基本 (e.g naive bayesian). alphago 個架構仲未可以令佢可以解決一般問題. 一個BB可能智力唔高, 但佢可以講下野, 又可以食野, 又識分辨父母個樣, 呢啲就係general intelligence. 我地暫時都未知點樣先可以做到咁. 我個人認為generall intelligence除左純綷經驗外, 仲要創意同分析.  人唔係單單 stimulus-response + memory 嘅動物. 我地仲識得<span style=\"color: red;\">自省, 透過經驗了解問題, 同埋提出創新的解決辦法.</span> 而呢個能力係可以應用係任何方面, 就好似聰明人做大部份事都成功. 唔一定因為佢地天生樣樣掂,而後佢地掌握左一套可以應該用任何事情嘅方法.</blockquote><br />\n前者只係簡單既regression, machine learning一早已經做緊<br />\n而後者，其實都只係multiple of multilayer neural network， 將所有方面既知識串連埋一齊再得出答案<br />\n你會話，唔係喎，人不斷重覆諗同一個問題，會有時可以創新到方法，而ai 每次都會得出同一個答案。但其實只係人會每次都evaluate 自己個答案既滿意度再feedback 番去自己個neural network 到做training。 而machine learning 一般唔會咁做只係想個result controllable, 同埋無咁易俾d poison (有意/無意)data 影響，而留待下一次做batch training 先version upgrade <br />\n而且其實呢個係人既弱點，因為好易俾人洗腦，只要不斷重覆講一個大話一百次，一千次，人就會不知不覺間信左<br />\n<br />\n人之所以好似好強大，創新力比ai 強係因為人真係有個非常大既neural network,每個cluster of nodes 都掌管唔同知識，但卻可以同時對所有input 作出反應，而且係每次input 都自動調整每個cluster of nodes 既weight<br />\n目前黎講machine learning 仲未有咁既computing power 去模擬呢樣野<br />\n或者咁講，其實可能已經有咁既computing power, 但一般由於效率比單一用途既machine learning低，暫時未有人倒足夠既錢落海去做呢樣野<br />\n其實見到google 搞cloud 就好明顯係想做呢樣野，因為佢想偷d 用家用剩既computation time 黎用， 成本基本上係cloud 既用家俾，但只要cloud 用家越黎越多，google 賺到既computing power 就越多</blockquote><br />\n我記得有人compare過人既neuron processing speed同電腦比<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n養部alpha go剩係電費都交死你, 要mobile robot carry舊咁既野,加埋比alpha go用既電池 應該都冇乜人養得起<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n而家半導體技術又係瓶頸左"},{"pid":"08f8391ca91ca9946e4bce6bea7e8788b80adad8","tid":257808,"uid":48679,"like":2,"dislike":1,"score":1,"citedBy":0,"replyTime":"2017-05-26T00:40:11.000Z","msg":"<blockquote><blockquote><blockquote>留名<br />\n<br />\n喂樓上指出錯係可以<br />\n但紅字加句咩此post已完/留舊<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> 就算<br />\n真係冇乜建設性<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> <br />\n<br />\n可否都麻煩大家 指錯處都留低一兩句 錯咩、正確係咩？<br />\n<br />\n難得有認真學術post 唔好一野打沉人啦<br />\n喺連登開呢啲post好大壓力<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /></blockquote><br />\nd 人要扮勁無計<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n具體野唔會講得出，剩係話自己讀乜鳩PhD, 係邊到邊到做咁</blockquote><br />\n讀乜鳩PhD都叫扮勁，話人講唔出 <img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /><br />\n你睇就有份，下下都係要人由頭到尾講俾你講到明，其實網上都有唔少資訊有人寫關於佢既文章，一定講得比我好，而且我都冇咁既時候去寫長文解釋<br />\n<br />\nAlphaGo用既主要idea係deep reinforcement learning with Monte Carlo tree search, 結果同成個methodology係Nature上面發表左超過一年，如果你係大學生可以免費download嚟睇<br />\n<br />\n你想學最多既野，一定係自己主動去搵嚟睇，慢慢理解，你可以選擇速食，但之後你好快就唔記得<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\nalphago 上年已經睇左啦，咁基本野睇完又要拎出嚟晒？<br />\n你無野係到貢獻就直接出去啦，留個讀乜鳩PhD 係到有咩意義？你又浪費時間，我又無得著<br />\n唯一推論咪就係有人想晒命，認叻囉<br />\n<br />\n如果有咩得罪，係度講句唔好意思"},{"pid":"b5e70dfa76c1fe9e29eec38d60984237c36aa0d1","tid":257808,"uid":98493,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-26T00:42:28.000Z","msg":"如果有興趣睇深啲既人可以Google search &ldquo;Mastering the game of Go with deep neural networks and tree search&rdquo; 呢篇文係Deepmind 同 Google 啲人寫既。大約解釋alphago 既Monte Carlo tree search method, 佢同書教既minimax, depth first 好唔同<br />\n<br />\nlink: <a href=\"https://gogameguru.com/i/2016/03/deepmind-mastering-go.pdf\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fgogameguru.com%2Fi%2F2016%2F03%2Fdeepmind-mastering-go.pdf&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=4558bfbb\" data-auto-link target=\"_blank\">https://gogameguru.com/i/2016/03/deepmind-mastering-go.pdf</a><br />\n<br />\n利申 讀緊AI"},{"pid":"ec8e138d91e5856f22b8833df39f46749691aca1","tid":257808,"uid":60582,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T00:45:12.000Z","msg":"留名"},{"pid":"39c1f64e49c043901a8d38884998756cdf701ff0","tid":257808,"uid":93084,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T00:46:11.000Z","msg":"留個名"},{"pid":"4b46194823ac8ccae201722513b49226747bcca9","tid":257808,"uid":20222,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T00:51:09.000Z","msg":"留明"},{"pid":"6d5ab88a8517ef77cd4853bf24a120c1968be326","tid":257808,"uid":70989,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-26T00:56:37.000Z","msg":"<blockquote><blockquote><blockquote><blockquote>留名<br />\n<br />\n喂樓上指出錯係可以<br />\n但紅字加句咩此post已完/留舊<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> 就算<br />\n真係冇乜建設性<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> <br />\n<br />\n可否都麻煩大家 指錯處都留低一兩句 錯咩、正確係咩？<br />\n<br />\n難得有認真學術post 唔好一野打沉人啦<br />\n喺連登開呢啲post好大壓力<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /></blockquote><br />\nd 人要扮勁無計<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n具體野唔會講得出，剩係話自己讀乜鳩PhD, 係邊到邊到做咁</blockquote><br />\n讀乜鳩PhD都叫扮勁，話人講唔出 <img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /><br />\n你睇就有份，下下都係要人由頭到尾講俾你講到明，其實網上都有唔少資訊有人寫關於佢既文章，一定講得比我好，而且我都冇咁既時候去寫長文解釋<br />\n<br />\nAlphaGo用既主要idea係deep reinforcement learning with Monte Carlo tree search, 結果同成個methodology係Nature上面發表左超過一年，如果你係大學生可以免費download嚟睇<br />\n<br />\n你想學最多既野，一定係自己主動去搵嚟睇，慢慢理解，你可以選擇速食，但之後你好快就唔記得<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\nalphago 上年已經睇左啦，咁基本野睇完又要拎出嚟晒？<br />\n你無野係到貢獻就直接出去啦，留個讀乜鳩PhD 係到有咩意義？你又浪費時間，我又無得著<br />\n唯一推論咪就係有人想晒命，認叻囉<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\n我講我讀PhD係因為我批評人都要有理據 如果我係乜都唔識就話人就係我唔啱 <br />\n<br />\n你覺得係基本既paper唔代表其他人睇過，更加唔係咩晒<br />\n<br />\n我覺得雖然我對呢方面識得多過人，但我學得越多就覺得自己識得越少，所以唔覺得PhD係乜野晒命認叻"},{"pid":"233f3f902d314f80c43f9d926f4adf5124e1c2ce","tid":257808,"uid":48679,"like":2,"dislike":1,"score":1,"citedBy":0,"replyTime":"2017-05-26T01:02:44.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote>留名<br />\n<br />\n喂樓上指出錯係可以<br />\n但紅字加句咩此post已完/留舊<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> 就算<br />\n真係冇乜建設性<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> <br />\n<br />\n可否都麻煩大家 指錯處都留低一兩句 錯咩、正確係咩？<br />\n<br />\n難得有認真學術post 唔好一野打沉人啦<br />\n喺連登開呢啲post好大壓力<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /></blockquote><br />\nd 人要扮勁無計<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n具體野唔會講得出，剩係話自己讀乜鳩PhD, 係邊到邊到做咁</blockquote><br />\n讀乜鳩PhD都叫扮勁，話人講唔出 <img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /><br />\n你睇就有份，下下都係要人由頭到尾講俾你講到明，其實網上都有唔少資訊有人寫關於佢既文章，一定講得比我好，而且我都冇咁既時候去寫長文解釋<br />\n<br />\nAlphaGo用既主要idea係deep reinforcement learning with Monte Carlo tree search, 結果同成個methodology係Nature上面發表左超過一年，如果你係大學生可以免費download嚟睇<br />\n<br />\n你想學最多既野，一定係自己主動去搵嚟睇，慢慢理解，你可以選擇速食，但之後你好快就唔記得<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\nalphago 上年已經睇左啦，咁基本野睇完又要拎出嚟晒？<br />\n你無野係到貢獻就直接出去啦，留個讀乜鳩PhD 係到有咩意義？你又浪費時間，我又無得著<br />\n唯一推論咪就係有人想晒命，認叻囉<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\n我講我讀PhD係因為我批評人都要有理據 如果我係乜都唔識就話人就係我唔啱 <br />\n<br />\n你覺得係基本既paper唔代表其他人睇過，更加唔係咩晒<br />\n<br />\n我覺得雖然我對呢方面識得多過人，但我學得越多就覺得自己識得越少，所以唔覺得PhD係乜野晒命認叻</blockquote><br />\n你批評人既理據就係你讀phd?<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n邊個professor 教你？<br />\n我從來未見過一個professor 表達意見既時候提出既理據就係佢係professor 囉(支那除外)<br />\n<br />\n你d 咁既權威性人格，我好懷疑你做research 既能力囉"},{"pid":"031520eed868c24ff12d736a7a3b927c2c15dd60","tid":257808,"uid":61512,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T01:08:54.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>留名<br />\n<br />\n喂樓上指出錯係可以<br />\n但紅字加句咩此post已完/留舊<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> 就算<br />\n真係冇乜建設性<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> <br />\n<br />\n可否都麻煩大家 指錯處都留低一兩句 錯咩、正確係咩？<br />\n<br />\n難得有認真學術post 唔好一野打沉人啦<br />\n喺連登開呢啲post好大壓力<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /></blockquote><br />\nd 人要扮勁無計<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n具體野唔會講得出，剩係話自己讀乜鳩PhD, 係邊到邊到做咁</blockquote><br />\n讀乜鳩PhD都叫扮勁，話人講唔出 <img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /><br />\n你睇就有份，下下都係要人由頭到尾講俾你講到明，其實網上都有唔少資訊有人寫關於佢既文章，一定講得比我好，而且我都冇咁既時候去寫長文解釋<br />\n<br />\nAlphaGo用既主要idea係deep reinforcement learning with Monte Carlo tree search, 結果同成個methodology係Nature上面發表左超過一年，如果你係大學生可以免費download嚟睇<br />\n<br />\n你想學最多既野，一定係自己主動去搵嚟睇，慢慢理解，你可以選擇速食，但之後你好快就唔記得<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\nalphago 上年已經睇左啦，咁基本野睇完又要拎出嚟晒？<br />\n你無野係到貢獻就直接出去啦，留個讀乜鳩PhD 係到有咩意義？你又浪費時間，我又無得著<br />\n唯一推論咪就係有人想晒命，認叻囉<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\n我講我讀PhD係因為我批評人都要有理據 如果我係乜都唔識就話人就係我唔啱 <br />\n<br />\n你覺得係基本既paper唔代表其他人睇過，更加唔係咩晒<br />\n<br />\n我覺得雖然我對呢方面識得多過人，但我學得越多就覺得自己識得越少，所以唔覺得PhD係乜野晒命認叻</blockquote><br />\n你批評人既理據就係你讀phd?<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n邊個professor 教你？<br />\n我從來未見過一個professor 表達意見既時候提出既理據就係佢係professor 囉(支那除外)<br />\n<br />\n你d 咁既權威性人格，我好懷疑你做research 既能力囉</blockquote><br />\n<img src=\"/assets/faces/normal/like.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/like.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/like.gif\" class=\"hkgmoji\" />"},{"pid":"cccf85f9e37fd73b38f6e1a34b0a579e5029a9ae","tid":257808,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T01:11:55.000Z","msg":"CNN 最強大係個C字<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /><br />\n<br />\n明嘅就會明<br />\n<br />\n利申: 玩過下mnist, 想應用係medical方面但prof唔想我咁做<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" />"},{"pid":"269a428189f78be08866a7e7a0481fc824c7825a","tid":257808,"uid":28279,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T01:16:31.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote>以前都見過呢啲模擬人類神經網絡嘅程式嘅書（咦咩嚟，拎起，嘩好深，放返低），又聽過人話AI發展停滯不前，點解去到呢期突飛猛進？外星人通水？<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n個人電腦勁咗，所以普及咗</blockquote><br />\n<img src=\"/assets/faces/normal/bye.gif\" class=\"hkgmoji\" /></blockquote><br />\n上網快咗或過data既harddisk/usb或網上掛既size大咗都有關，下下裝落floppy或者用56k，搞全日可能得幾廿粒data，蚊都瞓<br />\n不過大家上網大file size通常係用嚟打丁</blockquote><br />\nNeural network本身係已經提出左好耐既野 只係training時間要好長(single thread)<br />\n而gpu既parallellism同埋本身design計既graphics(matrix/grid)同neural net既設計好似 所以先突然間推得咁快<br />\n<br />\n如果上網就係關distributed system事</blockquote><br />\nDistributed 就好耐之前已經有NASA個乜乜program裝個software幫手分析，個人認為呢幾年個個掛越嚟越多data上網先係普及原因，當然train得快都好緊要"},{"pid":"d3cc6ebaef2680a13d1915bc20d363d13e15bf09","tid":257808,"uid":70989,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T01:16:49.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>留名<br />\n<br />\n喂樓上指出錯係可以<br />\n但紅字加句咩此post已完/留舊<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> 就算<br />\n真係冇乜建設性<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> <br />\n<br />\n可否都麻煩大家 指錯處都留低一兩句 錯咩、正確係咩？<br />\n<br />\n難得有認真學術post 唔好一野打沉人啦<br />\n喺連登開呢啲post好大壓力<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /></blockquote><br />\nd 人要扮勁無計<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n具體野唔會講得出，剩係話自己讀乜鳩PhD, 係邊到邊到做咁</blockquote><br />\n讀乜鳩PhD都叫扮勁，話人講唔出 <img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /><br />\n你睇就有份，下下都係要人由頭到尾講俾你講到明，其實網上都有唔少資訊有人寫關於佢既文章，一定講得比我好，而且我都冇咁既時候去寫長文解釋<br />\n<br />\nAlphaGo用既主要idea係deep reinforcement learning with Monte Carlo tree search, 結果同成個methodology係Nature上面發表左超過一年，如果你係大學生可以免費download嚟睇<br />\n<br />\n你想學最多既野，一定係自己主動去搵嚟睇，慢慢理解，你可以選擇速食，但之後你好快就唔記得<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\nalphago 上年已經睇左啦，咁基本野睇完又要拎出嚟晒？<br />\n你無野係到貢獻就直接出去啦，留個讀乜鳩PhD 係到有咩意義？你又浪費時間，我又無得著<br />\n唯一推論咪就係有人想晒命，認叻囉<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\n我講我讀PhD係因為我批評人都要有理據 <span style=\"color: red;\">如果我係乜都唔識就話人就係我唔啱</span> <br />\n<br />\n你覺得係基本既paper唔代表其他人睇過，更加唔係咩晒<br />\n<br />\n我覺得雖然我對呢方面識得多過人，但我學得越多就覺得自己識得越少，所以唔覺得PhD係乜野晒命認叻</blockquote><br />\n你批評人既理據就係你讀phd?<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n邊個professor 教你？<br />\n我從來未見過一個professor 表達意見既時候提出既理據就係佢係professor 囉(支那除外)<br />\n<br />\n你d 咁既權威性人格，我好懷疑你做research 既能力囉</blockquote><br />\n我批評人既理據係因為我識相關既野 見紅字 學歷只係一個證明 <br />\n<br />\n駁返你你就人身攻擊 話咩邊個professor教你 權威性人格 懷疑research能力等等 <br />\n想問問權威性人格同research能力個關係係邊？<br />\n<br />\n雖然呢度係網上世界 大家唔知對方身份 但係都講互相尊重"},{"pid":"604b6ad5444eb36dcea6f1045268b3bbf2faf615","tid":257808,"uid":24759,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T01:17:58.000Z","msg":"lm"},{"pid":"35e2d440cb8ae7d4a87c0bd67eb8a96a9ff73598","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T01:22:59.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>以前都見過呢啲模擬人類神經網絡嘅程式嘅書（咦咩嚟，拎起，嘩好深，放返低），又聽過人話AI發展停滯不前，點解去到呢期突飛猛進？外星人通水？<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n個人電腦勁咗，所以普及咗</blockquote><br />\n<img src=\"/assets/faces/normal/bye.gif\" class=\"hkgmoji\" /></blockquote><br />\n上網快咗或過data既harddisk/usb或網上掛既size大咗都有關，下下裝落floppy或者用56k，搞全日可能得幾廿粒data，蚊都瞓<br />\n不過大家上網大file size通常係用嚟打丁</blockquote><br />\nNeural network本身係已經提出左好耐既野 只係training時間要好長(single thread)<br />\n而gpu既parallellism同埋本身design計既graphics(matrix/grid)同neural net既設計好似 所以先突然間推得咁快<br />\n<br />\n如果上網就係關distributed system事</blockquote><br />\nDistributed 就好耐之前已經有NASA個乜乜program裝個software幫手分析，個人認為呢幾年個個掛越嚟越多data上網先係普及原因，當然train得快都好緊要</blockquote><br />\nImageNet果個project都係既"},{"pid":"5cdf15bb79d51d697fa7563175f60272360bf040","tid":257808,"uid":48679,"like":0,"dislike":1,"score":-1,"citedBy":0,"replyTime":"2017-05-26T01:35:00.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>留名<br />\n<br />\n喂樓上指出錯係可以<br />\n但紅字加句咩此post已完/留舊<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> 就算<br />\n真係冇乜建設性<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> <br />\n<br />\n可否都麻煩大家 指錯處都留低一兩句 錯咩、正確係咩？<br />\n<br />\n難得有認真學術post 唔好一野打沉人啦<br />\n喺連登開呢啲post好大壓力<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /></blockquote><br />\nd 人要扮勁無計<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n具體野唔會講得出，剩係話自己讀乜鳩PhD, 係邊到邊到做咁</blockquote><br />\n讀乜鳩PhD都叫扮勁，話人講唔出 <img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /><br />\n你睇就有份，下下都係要人由頭到尾講俾你講到明，其實網上都有唔少資訊有人寫關於佢既文章，一定講得比我好，而且我都冇咁既時候去寫長文解釋<br />\n<br />\nAlphaGo用既主要idea係deep reinforcement learning with Monte Carlo tree search, 結果同成個methodology係Nature上面發表左超過一年，如果你係大學生可以免費download嚟睇<br />\n<br />\n你想學最多既野，一定係自己主動去搵嚟睇，慢慢理解，你可以選擇速食，但之後你好快就唔記得<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\nalphago 上年已經睇左啦，咁基本野睇完又要拎出嚟晒？<br />\n你無野係到貢獻就直接出去啦，留個讀乜鳩PhD 係到有咩意義？你又浪費時間，我又無得著<br />\n唯一推論咪就係有人想晒命，認叻囉<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\n我講我讀PhD係因為我批評人都要有理據 <span style=\"color: red;\">如果我係乜都唔識就話人就係我唔啱</span> <br />\n<br />\n你覺得係基本既paper唔代表其他人睇過，更加唔係咩晒<br />\n<br />\n我覺得雖然我對呢方面識得多過人，但我學得越多就覺得自己識得越少，所以唔覺得PhD係乜野晒命認叻</blockquote><br />\n你批評人既理據就係你讀phd?<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n邊個professor 教你？<br />\n我從來未見過一個professor 表達意見既時候提出既理據就係佢係professor 囉(支那除外)<br />\n<br />\n你d 咁既權威性人格，我好懷疑你做research 既能力囉</blockquote><br />\n我批評人既理據係因為我識相關既野 見紅字 學歷只係一個證明 <br />\n<br />\n駁返你你就人身攻擊 話咩邊個professor教你 權威性人格 懷疑research能力等等 <br />\n想問問權威性人格同research能力個關係係邊？<br />\n<br />\n雖然呢度係網上世界 大家唔知對方身份 但係都講互相尊重</blockquote>咁叫人生攻擊你好應該反省下自己一路睇住個post 之前已經有既bias 係幾大<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n我就唔晒時間係到答你權威性人格同research 能力個關係啦，我講你都未必有能力吸收/你既bias 根本就會懵閉你雙眼/個腦<br />\n自己睇paper 啦<br />\n<a href=\"https://www.google.com.hk/url?sa=t&amp;source=web&amp;rct=j&amp;url=http://citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.580.1825%26rep%3Drep1%26type%3Dpdf&amp;ved=0ahUKEwjl5cOOs4zUAhUDoZQKHbbkA244ChAWCCYwAw&amp;usg=AFQjCNGP_bSXhM39EFpKiQ7SqHDnmQtk-g&amp;sig2=-rcfSsq8ySXiNlK3H-6K3g\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.google.com.hk%2Furl%3Fsa%3Dt%26source%3Dweb%26rct%3Dj%26url%3Dhttp%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%253Fdoi%253D10.1.1.580.1825%2526rep%253Drep1%2526type%253Dpdf%26ved%3D0ahUKEwjl5cOOs4zUAhUDoZQKHbbkA244ChAWCCYwAw%26usg%3DAFQjCNGP_bSXhM39EFpKiQ7SqHDnmQtk-g%26sig2%3D-rcfSsq8ySXiNlK3H-6K3g&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=3454ea27\" target=\"_blank\">https://www.google.com.hk/url?sa=t&amp;source=web&amp;rct=j&amp;url=http://citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.580.1825%26rep%3Drep1%26type%3Dpdf&amp;ved=0ahUKEwjl5cOOs4zUAhUDoZQKHbbkA244ChAWCCYwAw&amp;usg=AFQjCNGP_bSXhM39EFpKiQ7SqHDnmQtk-g&amp;sig2=-rcfSsq8ySXiNlK3H-6K3g</a>"},{"pid":"e1b45839362132ac16ca4e604ca768adb3ae179f","tid":257808,"uid":48679,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T01:38:25.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>留名<br />\n<br />\n喂樓上指出錯係可以<br />\n但紅字加句咩此post已完/留舊<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> 就算<br />\n真係冇乜建設性<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> <br />\n<br />\n可否都麻煩大家 指錯處都留低一兩句 錯咩、正確係咩？<br />\n<br />\n難得有認真學術post 唔好一野打沉人啦<br />\n喺連登開呢啲post好大壓力<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /></blockquote><br />\nd 人要扮勁無計<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n具體野唔會講得出，剩係話自己讀乜鳩PhD, 係邊到邊到做咁</blockquote><br />\n讀乜鳩PhD都叫扮勁，話人講唔出 <img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /><br />\n你睇就有份，下下都係要人由頭到尾講俾你講到明，其實網上都有唔少資訊有人寫關於佢既文章，一定講得比我好，而且我都冇咁既時候去寫長文解釋<br />\n<br />\nAlphaGo用既主要idea係deep reinforcement learning with Monte Carlo tree search, 結果同成個methodology係Nature上面發表左超過一年，如果你係大學生可以免費download嚟睇<br />\n<br />\n你想學最多既野，一定係自己主動去搵嚟睇，慢慢理解，你可以選擇速食，但之後你好快就唔記得<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\nalphago 上年已經睇左啦，咁基本野睇完又要拎出嚟晒？<br />\n你無野係到貢獻就直接出去啦，留個讀乜鳩PhD 係到有咩意義？你又浪費時間，我又無得著<br />\n唯一推論咪就係有人想晒命，認叻囉<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\n我講我讀PhD係因為我批評人都要有理據 <span style=\"color: red;\">如果我係乜都唔識就話人就係我唔啱</span> <br />\n<br />\n你覺得係基本既paper唔代表其他人睇過，更加唔係咩晒<br />\n<br />\n我覺得雖然我對呢方面識得多過人，但我學得越多就覺得自己識得越少，所以唔覺得PhD係乜野晒命認叻</blockquote><br />\n你批評人既理據就係你讀phd?<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n邊個professor 教你？<br />\n我從來未見過一個professor 表達意見既時候提出既理據就係佢係professor 囉(支那除外)<br />\n<br />\n你d 咁既權威性人格，我好懷疑你做research 既能力囉</blockquote><br />\n我批評人既理據係因為我識相關既野 見紅字 學歷只係一個證明 <br />\n<br />\n駁返你你就人身攻擊 話咩邊個professor教你 權威性人格 懷疑research能力等等 <br />\n想問問權威性人格同research能力個關係係邊？<br />\n<br />\n雖然呢度係網上世界 大家唔知對方身份 但係都講互相尊重</blockquote>咁叫人生攻擊你好應該反省下自己一路睇住個post 之前已經有既bias 係幾大<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n我就唔晒時間係到答你權威性人格同research 能力個關係啦，我講你都未必有能力吸收/你既bias 根本就會懵閉你雙眼/個腦<br />\n自己睇paper 啦<br />\n</blockquote><br />\n條link 貼唔到<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n自己google scholar 啦<br />\nAuthoritarianism and its relation to creativity"},{"pid":"57bf66ddb84434a7a6afc914d916b712a983f429","tid":257808,"uid":12599,"like":5,"dislike":0,"score":5,"citedBy":0,"replyTime":"2017-05-26T01:43:35.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>留名<br />\n<br />\n喂樓上指出錯係可以<br />\n但紅字加句咩此post已完/留舊<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> 就算<br />\n真係冇乜建設性<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> <br />\n<br />\n可否都麻煩大家 指錯處都留低一兩句 錯咩、正確係咩？<br />\n<br />\n難得有認真學術post 唔好一野打沉人啦<br />\n喺連登開呢啲post好大壓力<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /></blockquote><br />\nd 人要扮勁無計<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n具體野唔會講得出，剩係話自己讀乜鳩PhD, 係邊到邊到做咁</blockquote><br />\n讀乜鳩PhD都叫扮勁，話人講唔出 <img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /><br />\n你睇就有份，下下都係要人由頭到尾講俾你講到明，其實網上都有唔少資訊有人寫關於佢既文章，一定講得比我好，而且我都冇咁既時候去寫長文解釋<br />\n<br />\nAlphaGo用既主要idea係deep reinforcement learning with Monte Carlo tree search, 結果同成個methodology係Nature上面發表左超過一年，如果你係大學生可以免費download嚟睇<br />\n<br />\n你想學最多既野，一定係自己主動去搵嚟睇，慢慢理解，你可以選擇速食，但之後你好快就唔記得<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\nalphago 上年已經睇左啦，咁基本野睇完又要拎出嚟晒？<br />\n你無野係到貢獻就直接出去啦，留個讀乜鳩PhD 係到有咩意義？你又浪費時間，我又無得著<br />\n唯一推論咪就係有人想晒命，認叻囉<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\n我講我讀PhD係因為我批評人都要有理據 <span style=\"color: red;\">如果我係乜都唔識就話人就係我唔啱</span> <br />\n<br />\n你覺得係基本既paper唔代表其他人睇過，更加唔係咩晒<br />\n<br />\n我覺得雖然我對呢方面識得多過人，但我學得越多就覺得自己識得越少，所以唔覺得PhD係乜野晒命認叻</blockquote><br />\n你批評人既理據就係你讀phd?<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n邊個professor 教你？<br />\n我從來未見過一個professor 表達意見既時候提出既理據就係佢係professor 囉(支那除外)<br />\n<br />\n你d 咁既權威性人格，我好懷疑你做research 既能力囉</blockquote><br />\n我批評人既理據係因為我識相關既野 見紅字 學歷只係一個證明 <br />\n<br />\n駁返你你就人身攻擊 話咩邊個professor教你 權威性人格 懷疑research能力等等 <br />\n想問問權威性人格同research能力個關係係邊？<br />\n<br />\n雖然呢度係網上世界 大家唔知對方身份 但係都講互相尊重</blockquote>咁叫人生攻擊你好應該反省下自己一路睇住個post 之前已經有既bias 係幾大<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n我就唔晒時間係到答你權威性人格同research 能力個關係啦，我講你都未必有能力吸收/你既bias 根本就會懵閉你雙眼/個腦<br />\n自己睇paper 啦<br />\n</blockquote><br />\n條link 貼唔到<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n自己google scholar 啦<br />\nAuthoritarianism and its relation to creativity</blockquote><br />\n講返ai啦講呢d有柒用咩"},{"pid":"8469d37dc5177b820b546e691f52cf2c6751ff28","tid":257808,"uid":90938,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T01:50:35.000Z","msg":"留名"},{"pid":"3b4f9a732ace8d06242e8da53f14ee0e282b9355","tid":257808,"uid":9248,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T01:52:02.000Z","msg":"<a href=\"https://lihkg.com/thread/257709/page/3?ref=ios\" data-auto-link target=\"_blank\">https://lihkg.com/thread/257709/page/3?ref=ios</a><br />\n<br />\n研發Alpha Go嘅人真係會知自己部機個極限係咩？<br />\n佢話柯潔頭50步接近完美 <br />\n但又代表即使柯潔無犯錯Alpha Go都可能會輸<br />\n會唔會事實係AlphaGo本身都係plan緊幾十步之後去拆解個局？<br />\n<br />\n定係純粹係佢謙虛咁講？"},{"pid":"f2f203c45d33d07c89a6f0eb3cdea7d19239cb6d","tid":257808,"uid":63846,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T01:54:12.000Z","msg":"<blockquote><a href=\"https://lihkg.com/thread/257709/page/3?ref=ios\" target=\"_blank\">https://lihkg.com/thread/257709/page/3?ref=ios</a><br />\n<br />\n研發Alpha Go嘅人真係會知自己部機個極限係咩？<br />\n佢話柯潔頭50步接近完美 <br />\n但又代表即使柯潔無犯錯Alpha Go都可能會輸<br />\n會唔會事實係AlphaGo本身都係plan緊幾十步之後去拆解個局？<br />\n<br />\n定係純粹係佢謙虛咁講？</blockquote><br />\n「完美」的意思係個network eval出黎最高勝率的move一樣，而唔係game theory入面的optimal strategy"},{"pid":"1718f46dae4e3ba123496635b5a63a985e6cf899","tid":257808,"uid":48724,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T01:55:19.000Z","msg":"此回覆已被刪除"},{"pid":"1358e4110ce3cdd7b7d8ffa196429d137568a12c","tid":257808,"uid":21699,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T01:55:32.000Z","msg":"<blockquote><br />\n前者只係簡單既regression, machine learning一早已經做緊<br />\n而後者，其實都只係multiple of multilayer neural network， 將所有方面既知識串連埋一齊再得出答案<br />\n你會話，唔係喎，人不斷重覆諗同一個問題，會有時可以創新到方法，而ai 每次都會得出同一個答案。但其實只係人會每次都evaluate 自己個答案既滿意度再feedback 番去自己個neural network 到做training。 而machine learning 一般唔會咁做只係想個result controllable, 同埋無咁易俾d poison (有意/無意)data 影響，而留待下一次做batch training 先version upgrade <br />\n而且其實呢個係人既弱點，因為好易俾人洗腦，只要不斷重覆講一個大話一百次，一千次，人就會不知不覺間信左<br />\n<br />\n人之所以好似好強大，創新力比ai 強係因為人真係有個非常大既neural network,每個cluster of nodes 都掌管唔同知識，但卻可以同時對所有input 作出反應，而且係每次input 都自動調整每個cluster of nodes 既weight<br />\n目前黎講machine learning 仲未有咁既computing power 去模擬呢樣野<br />\n或者咁講，其實可能已經有咁既computing power, 但一般由於效率比單一用途既machine learning低，暫時未有人倒足夠既錢落海去做呢樣野<br />\n其實見到google 搞cloud 就好明顯係想做呢樣野，因為佢想偷d 用家用剩既computation time 黎用， 成本基本上係cloud 既用家俾，但只要cloud 用家越黎越多，google 賺到既computing power 就越多</blockquote><br />\n<br />\n呢個真係現有computing techniques既缺點。要update variables，電腦實質做既野就係serial operations，逐個逐個咁黎，最多係快到好似同時咁。但人腦唔同，可能望一眼一舊蘋果，已經同時有(100 billions)^n 個long term potentiation/depression進行緊"},{"pid":"13fdd08de44e4fe56c78e634ffca4c8f1afe0599","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T01:56:41.000Z","msg":"<blockquote><a href=\"https://lihkg.com/thread/257709/page/3?ref=ios\" target=\"_blank\">https://lihkg.com/thread/257709/page/3?ref=ios</a><br />\n<br />\n研發Alpha Go嘅人真係會知自己部機個極限係咩？<br />\n佢話柯潔頭50步接近完美 <br />\n但又代表即使柯潔無犯錯Alpha Go都可能會輸<br />\n會唔會事實係AlphaGo本身都係plan緊幾十步之後去拆解個局？<br />\n<br />\n定係純粹係佢謙虛咁講？</blockquote><br />\n我諗係因為alpha go可以output一個predict自己贏既probi 而頭50move佢都predict自己勝率唔高"},{"pid":"9e939162bd37ea4be36a2bc2e47736676493a853","tid":257808,"uid":48679,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T01:58:56.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>留名<br />\n<br />\n喂樓上指出錯係可以<br />\n但紅字加句咩此post已完/留舊<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> 就算<br />\n真係冇乜建設性<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> <br />\n<br />\n可否都麻煩大家 指錯處都留低一兩句 錯咩、正確係咩？<br />\n<br />\n難得有認真學術post 唔好一野打沉人啦<br />\n喺連登開呢啲post好大壓力<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /></blockquote><br />\nd 人要扮勁無計<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n具體野唔會講得出，剩係話自己讀乜鳩PhD, 係邊到邊到做咁</blockquote><br />\n讀乜鳩PhD都叫扮勁，話人講唔出 <img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /><br />\n你睇就有份，下下都係要人由頭到尾講俾你講到明，其實網上都有唔少資訊有人寫關於佢既文章，一定講得比我好，而且我都冇咁既時候去寫長文解釋<br />\n<br />\nAlphaGo用既主要idea係deep reinforcement learning with Monte Carlo tree search, 結果同成個methodology係Nature上面發表左超過一年，如果你係大學生可以免費download嚟睇<br />\n<br />\n你想學最多既野，一定係自己主動去搵嚟睇，慢慢理解，你可以選擇速食，但之後你好快就唔記得<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\nalphago 上年已經睇左啦，咁基本野睇完又要拎出嚟晒？<br />\n你無野係到貢獻就直接出去啦，留個讀乜鳩PhD 係到有咩意義？你又浪費時間，我又無得著<br />\n唯一推論咪就係有人想晒命，認叻囉<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\n我講我讀PhD係因為我批評人都要有理據 <span style=\"color: red;\">如果我係乜都唔識就話人就係我唔啱</span> <br />\n<br />\n你覺得係基本既paper唔代表其他人睇過，更加唔係咩晒<br />\n<br />\n我覺得雖然我對呢方面識得多過人，但我學得越多就覺得自己識得越少，所以唔覺得PhD係乜野晒命認叻</blockquote><br />\n你批評人既理據就係你讀phd?<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n邊個professor 教你？<br />\n我從來未見過一個professor 表達意見既時候提出既理據就係佢係professor 囉(支那除外)<br />\n<br />\n你d 咁既權威性人格，我好懷疑你做research 既能力囉</blockquote><br />\n我批評人既理據係因為我識相關既野 見紅字 學歷只係一個證明 <br />\n<br />\n駁返你你就人身攻擊 話咩邊個professor教你 權威性人格 懷疑research能力等等 <br />\n想問問權威性人格同research能力個關係係邊？<br />\n<br />\n雖然呢度係網上世界 大家唔知對方身份 但係都講互相尊重</blockquote>咁叫人生攻擊你好應該反省下自己一路睇住個post 之前已經有既bias 係幾大<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n我就唔晒時間係到答你權威性人格同research 能力個關係啦，我講你都未必有能力吸收/你既bias 根本就會懵閉你雙眼/個腦<br />\n自己睇paper 啦<br />\n</blockquote><br />\n條link 貼唔到<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n自己google scholar 啦<br />\nAuthoritarianism and its relation to creativity</blockquote><br />\n講返ai啦講呢d有柒用咩</blockquote><br />\n我都專登帶出左個同ai 有關既話題架，就係bias<br />\n而相關既題目仲有under/over fitting<br />\n不過我剩係想討論，無乜心機講書，呢part 都係等樓主黎"},{"pid":"fd47ce5a7bc88b1525ce1e39d40281d7d3bdf8aa","tid":257808,"uid":63846,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T01:59:19.000Z","msg":"<blockquote><blockquote><a href=\"https://lihkg.com/thread/257709/page/3?ref=ios\" target=\"_blank\">https://lihkg.com/thread/257709/page/3?ref=ios</a><br />\n<br />\n研發Alpha Go嘅人真係會知自己部機個極限係咩？<br />\n佢話柯潔頭50步接近完美 <br />\n但又代表即使柯潔無犯錯Alpha Go都可能會輸<br />\n會唔會事實係AlphaGo本身都係plan緊幾十步之後去拆解個局？<br />\n<br />\n定係純粹係佢謙虛咁講？</blockquote><br />\n我諗係因為alpha go可以output一個predict自己贏既probi 而頭50move佢都predict自己勝率唔高</blockquote><br />\n應該係盤轉角色係人的個方eval出黎個move用柯潔行的一樣"},{"pid":"b7293d8c27689527bc3aadaac76fe3b4712da23f","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T01:59:31.000Z","msg":"<blockquote><blockquote><blockquote><blockquote>我地見到一個人既樣既舜間，無意中某啲神經網絡就會有所活動，而當中所牽涉既神經元都有機會修改自身對所接收訊號之強弱既調節。即係話 S 對 N 既加減唔一定係每次都一樣，有機會變。呢個性質引致到如果我地每日都見一次同一個人，久而久之訊號既傳遞係呢啲網絡之間就會形成一種 Pattern，即是我地既記憶。<br />\n<br />\n今日寫住咁多先 ...</blockquote><br />\n一開始已經concept錯<img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /></blockquote><br />\nthat is based on findings of Neuroscience<br />\n<br />\n&quot;When neuroscientists use functional magnetic resonance imaging to see how a monkey&rsquo;s brain responds to familiar faces, something odd happens. When shown a familiar face, a monkey&rsquo;s brain lights up, not in a specific area, but in nine different ones. <strong>Neuroscientists call these areas &ldquo;face patches&rdquo; and think they are neural networks with the specialised functions associated with face recognition.</strong>&quot;<br />\n<br />\n<a href=\"https://www.technologyreview.com/s/535176/human-face-recognition-found-in-neural-network-based-on-monkey-brains/\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.technologyreview.com%2Fs%2F535176%2Fhuman-face-recognition-found-in-neural-network-based-on-monkey-brains%2F&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=68e68ad2\" target=\"_blank\">https://www.technologyreview.com/s/535176/human-face-recognition-found-in-neural-network-based-on-monkey-brains/</a></blockquote><br />\n<br />\n個fMRI連test specimen都唔同，同人腦好大分別。搵倒既pattern係馬騮既CNN，唔係人既記憶</blockquote><br />\n認人樣據我所知同compress sensing有關<br />\n<br />\n有人做過實驗係根據人認樣果時eyemovement 而搵出認人樣會搵邊幾d pixel<br />\n<br />\n詳細要搵返份paper<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\nSor1999<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"153c7e248ba85002d3191be40f1830e49200c50e","tid":257808,"uid":48679,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T02:03:40.000Z","msg":"<blockquote><blockquote><blockquote><a href=\"https://lihkg.com/thread/257709/page/3?ref=ios\" target=\"_blank\">https://lihkg.com/thread/257709/page/3?ref=ios</a><br />\n<br />\n研發Alpha Go嘅人真係會知自己部機個極限係咩？<br />\n佢話柯潔頭50步接近完美 <br />\n但又代表即使柯潔無犯錯Alpha Go都可能會輸<br />\n會唔會事實係AlphaGo本身都係plan緊幾十步之後去拆解個局？<br />\n<br />\n定係純粹係佢謙虛咁講？</blockquote><br />\n我諗係因為alpha go可以output一個predict自己贏既probi 而頭50move佢都predict自己勝率唔高</blockquote><br />\n應該係盤轉角色係人的個方eval出黎個move用柯潔行的一樣</blockquote><br />\n唔會咁巧合，個棋盤咁大，一開始幾步棋可以達至勝率相似甚至一樣既地方太多"},{"pid":"6709aa981fb01ed3549c36e963acb0dbaff5604e","tid":257808,"uid":48724,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-26T02:06:58.000Z","msg":"此回覆已被刪除"},{"pid":"7bcd52164152b54dcb17e297c36a6b8f190aaedb","tid":257808,"uid":2211,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T02:10:24.000Z","msg":"樓主<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"2de4f8d53fd4e4d65a758cabc9888b3db2b1e6bb","tid":257808,"uid":54876,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T02:20:58.000Z","msg":"<img src=\"https://xkcd.com/1838/\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fxkcd.com%2F1838%2F&h=5610d1e3&s={SIZE}\" />"},{"pid":"4983686ab3a8fdb31937592440d604d4b4c63bfc","tid":257808,"uid":54876,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T02:21:12.000Z","msg":"<a href=\"https://xkcd.com/1838/\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fxkcd.com%2F1838%2F&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=ffc5be82\" target=\"_blank\">https://xkcd.com/1838/</a>"},{"pid":"bc33934a89a3e05be9a5128db4ecec99709030b6","tid":257808,"uid":51817,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T02:23:57.000Z","msg":"lm"},{"pid":"5d03c62faec8d0ff3dbd9fc3aa6d80f00d5b16f9","tid":257808,"uid":28279,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T02:24:05.000Z","msg":"<blockquote><a href=\"https://xkcd.com/1838/\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fxkcd.com%2F1838%2F&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=ffc5be82\" target=\"_blank\">https://xkcd.com/1838/</a></blockquote><br />\n<img src=\"/assets/faces/lomoji/37.png\" class=\"hkgmoji\" /> 每個ai post都貼呢篇漫畫"},{"pid":"89478713a7e53c3f9d1e286e018ab6b986e2e0dd","tid":257808,"uid":55598,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T02:37:48.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>留名<br />\n<br />\n喂樓上指出錯係可以<br />\n但紅字加句咩此post已完/留舊<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> 就算<br />\n真係冇乜建設性<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> <br />\n<br />\n可否都麻煩大家 指錯處都留低一兩句 錯咩、正確係咩？<br />\n<br />\n難得有認真學術post 唔好一野打沉人啦<br />\n喺連登開呢啲post好大壓力<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /></blockquote><br />\nd 人要扮勁無計<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n具體野唔會講得出，剩係話自己讀乜鳩PhD, 係邊到邊到做咁</blockquote><br />\n讀乜鳩PhD都叫扮勁，話人講唔出 <img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /><br />\n你睇就有份，下下都係要人由頭到尾講俾你講到明，其實網上都有唔少資訊有人寫關於佢既文章，一定講得比我好，而且我都冇咁既時候去寫長文解釋<br />\n<br />\nAlphaGo用既主要idea係deep reinforcement learning with Monte Carlo tree search, 結果同成個methodology係Nature上面發表左超過一年，如果你係大學生可以免費download嚟睇<br />\n<br />\n你想學最多既野，一定係自己主動去搵嚟睇，慢慢理解，你可以選擇速食，但之後你好快就唔記得<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\nalphago 上年已經睇左啦，咁基本野睇完又要拎出嚟晒？<br />\n你無野係到貢獻就直接出去啦，留個讀乜鳩PhD 係到有咩意義？你又浪費時間，我又無得著<br />\n唯一推論咪就係有人想晒命，認叻囉<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\n我講我讀PhD係因為我批評人都要有理據 如果我係乜都唔識就話人就係我唔啱 <br />\n<br />\n你覺得係基本既paper唔代表其他人睇過，更加唔係咩晒<br />\n<br />\n我覺得雖然我對呢方面識得多過人，但我學得越多就覺得自己識得越少，所以唔覺得PhD係乜野晒命認叻</blockquote><br />\n你批評人既理據就係你讀phd?<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n邊個professor 教你？<br />\n我從來未見過一個professor 表達意見既時候提出既理據就係佢係professor 囉(支那除外)<br />\n<br />\n你d 咁既權威性人格，我好懷疑你做research 既能力囉</blockquote><br />\n<br />\n多口講句，你曲解咗人哋意思。佢好明顯講緊話自己讀PhD係想識多啲嘢，從而有咁嘅知識底可以評論姐。。"},{"pid":"44f63c074814e0f876407d66935412bb5255ac75","tid":257808,"uid":48724,"like":0,"dislike":1,"score":-1,"citedBy":0,"replyTime":"2017-05-26T02:39:22.000Z","msg":"此回覆已被刪除"},{"pid":"983dd29a7d64455d665be55deeb96f2bd7855d7a","tid":257808,"uid":29643,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T02:41:52.000Z","msg":"留名"},{"pid":"f43c317abd4586b9c09176937247a7e11ab396b8","tid":257808,"uid":55598,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-26T02:44:01.000Z","msg":"我建議樓主由multilayer perceptron 講起，要了解ai 嘅話咁樣係最最最簡單，CNN都唔係個個知道做緊乜嘅。<br />\n<br />\n同埋人腦個part亦可以唔講嘅（講左會得意啲），只係neural network最初果種諗法起源，同佢點function又係另一回事。<br />\n<br />\n<img src=\"/assets/faces/normal/hoho.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/hoho.gif\" class=\"hkgmoji\" />"},{"pid":"2ef24d7ec13f618f817ac139c6bfaf37b588185b","tid":257808,"uid":5218,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T02:45:48.000Z","msg":"( Show Blocked User - 老舉唔怕柒大 )"},{"pid":"b5af33ad40de1cb542d2d301d2080535175216e0","tid":257808,"uid":78678,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-26T02:47:07.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><br />\nd 人要扮勁無計<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n具體野唔會講得出，剩係話自己讀乜鳩PhD, 係邊到邊到做咁</blockquote><br />\n讀乜鳩PhD都叫扮勁，話人講唔出 <img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /><br />\n你睇就有份，下下都係要人由頭到尾講俾你講到明，其實網上都有唔少資訊有人寫關於佢既文章，一定講得比我好，而且我都冇咁既時候去寫長文解釋<br />\n<br />\nAlphaGo用既主要idea係deep reinforcement learning with Monte Carlo tree search, 結果同成個methodology係Nature上面發表左超過一年，如果你係大學生可以免費download嚟睇<br />\n<br />\n你想學最多既野，一定係自己主動去搵嚟睇，慢慢理解，你可以選擇速食，但之後你好快就唔記得<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\nalphago 上年已經睇左啦，咁基本野睇完又要拎出嚟晒？<br />\n你無野係到貢獻就直接出去啦，留個讀乜鳩PhD 係到有咩意義？你又浪費時間，我又無得著<br />\n唯一推論咪就係有人想晒命，認叻囉<br />\n<br />\n如果有咩得罪，係度講句唔好意思</blockquote><br />\n我講我讀PhD係因為我批評人都要有理據 如果我係乜都唔識就話人就係我唔啱 <br />\n<br />\n你覺得係基本既paper唔代表其他人睇過，更加唔係咩晒<br />\n<br />\n我覺得雖然我對呢方面識得多過人，但我學得越多就覺得自己識得越少，所以唔覺得PhD係乜野晒命認叻</blockquote><br />\n你批評人既理據就係你讀phd?<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n邊個professor 教你？<br />\n我從來未見過一個professor 表達意見既時候提出既理據就係佢係professor 囉(支那除外)<br />\n<br />\n你d 咁既權威性人格，我好懷疑你做research 既能力囉</blockquote><br />\n<br />\n多口講句，你曲解咗人哋意思。佢好明顯講緊話自己讀PhD係想識多啲嘢，從而有咁嘅知識底可以評論姐。。</blockquote><br />\n<br />\n利申agent仔一名</blockquote><br />\nResearch友唔洗同普通人嘈啦<img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /> <br />\n大家水平都唔同<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" />"},{"pid":"0845e3839e5a3003779dc27697e9f067cbd856a9","tid":257808,"uid":55598,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-26T02:48:50.000Z","msg":"<blockquote>CNN 最強大係個C字<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /><br />\n<br />\n明嘅就會明<br />\n<br />\n利申: 玩過下mnist, 想應用係medical方面但prof唔想我咁做<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n而家乜鬼都用 neural network，個人嚟講真係唔係幾中意<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> 雖然設計個 architecture係好花功夫，亦有好多additional layer去令到 performance 更加好，但我硬係覺得，呢樣嘢係一個 black box，人類嘅智慧係無限嘅data入邊真係不值一提，好似諗到咩新嘢，都唔會夠 neural network 嚟咁<img src=\"/assets/faces/fs/want_die.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/fs/want_die.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/fs/want_die.gif\" class=\"hkgmoji\" />"},{"pid":"5d32090f7eabe1adf7347abb9134ac8a285a1c4a","tid":257808,"uid":14350,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T03:11:01.000Z","msg":"<img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" />"},{"pid":"6e136f1fa2469e231db5c1948de48b61b341483d","tid":257808,"uid":4934,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T03:29:59.000Z","msg":"少見既認真討論<img src=\"/assets/faces/normal/oh.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"1169c8f94078439dbfdda627837b130cc181e285","tid":257808,"uid":75112,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T03:31:01.000Z","msg":"我明Monte Carlo Tree Search<br />\n但係有無人可以解釋下點解個Convolution Network可以做到人類個種intuition？"},{"pid":"99f924cf4976e539a31b27ce9038167ff49dac0f","tid":257808,"uid":70460,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T03:34:40.000Z","msg":"LM"},{"pid":"0a7d8a46bd30674ef31ffdfb492e70525dde6ee9","tid":257808,"uid":12599,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-26T03:34:49.000Z","msg":"<blockquote>我明Monte Carlo Tree Search<br />\n但係有無人可以解釋下點解個Convolution Network可以做到人類個種intuition？</blockquote><br />\nCNN 可以best fit visual features, 而且呢d visual feature可以做到極abstract"},{"pid":"394e25123d611b541112a0d6b6fa0e5210e67d1f","tid":257808,"uid":43570,"like":5,"dislike":0,"score":5,"citedBy":0,"replyTime":"2017-05-26T03:46:24.000Z","msg":"<blockquote><br />\n前者只係簡單既regression, machine learning一早已經做緊<br />\n而後者，其實都只係multiple of multilayer neural network， 將所有方面既知識串連埋一齊再得出答案<br />\n你會話，唔係喎，人不斷重覆諗同一個問題，會有時可以創新到方法，而ai 每次都會得出同一個答案。但其實只係人會每次都evaluate 自己個答案既滿意度再feedback 番去自己個neural network 到做training。 而machine learning 一般唔會咁做只係想個result controllable, 同埋無咁易俾d poison (有意/無意)data 影響，而留待下一次做batch training 先version upgrade <br />\n而且其實呢個係人既弱點，因為好易俾人洗腦，只要不斷重覆講一個大話一百次，一千次，人就會不知不覺間信左<br />\n<br />\n人之所以好似好強大，創新力比ai 強係因為人真係有個非常大既neural network,每個cluster of nodes 都掌管唔同知識，但卻可以同時對所有input 作出反應，而且係每次input 都自動調整每個cluster of nodes 既weight<br />\n目前黎講machine learning 仲未有咁既computing power 去模擬呢樣野<br />\n或者咁講，其實可能已經有咁既computing power, 但一般由於效率比單一用途既machine learning低，暫時未有人倒足夠既錢落海去做呢樣野<br />\n其實見到google 搞cloud 就好明顯係想做呢樣野，因為佢想偷d 用家用剩既computation time 黎用， 成本基本上係cloud 既用家俾，但只要cloud 用家越黎越多，google 賺到既computing power 就越多</blockquote><br />\n其實咁樣諗係將人腦睇得太簡單啦。<br />\n<br />\n第一，neural network training其實係好唔efficient, 先唔講deep learning 要百萬sample，就算係一個普通MNIST dataset用普通multilayer perceptron, 都要backpropagate N 次先training到個network. 但人唔先睇一百萬張車嘅相都好易識別到玩具車，真車，漫畫車嘅分別。另一例子，就係語言，而家machine translation又係幾百萬字文本，但人呢？可能跟住本幾百頁嘅書學一兩年都可以明白大概。人類嘅智力唔係簡單嘅input-output mapping, 而係可以理解組成部份之間的闗係，並且加以再組合。有啲似GAN, 但當然比GAN強好多。人腦係parameter極高(無數synapse) 而資料相對極少的情況下，都可以唔overfit, 呢個先係佢勁嘅地方。同埋人腦係transfer learning 方面又係勁到離譜。deep learning 係data 少嘅情況下甚至比唔上一個簡單 linear discriminant analysis.<br />\n<br />\n第二，究竟「創意」係咪一句error feedback就可以解釋晒係好有疑問。alphago 之以受到觸目並唔係單單因為棋力強，而係佢係某個程度上展現出「直覺」同「創意」。但呢種「創意」可唔可以應用係其它方尚係未知之數。例如，係冇明顯對錯作為feedback的情況下，佢可唔可以展現創意？「創意」並唔係純綷隨機，而係「意料之外，情理之中」。咁就必然包含一個evaluate function 去判斷output, 但對於不是非黑即白的結果，例如寫作，對話等等，AI仍然有好多改進空間。<br />\n<br />\n第三，而家AI玩緊嘅，全部都係perfect information, deterministic game. 但如果結果係有隨機或者未知性呢？呢方面AI都係有侍進步。deepmind 想玩starcraft就係呢個原因。starcraft有隨機同埋未知性，圍棋冇。<br />\n<br />\n最後我想講近來有一個唔係咁好嘅趨勢，就係好多人想用artificial neural network去理解人腦係點運作。當然ANN一開始係有少少借鍳人腦，但人腦運作其實比我地想像複雜好多，神經元嘅output 亦未必一定係簡單一個weighted sum. 佢亦都可以做spatial-temporal 嘅運算。我都唔去講記憶，情感，意識呢啲複雜野啦。事實上除左ANN, 仲有好多其它AI技術，例如graphical models, decision tree etc.，較果唔一定比deep learning差。 XGBoost就係用decision tree, 係kaggle 到贏到開巷。我個人就覺得graphical models 係比較接近人腦嘅方式，咁當然事實上人腦可能係咁同model都有啲都未定。"},{"pid":"905e00eb0cdbccb7dd9ffa1a6ace489270ee1dee","tid":257808,"uid":43570,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T03:59:33.000Z","msg":"<blockquote><blockquote>CNN 最強大係個C字<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /><br />\n<br />\n明嘅就會明<br />\n<br />\n利申: 玩過下mnist, 想應用係medical方面但prof唔想我咁做<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n而家乜鬼都用 neural network，個人嚟講真係唔係幾中意<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> 雖然設計個 architecture係好花功夫，亦有好多additional layer去令到 performance 更加好，但我硬係覺得，呢樣嘢係一個 black box，人類嘅智慧係無限嘅data入邊真係不值一提，好似諗到咩新嘢，都唔會夠 neural network 嚟咁<img src=\"/assets/faces/fs/want_die.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/fs/want_die.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/fs/want_die.gif\" class=\"hkgmoji\" /></blockquote><br />\n好同意巴打講法。而家差唔多咩問題掉個CNN, RNN都好似即刻勁過晒之前啲方法。但啲neural network成個black box, 佢用左啲咩feature 根本人好難理解。究竟對人類觸決問題係咪真係有幫助，真係好難講。"},{"pid":"733c7892787c7e80a39c644b723b8cf0e7bd60ef","tid":257808,"uid":82234,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T04:06:34.000Z","msg":"lm"},{"pid":"28fbc7e8e32f1b5155a17cc5d8347e1d89e26124","tid":257808,"uid":52543,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T04:08:09.000Z","msg":"好多CS JJ"},{"pid":"e64774e92515207f464112a920309b63c2317679","tid":257808,"uid":48724,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T04:27:20.000Z","msg":"此回覆已被刪除"},{"pid":"97a98c0ea14b25028201bba7561ffa877f6a47cd","tid":257808,"uid":28279,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T04:30:52.000Z","msg":"<blockquote><blockquote><br />\n前者只係簡單既regression, machine learning一早已經做緊<br />\n而後者，其實都只係multiple of multilayer neural network， 將所有方面既知識串連埋一齊再得出答案<br />\n你會話，唔係喎，人不斷重覆諗同一個問題，會有時可以創新到方法，而ai 每次都會得出同一個答案。但其實只係人會每次都evaluate 自己個答案既滿意度再feedback 番去自己個neural network 到做training。 而machine learning 一般唔會咁做只係想個result controllable, 同埋無咁易俾d poison (有意/無意)data 影響，而留待下一次做batch training 先version upgrade <br />\n而且其實呢個係人既弱點，因為好易俾人洗腦，只要不斷重覆講一個大話一百次，一千次，人就會不知不覺間信左<br />\n<br />\n人之所以好似好強大，創新力比ai 強係因為人真係有個非常大既neural network,每個cluster of nodes 都掌管唔同知識，但卻可以同時對所有input 作出反應，而且係每次input 都自動調整每個cluster of nodes 既weight<br />\n目前黎講machine learning 仲未有咁既computing power 去模擬呢樣野<br />\n或者咁講，其實可能已經有咁既computing power, 但一般由於效率比單一用途既machine learning低，暫時未有人倒足夠既錢落海去做呢樣野<br />\n其實見到google 搞cloud 就好明顯係想做呢樣野，因為佢想偷d 用家用剩既computation time 黎用， 成本基本上係cloud 既用家俾，但只要cloud 用家越黎越多，google 賺到既computing power 就越多</blockquote><br />\n其實咁樣諗係將人腦睇得太簡單啦。<br />\n<br />\n第一，neural network training其實係好唔efficient, 先唔講deep learning 要百萬sample，就算係一個普通MNIST dataset用普通multilayer perceptron, 都要backpropagate N 次先training到個network. 但人唔先睇一百萬張車嘅相都好易識別到玩具車，真車，漫畫車嘅分別。另一例子，就係語言，而家machine translation又係幾百萬字文本，但人呢？可能跟住本幾百頁嘅書學一兩年都可以明白大概。人類嘅智力唔係簡單嘅input-output mapping, 而係可以理解組成部份之間的闗係，並且加以再組合。有啲似GAN, 但當然比GAN強好多。人腦係parameter極高(無數synapse) 而資料相對極少的情況下，都可以唔overfit, 呢個先係佢勁嘅地方。同埋人腦係transfer learning 方面又係勁到離譜。deep learning 係data 少嘅情況下甚至比唔上一個簡單 linear discriminant analysis.<br />\n<br />\n第二，究竟「創意」係咪一句error feedback就可以解釋晒係好有疑問。alphago 之以受到觸目並唔係單單因為棋力強，而係佢係某個程度上展現出「直覺」同「創意」。但呢種「創意」可唔可以應用係其它方尚係未知之數。例如，係冇明顯對錯作為feedback的情況下，佢可唔可以展現創意？「創意」並唔係純綷隨機，而係「意料之外，情理之中」。咁就必然包含一個evaluate function 去判斷output, 但對於不是非黑即白的結果，例如寫作，對話等等，AI仍然有好多改進空間。<br />\n<br />\n第三，而家AI玩緊嘅，全部都係perfect information, deterministic game. 但如果結果係有隨機或者未知性呢？呢方面AI都係有侍進步。deepmind 想玩starcraft就係呢個原因。starcraft有隨機同埋未知性，圍棋冇。<br />\n<br />\n最後我想講近來有一個唔係咁好嘅趨勢，就係好多人想用artificial neural network去理解人腦係點運作。當然ANN一開始係有少少借鍳人腦，但人腦運作其實比我地想像複雜好多，神經元嘅output 亦未必一定係簡單一個weighted sum. 佢亦都可以做spatial-temporal 嘅運算。我都唔去講記憶，情感，意識呢啲複雜野啦。事實上除左ANN, 仲有好多其它AI技術，例如graphical models, decision tree etc.，較果唔一定比deep learning差。 XGBoost就係用decision tree, 係kaggle 到贏到開巷。我個人就覺得graphical models 係比較接近人腦嘅方式，咁當然事實上人腦可能係咁同model都有啲都未定。</blockquote><br />\n關於第三，唔係喎，我記得早排好似有新聞話隊咗個ai去打德州撲克"},{"pid":"881c4382c716aa5c862e47008ff0cb7068547376","tid":257808,"uid":48679,"like":1,"dislike":1,"score":0,"citedBy":0,"replyTime":"2017-05-26T04:39:47.000Z","msg":"太長，唔quote 無夢人<br />\n一。你會唔會太睇高左人腦呢？其實人腦好多時都好唔efficient,要分玩具車，真車，漫畫車係咪真係幾張相就得呢？你可以試下俾一個細佬仔去分，依家d 日日對住部iPad 既細佬除時分唔到現實同二次元，你作為一個成年人能夠輕易分辨到係因為你已經係過去睇過無數既真車，玩具車，漫畫車，而且唔止係咁，你仲睇過好多真船，玩具船，漫畫船，真實環境，玩具場境，漫畫背景,etc<br />\n你有非常多既feature 去分辨真，玩具，漫畫，而且仲係以年為單位既training data<br />\n人腦既強大之處係在於其實每次input 都係將五感所感受到既野同時輸入，而且仲會對前後場景同時作出分析，預估完先會對現在呢一刻既問題作出回答，所有種類既問題都係同一個neural network 作出回答。 唔知你有無試過突然聽唔到人地講野成句句子入面既某一個詞語，對方重覆幾次都仲係聽唔到，呢種情況特別容易發生係你唔熟悉既topic。因為人腦唔單單係聽果個詞語既音頻黎分析，重會跟對象，前文後理黎判斷。所以當對方講一個唔係你expect 範圍內既詞語你會突然理解不能(雖然果個詞語單獨黎講你係識既)<br />\n<br />\n人腦其實係一個極典型under fitting 既例子，所以人腦對一d noise 極少甚至無noise 既問題 efficiency 係極低(e.g. 數學運算)<br />\n<br />\n而人腦真正強大既係power efficiency, 用極少能量已經可以做到極大量計算<br />\n<br />\n人腦同ai 各有好處，無乜必要去誇大人腦既強項"},{"pid":"7d286cf3ebed019ea97172b7614cc2f298c1764a","tid":257808,"uid":81443,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T04:43:17.000Z","msg":"im"},{"pid":"d3c3d966457ddff42923021061168471cc4ad5bd","tid":257808,"uid":103983,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T04:46:29.000Z","msg":"劫已騎"},{"pid":"05e3b799af1f77932f4cf875515df6eebe32e0f9","tid":257808,"uid":62514,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T04:54:27.000Z","msg":"好多高手<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"5c9104bcc5c33e6f2ca36c1482df5ab051a8339a","tid":257808,"uid":55598,"like":2,"dislike":0,"score":2,"citedBy":0,"replyTime":"2017-05-26T04:58:00.000Z","msg":"<blockquote>太長，唔quote 無夢人<br />\n一。你會唔會太睇高左人腦呢？其實人腦好多時都好唔efficient,要分玩具車，真車，漫畫車係咪真係幾張相就得呢？你可以試下俾一個細佬仔去分，依家d 日日對住部iPad 既細佬除時分唔到現實同二次元，你作為一個成年人能夠輕易分辨到係因為你已經係過去睇過無數既真車，玩具車，漫畫車，而且唔止係咁，你仲睇過好多真船，玩具船，漫畫船，真實環境，玩具場境，漫畫背景,etc<br />\n你有非常多既feature 去分辨真，玩具，漫畫，而且仲係以年為單位既training data<br />\n人腦既強大之處係在於其實每次input 都係將五感所感受到既野同時輸入，而且仲會對前後場景同時作出分析，預估完先會對現在呢一刻既問題作出回答，所有種類既問題都係同一個neural network 作出回答。 唔知你有無試過突然聽唔到人地講野成句句子入面既某一個詞語，對方重覆幾次都仲係聽唔到，呢種情況特別容易發生係你唔熟悉既topic。因為人腦唔單單係聽果個詞語既音頻黎分析，重會跟對象，前文後理黎判斷。所以當對方講一個唔係你expect 範圍內既詞語你會突然理解不能(雖然果個詞語單獨黎講你係識既)<br />\n<br />\n人腦其實係一個極典型under fitting 既例子，所以人腦對一d noise 極少甚至無noise 既問題 efficiency 係極低(e.g. 數學運算)<br />\n<br />\n而人腦真正強大既係power efficiency, 用極少能量已經可以做到極大量計算<br />\n<br />\n人腦同ai 各有好處，無乜必要去誇大人腦既強項</blockquote><br />\n<br />\n我絕對唔會話佢睇高咗人腦。<br />\n<br />\n你諗下，電腦去分咩車咩車，training data閒閒地都幾萬架車，當你25歲，我哋有冇見過幾萬架車？我就覺得唔會有啦。仲有，我哋即時見過一個人一面，下次見番嘅時候都可以認得出條友，電腦就難啲，training sample 得一個其實乜都做唔到。<br />\n<br />\n係好多範疇底下ai都係未贏到人腦，不過的確係進步得極快，例如computer vision個邊嘅 classification, detection problem每年幾個% accuracy 咁上。例如係一張圖入邊有馬路有車有乜咁樣，要認晒佢哋呢個task其實係好撚柒難，但係而家嘅電腦已經好撚屈機：<br />\n<br />\n<a href=\"https://www.youtube.com/watch?v=VOC3huqHrss\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DVOC3huqHrss&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=23d52d4d\" data-auto-link target=\"_blank\">https://www.youtube.com/watch?v=VOC3huqHrss</a><br />\n<br />\n至於你講果種係人腦用咗 high level knowledge，machine有無類似？long short term memory 你又覺得算唔算？<br />\n<br />\n我會咁講，有無限嘅data，電腦一定贏人腦。"},{"pid":"17e1ac6bc6ce729511fae2cc85b77f8dff7cfc1f","tid":257808,"uid":2404,"like":0,"dislike":1,"score":-1,"citedBy":0,"replyTime":"2017-05-26T05:08:23.000Z","msg":"前員工<img src=\"/assets/faces/normal/surprise.gif\" class=\"hkgmoji\" /> <br />\n樓主係咪畀自己寫既AI搶咗飯碗？"},{"pid":"e29576ca0e1cd4b4bef4e7b2e2cd479b3e241ae5","tid":257808,"uid":3307,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T05:22:07.000Z","msg":"Alphago篇paper<br />\n<a href=\"https://gogameguru.com/i/2016/03/deepmind-mastering-go.pdf\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fgogameguru.com%2Fi%2F2016%2F03%2Fdeepmind-mastering-go.pdf&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=4558bfbb\" data-auto-link target=\"_blank\">https://gogameguru.com/i/2016/03/deepmind-mastering-go.pdf</a><br />\n<br />\n其實唔難睇"},{"pid":"53aa8fe88bb51a2cb66e19c2bebbfabad5ef2680","tid":257808,"uid":35726,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T05:24:26.000Z","msg":"Lm"},{"pid":"45a9b350f0c97b3c9efa2299223131a5d3d14219","tid":257808,"uid":77632,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T05:25:39.000Z","msg":"<img src=\"/assets/faces/normal/smile.gif\" class=\"hkgmoji\" />"},{"pid":"afb101afaf7d44ce6181ecb3a34dca1f92344eee","tid":257808,"uid":103983,"like":0,"dislike":3,"score":-3,"citedBy":0,"replyTime":"2017-05-26T05:36:20.000Z","msg":"不如由淺入深講個原理<br />\n<span style=\"color: red;\">專業名詞唔撚明</span><img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" />"},{"pid":"83556ad900c26c0b3945831bc3fbedea0c0dd068","tid":257808,"uid":48724,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-26T05:37:11.000Z","msg":"此回覆已被刪除"},{"pid":"31c5398d04391129f9fb32131fde1b4d187db2e1","tid":257808,"uid":43570,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T05:37:16.000Z","msg":"<blockquote>太長，唔quote 無夢人<br />\n一。你會唔會太睇高左人腦呢？其實人腦好多時都好唔efficient,要分玩具車，真車，漫畫車係咪真係幾張相就得呢？你可以試下俾一個細佬仔去分，依家d 日日對住部iPad 既細佬除時分唔到現實同二次元，你作為一個成年人能夠輕易分辨到係因為你已經係過去睇過無數既真車，玩具車，漫畫車，而且唔止係咁，你仲睇過好多真船，玩具船，漫畫船，真實環境，玩具場境，漫畫背景,etc<br />\n你有非常多既feature 去分辨真，玩具，漫畫，而且仲係以年為單位既training data<br />\n人腦既強大之處係在於其實每次input 都係將五感所感受到既野同時輸入，而且仲會對前後場景同時作出分析，預估完先會對現在呢一刻既問題作出回答，所有種類既問題都係同一個neural network 作出回答。 唔知你有無試過突然聽唔到人地講野成句句子入面既某一個詞語，對方重覆幾次都仲係聽唔到，呢種情況特別容易發生係你唔熟悉既topic。因為人腦唔單單係聽果個詞語既音頻黎分析，重會跟對象，前文後理黎判斷。所以當對方講一個唔係你expect 範圍內既詞語你會突然理解不能(雖然果個詞語單獨黎講你係識既)<br />\n<br />\n人腦其實係一個極典型under fitting 既例子，所以人腦對一d noise 極少甚至無noise 既問題 efficiency 係極低(e.g. 數學運算)<br />\n<br />\n而人腦真正強大既係power efficiency, 用極少能量已經可以做到極大量計算<br />\n<br />\n人腦同ai 各有好處，無乜必要去誇大人腦既強項</blockquote><br />\n上面問學人巴打已經答過你數據量問題，我就唔再多講啦。即使退一萬步講，我真係睇過好多真，玩具，漫畫，但都唔代表我睇嘅係車相關嘅圖。咁即係話，人腦係transfer learning 好強。而家人面識別嘅ANN唔可以用黎分狗嘅品種，因為AI未做到好好嘅transfer learning. 呢樣野係AI 仍然需要一段時間研究。<br />\n<br />\n而你所講嘅context sensitive 推論，其實就係bayesian brain. 用probablistic graphical model 可以簡單摸擬到，反而用ANN會好複雜。<br />\n<br />\n而另外，underfitting 問題。可能我理解錯你講法啦，underfitting 係因為model複雜度低所以 accuracy 低，同數據本身noise多唔多，速度快唔快冇乜關係。overfit先同noise有關。一般人係鋼琴學彈小星星不出十五分鐘，佢可能只係睇過老師彈過一兩次，但結果可以係八成似。如果咁都叫underfit未必太誇張。 <br />\n<br />\n另外，就以數學推算為例，其實極多極複雜數學理論都係人腦推論出黎。人腦係arithmatics 唔夠電腦黎只係因為人腦係parallel computing, 唔夠電腦運轉得快，同埋人腦working memory有限，唔似電腦勁多memory 儲住啲intermediate results。<br />\n<br />\n人腦唔單只係power efficient, 仲有驚人嘅transfer learning能力。呢啲唔單止network architecture 唔同，而係可能根本用緊唔用algorithm做野。例如neuron啲action potential 係digital, 但firing rate  又係analog. 人腦可能係一部 digital/analog 混算機。<br />\n<br />\n我嘅立場唔係話電腦一定比唔上人腦，因為事實上alphago而家咪贏晒人類。我只係講，人腦比我地想像中複雜，唔係單用ANN就可以解釋晒。而人工智能而家亦唔係天下無敵，只係係某幾個領域做得比較好，離真正strong AI仲有好遠一條路。"},{"pid":"f2449e5016c6242ec87ee849593746d1925a3767","tid":257808,"uid":43570,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T05:44:19.000Z","msg":"另外，deep learning overfit之強，由adversial examples可以見到<br />\n<br />\n<a href=\"https://blog.openai.com/adversarial-example-research/\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fblog.openai.com%2Fadversarial-example-research%2F&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=8176d7d4\" data-auto-link target=\"_blank\">https://blog.openai.com/adversarial-example-research/</a><br />\n<br />\n<br />\n<br />\n<img src=\"https://blog.openai.com/content/images/2017/02/adversarial_img_1.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fblog.openai.com%2Fcontent%2Fimages%2F2017%2F02%2Fadversarial_img_1.png&h=3f9d7d70&s={SIZE}\" />"},{"pid":"22a5570f563239619d9d92a23716c387fe764c9a","tid":257808,"uid":55598,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-26T05:53:10.000Z","msg":"<blockquote><blockquote>太長，唔quote 無夢人<br />\n一。你會唔會太睇高左人腦呢？其實人腦好多時都好唔efficient,要分玩具車，真車，漫畫車係咪真係幾張相就得呢？你可以試下俾一個細佬仔去分，依家d 日日對住部iPad 既細佬除時分唔到現實同二次元，你作為一個成年人能夠輕易分辨到係因為你已經係過去睇過無數既真車，玩具車，漫畫車，而且唔止係咁，你仲睇過好多真船，玩具船，漫畫船，真實環境，玩具場境，漫畫背景,etc<br />\n你有非常多既feature 去分辨真，玩具，漫畫，而且仲係以年為單位既training data<br />\n人腦既強大之處係在於其實每次input 都係將五感所感受到既野同時輸入，而且仲會對前後場景同時作出分析，預估完先會對現在呢一刻既問題作出回答，所有種類既問題都係同一個neural network 作出回答。 唔知你有無試過突然聽唔到人地講野成句句子入面既某一個詞語，對方重覆幾次都仲係聽唔到，呢種情況特別容易發生係你唔熟悉既topic。因為人腦唔單單係聽果個詞語既音頻黎分析，重會跟對象，前文後理黎判斷。所以當對方講一個唔係你expect 範圍內既詞語你會突然理解不能(雖然果個詞語單獨黎講你係識既)<br />\n<br />\n人腦其實係一個極典型under fitting 既例子，所以人腦對一d noise 極少甚至無noise 既問題 efficiency 係極低(e.g. 數學運算)<br />\n<br />\n而人腦真正強大既係power efficiency, 用極少能量已經可以做到極大量計算<br />\n<br />\n人腦同ai 各有好處，無乜必要去誇大人腦既強項</blockquote><br />\n上面問學人巴打已經答過你數據量問題，我就唔再多講啦。即使退一萬步講，我真係睇過好多真，玩具，漫畫，但都唔代表我睇嘅係車相關嘅圖。咁即係話，人腦係transfer learning 好強。而家人面識別嘅ANN唔可以用黎分狗嘅品種，因為AI未做到好好嘅transfer learning. 呢樣野係AI 仍然需要一段時間研究。<br />\n<br />\n而你所講嘅context sensitive 推論，其實就係bayesian brain. 用probablistic graphical model 可以簡單摸擬到，反而用ANN會好複雜。<br />\n<br />\n而另外，underfitting 問題。可能我理解錯你講法啦，underfitting 係因為model複雜度低所以 accuracy 低，同數據本身noise多唔多，速度快唔快冇乜關係。overfit先同noise有關。一般人係鋼琴學彈小星星不出十五分鐘，佢可能只係睇過老師彈過一兩次，但結果可以係八成似。如果咁都叫underfit未必太誇張。 <br />\n<br />\n另外，就以數學推算為例，其實極多極複雜數學理論都係人腦推論出黎。人腦係arithmatics 唔夠電腦黎只係因為人腦係parallel computing, 唔夠電腦運轉得快，同埋人腦working memory有限，唔似電腦勁多memory 儲住啲intermediate results。<br />\n<br />\n人腦唔單只係power efficient, 仲有驚人嘅transfer learning能力。呢啲唔單止network architecture 唔同，而係可能根本用緊唔用algorithm做野。例如neuron啲action potential 係digital, 但firing rate  又係analog. 人腦可能係一部 digital/analog 混算機。<br />\n<br />\n我嘅立場唔係話電腦一定比唔上人腦，因為事實上alphago而家咪贏晒人類。我只係講，人腦比我地想像中複雜，唔係單用ANN就可以解釋晒。而人工智能而家亦唔係天下無敵，只係係某幾個領域做得比較好，離真正strong AI仲有好遠一條路。</blockquote><br />\n<br />\n仲有人腦隨時有能力form新既neural pathway，老左又可以將某啲functions由frontal 轉去parietal<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"60e7631bcbbd159e1bdb79b931cada857071e6c8","tid":257808,"uid":43570,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-26T05:56:56.000Z","msg":"<blockquote><blockquote><blockquote>太長，唔quote 無夢人<br />\n一。你會唔會太睇高左人腦呢？其實人腦好多時都好唔efficient,要分玩具車，真車，漫畫車係咪真係幾張相就得呢？你可以試下俾一個細佬仔去分，依家d 日日對住部iPad 既細佬除時分唔到現實同二次元，你作為一個成年人能夠輕易分辨到係因為你已經係過去睇過無數既真車，玩具車，漫畫車，而且唔止係咁，你仲睇過好多真船，玩具船，漫畫船，真實環境，玩具場境，漫畫背景,etc<br />\n你有非常多既feature 去分辨真，玩具，漫畫，而且仲係以年為單位既training data<br />\n人腦既強大之處係在於其實每次input 都係將五感所感受到既野同時輸入，而且仲會對前後場景同時作出分析，預估完先會對現在呢一刻既問題作出回答，所有種類既問題都係同一個neural network 作出回答。 唔知你有無試過突然聽唔到人地講野成句句子入面既某一個詞語，對方重覆幾次都仲係聽唔到，呢種情況特別容易發生係你唔熟悉既topic。因為人腦唔單單係聽果個詞語既音頻黎分析，重會跟對象，前文後理黎判斷。所以當對方講一個唔係你expect 範圍內既詞語你會突然理解不能(雖然果個詞語單獨黎講你係識既)<br />\n<br />\n人腦其實係一個極典型under fitting 既例子，所以人腦對一d noise 極少甚至無noise 既問題 efficiency 係極低(e.g. 數學運算)<br />\n<br />\n而人腦真正強大既係power efficiency, 用極少能量已經可以做到極大量計算<br />\n<br />\n人腦同ai 各有好處，無乜必要去誇大人腦既強項</blockquote><br />\n上面問學人巴打已經答過你數據量問題，我就唔再多講啦。即使退一萬步講，我真係睇過好多真，玩具，漫畫，但都唔代表我睇嘅係車相關嘅圖。咁即係話，人腦係transfer learning 好強。而家人面識別嘅ANN唔可以用黎分狗嘅品種，因為AI未做到好好嘅transfer learning. 呢樣野係AI 仍然需要一段時間研究。<br />\n<br />\n而你所講嘅context sensitive 推論，其實就係bayesian brain. 用probablistic graphical model 可以簡單摸擬到，反而用ANN會好複雜。<br />\n<br />\n而另外，underfitting 問題。可能我理解錯你講法啦，underfitting 係因為model複雜度低所以 accuracy 低，同數據本身noise多唔多，速度快唔快冇乜關係。overfit先同noise有關。一般人係鋼琴學彈小星星不出十五分鐘，佢可能只係睇過老師彈過一兩次，但結果可以係八成似。如果咁都叫underfit未必太誇張。 <br />\n<br />\n另外，就以數學推算為例，其實極多極複雜數學理論都係人腦推論出黎。人腦係arithmatics 唔夠電腦黎只係因為人腦係parallel computing, 唔夠電腦運轉得快，同埋人腦working memory有限，唔似電腦勁多memory 儲住啲intermediate results。<br />\n<br />\n人腦唔單只係power efficient, 仲有驚人嘅transfer learning能力。呢啲唔單止network architecture 唔同，而係可能根本用緊唔用algorithm做野。例如neuron啲action potential 係digital, 但firing rate  又係analog. 人腦可能係一部 digital/analog 混算機。<br />\n<br />\n我嘅立場唔係話電腦一定比唔上人腦，因為事實上alphago而家咪贏晒人類。我只係講，人腦比我地想像中複雜，唔係單用ANN就可以解釋晒。而人工智能而家亦唔係天下無敵，只係係某幾個領域做得比較好，離真正strong AI仲有好遠一條路。</blockquote><br />\n<br />\n仲有人腦隨時有能力form新既neural pathway，老左又可以將某啲functions由frontal 轉去parietal<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n仲有一個人如果盲左，visual cortex會被轉去俾觸覺用，好癲 ....<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"836cc23e2769329ccaa0419f87bae478500860be","tid":257808,"uid":62481,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T06:04:17.000Z","msg":"Lm"},{"pid":"445586c059ba718fbbdd1668b3254eb6f67abb36","tid":257808,"uid":55598,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T06:10:21.000Z","msg":"<blockquote><blockquote><br />\n仲有人腦隨時有能力form新既neural pathway，老左又可以將某啲functions由frontal 轉去parietal<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n仲有一個人如果盲左，visual cortex會被轉去俾觸覺用，好癲 ....<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n仲有一樣我都覺得神奇既係手語activate既brain region同speech係極度相似<img src=\"/assets/faces/lomoji/42.png\" class=\"hkgmoji\" /><br />\n<br />\n巴打讀neuro但又了解咁多neural network野<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"91904d304d3f88118a349004fb41ef413ee8fef6","tid":257808,"uid":51397,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T06:35:11.000Z","msg":"留名<img src=\"/assets/faces/normal/angel.gif\" class=\"hkgmoji\" />"},{"pid":"c7ba0aa6276ed433e698e77f745b79d8eb6db827","tid":257808,"uid":79571,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T06:36:44.000Z","msg":"留名學嘢"},{"pid":"70966728c5e59065c3dffbb0c4cc20dc2ef79dc0","tid":257808,"uid":109393,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T06:36:58.000Z","msg":"Lm"},{"pid":"8f2cde3bc9ed2ce69c388051aafbf1ca05e165e4","tid":257808,"uid":186,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T06:37:30.000Z","msg":"<blockquote>前員工？咁依家做緊乜</blockquote><br />\n做緊alpha go"},{"pid":"61b3fbd4386d60fbb21ca9d927c02b9a3dfe8064","tid":257808,"uid":48679,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T06:48:05.000Z","msg":"<blockquote><blockquote>太長，唔quote 無夢人<br />\n一。你會唔會太睇高左人腦呢？其實人腦好多時都好唔efficient,要分玩具車，真車，漫畫車係咪真係幾張相就得呢？你可以試下俾一個細佬仔去分，依家d 日日對住部iPad 既細佬除時分唔到現實同二次元，你作為一個成年人能夠輕易分辨到係因為你已經係過去睇過無數既真車，玩具車，漫畫車，而且唔止係咁，你仲睇過好多真船，玩具船，漫畫船，真實環境，玩具場境，漫畫背景,etc<br />\n你有非常多既feature 去分辨真，玩具，漫畫，而且仲係以年為單位既training data<br />\n人腦既強大之處係在於其實每次input 都係將五感所感受到既野同時輸入，而且仲會對前後場景同時作出分析，預估完先會對現在呢一刻既問題作出回答，所有種類既問題都係同一個neural network 作出回答。 唔知你有無試過突然聽唔到人地講野成句句子入面既某一個詞語，對方重覆幾次都仲係聽唔到，呢種情況特別容易發生係你唔熟悉既topic。因為人腦唔單單係聽果個詞語既音頻黎分析，重會跟對象，前文後理黎判斷。所以當對方講一個唔係你expect 範圍內既詞語你會突然理解不能(雖然果個詞語單獨黎講你係識既)<br />\n<br />\n人腦其實係一個極典型under fitting 既例子，所以人腦對一d noise 極少甚至無noise 既問題 efficiency 係極低(e.g. 數學運算)<br />\n<br />\n而人腦真正強大既係power efficiency, 用極少能量已經可以做到極大量計算<br />\n<br />\n人腦同ai 各有好處，無乜必要去誇大人腦既強項</blockquote><br />\n上面問學人巴打已經答過你數據量問題，我就唔再多講啦。即使退一萬步講，我真係睇過好多真，玩具，漫畫，但都唔代表我睇嘅係車相關嘅圖。咁即係話，人腦係transfer learning 好強。而家人面識別嘅ANN唔可以用黎分狗嘅品種，因為AI未做到好好嘅transfer learning. 呢樣野係AI 仍然需要一段時間研究。<br />\n<br />\n而你所講嘅context sensitive 推論，其實就係bayesian brain. 用probablistic graphical model 可以簡單摸擬到，反而用ANN會好複雜。<br />\n<br />\n而另外，underfitting 問題。可能我理解錯你講法啦，underfitting 係因為model複雜度低所以 accuracy 低，同數據本身noise多唔多，速度快唔快冇乜關係。overfit先同noise有關。一般人係鋼琴學彈小星星不出十五分鐘，佢可能只係睇過老師彈過一兩次，但結果可以係八成似。如果咁都叫underfit未必太誇張。 <br />\n<br />\n另外，就以數學推算為例，其實極多極複雜數學理論都係人腦推論出黎。人腦係arithmatics 唔夠電腦黎只係因為人腦係parallel computing, 唔夠電腦運轉得快，同埋人腦working memory有限，唔似電腦勁多memory 儲住啲intermediate results。<br />\n<br />\n人腦唔單只係power efficient, 仲有驚人嘅transfer learning能力。呢啲唔單止network architecture 唔同，而係可能根本用緊唔用algorithm做野。例如neuron啲action potential 係digital, 但firing rate  又係analog. 人腦可能係一部 digital/analog 混算機。<br />\n<br />\n我嘅立場唔係話電腦一定比唔上人腦，因為事實上alphago而家咪贏晒人類。我只係講，人腦比我地想像中複雜，唔係單用ANN就可以解釋晒。而人工智能而家亦唔係天下無敵，只係係某幾個領域做得比較好，離真正strong AI仲有好遠一條路。</blockquote><br />\n數據量問題其實我已經答左你, 人腦收集左好多數據而不自知, 而果D 數據正正就係人腦能夠加上少量supervised learning 既sample 之後就能夠得到高accuracy 既原因, 詳細你可以睇下One-shot learning 既相關資料<br />\n你講既分狗既品種, 依家CNN 係咪做唔到呢? 其實又唔係, 而且仲準過人, 不過事先你輸入既要過濾清晒一定全部都係狗, 人腦既強大之處係generalized problem solving<br />\n<br />\n人腦你可以想像佢係chain of cluster of nodes.而每組cluster 所做既野都唔同, 呢野大把paper 講過,至於分到幾細就未有人知, 呢D 等科學家去研究<br />\n但佢地研究到之前我地係咪就唔能夠作出一D 大膽既假設呢?<br />\n點解你覺得bayesian brain/probabilistic graphical model  唔係AI 呢?<br />\nAI 點解唔可以係neural network of value evaluation nodes/cluster of nodes, 而個value evaluation method 點解要局限左係ANN/CNN, regression?bayesian brain/probabilistic graphical model, arithmetic equation一樣可以係其中一種value evaluation 既方法<br />\n而因為人腦既neural 數目實在係太多,其實佢係一次過chain 埋左classification, value evaluation, inference, prediction而且好可能係每樣都反覆咁做N 次,所以佢先可以咁generalized<br />\nAI 係咪做唔到呢?依家所有puzzle 都有齊, 之不過係有無咁多computing power 姐....<br />\n<br />\nunderfitting not  necessary 係因為complexity 太低, 你諗野太practical,退後D 睇<br />\nunderfitting 係因為對問題/答案over generalization"},{"pid":"8367297b6e6b4f69ac92d3167f7e5d93ee83611c","tid":257808,"uid":97849,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T07:19:25.000Z","msg":"留名學野"},{"pid":"b9c40165bc12c4181a9f38fdf0bf2a1272588134","tid":257808,"uid":55598,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T07:21:00.000Z","msg":"<blockquote><br />\n數據量問題其實我已經答左你, 人腦收集左好多數據而不自知, 而果D 數據正正就係人腦能夠加上少量supervised learning 既sample 之後就能夠得到高accuracy 既原因, 詳細你可以睇下One-shot learning 既相關資料<br />\n你講既分狗既品種, 依家CNN 係咪做唔到呢? 其實又唔係, 而且仲準過人, 不過事先你輸入既要過濾清晒一定全部都係狗, 人腦既強大之處係generalized problem solving<br />\n<br />\n人腦你可以想像佢係chain of cluster of nodes.而每組cluster 所做既野都唔同, 呢野大把paper 講過,至於分到幾細就未有人知, 呢D 等科學家去研究<br />\n但佢地研究到之前我地係咪就唔能夠作出一D 大膽既假設呢?<br />\n點解你覺得bayesian brain/probabilistic graphical model  唔係AI 呢?<br />\nAI 點解唔可以係neural network of value evaluation nodes/cluster of nodes, 而個value evaluation method 點解要局限左係ANN/CNN, regression?bayesian brain/probabilistic graphical model, arithmetic equation一樣可以係其中一種value evaluation 既方法<br />\n而因為人腦既neural 數目實在係太多,其實佢係一次過chain 埋左classification, value evaluation, inference, prediction而且好可能係每樣都反覆咁做N 次,所以佢先可以咁generalized<br />\nAI 係咪做唔到呢?依家所有puzzle 都有齊, 之不過係有無咁多computing power 姐....<br />\n<br />\nunderfitting not  necessary 係因為complexity 太低, 你諗野太practical,退後D 睇<br />\nunderfitting 係因為對問題/答案over generalization</blockquote><br />\n<br />\n簡單講其實即係 train 好多個唔同嘅network group埋一齊？而家都有啲類似做法。<br />\n<br />\n<img src=\"/assets/faces/lomoji/26.png\" class=\"hkgmoji\" /> 如果話人腦係 underfitting 咁其實個原因係因為人腦即使得好少data都可以 generally apply去其他地方？其實唔係好明你嘅意思，可否解釋下？如果得好少data而去做一啲task，over generalization 似乎係必然結果？<img src=\"/assets/faces/lomoji/26.png\" class=\"hkgmoji\" />"},{"pid":"6e50f7087e5cb6b6988d45181b7763726ae61a83","tid":257808,"uid":103353,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T07:40:26.000Z","msg":"留名學野"},{"pid":"48dfa6d52fab5a7d7d8ad4d2582f7f99e831b14f","tid":257808,"uid":66895,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T08:29:00.000Z","msg":"Lm"},{"pid":"f0b5402463d58b039f532f2febf7b84c50f61cb4","tid":257808,"uid":46739,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T08:41:31.000Z","msg":"留名"},{"pid":"73d7f3e98abfd783660eba3864bf9bb28f9183c3","tid":257808,"uid":58358,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T10:33:19.000Z","msg":"（咁遟先出真係唔好意思<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> ）<br />\n <br />\n神經元所接收訊號之強弱可以有所調節，而兩個神經元之間所傳遞信號之強度叫做 Synaptic Weight。<br />\n生物最簡單既「記億」，可以係兩個神經元之間搵到。如果兩個神經元之間既 Synaptic Weight 基於某種 Pattern 而不斷增強，呢種現像就叫做 Long Term Potentiation (LTP)。LTP 係腦部既海馬體尤其明顯，被認為係學習同記憶既主要機理。擴展至一個神經元既網絡，唔同既 Synaptic Weight 形成複雜既路徑，用以重構相關既 Pattern，Synaptic Weight 越大，路徑越暢順，Pattern 越明顯。從呢個角度可以理解為何我地對啲重覆出現既事物有比較深既印象。<br />\n <br />\n講到呢度好簡單咁介紹左生物既神經網絡 (Biological Neural Network) 既一啲特質。呢個應用係人類身上既精密設計，啓發左人類利用呢啲特質去創造出人工神經網絡 (Artificial Neural Network / ANN)。<br />\n <br />\nANN 相對於生物神經網絡可以話被超級簡化左（就算超級簡化左之後所建立既系統一般都相當複雜），其單位係人工神經元(Artificial Neuron)。佢完全係一個由數學 function 組成既模型。睇下以下幅圖：<br />\n <br />\n<img src=\"https://holland.pk/uptow/i4/1a35e5cf46e80c6c37c887814a7b251b.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fholland.pk%2Fuptow%2Fi4%2F1a35e5cf46e80c6c37c887814a7b251b.png&h=081984dd&s={SIZE}\" /><br />\n <br />\n呢個就係最簡單既人工神經元模型既樣。而呢個模型可以做既野就係：當 S 所接收既信息(Input) 大於門檻(Threshold / 圖中希臘之母 theta)，咁佢既狀態(State) 就係 1，否則就係 0。<br />\n <br />\n我地又可以設定一個模型既各項參數，去令到個模型進行一啲運算。<br />\n <br />\n<img src=\"https://holland.pk/uptow/i4/f88cee95e5a3a9a358d767c3f75cdeb8.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fholland.pk%2Fuptow%2Fi4%2Ff88cee95e5a3a9a358d767c3f75cdeb8.png&h=22620895&s={SIZE}\" /><br />\n <br />\n上圖展示左分別設定神經元 x 既 weight (模擬生物神經元既 Synaptic Weight), 神經元 y 既 weight，同埋門檻值 theta, 可以用黎做電腦既基本邏輯 AND, OR, NOT 既計算。<br />\n <br />\n係計算 AND 既模型裡面，將神經元 x 同 y 既 weight 設定為 1，而神經元 S 既門檻 (theta) 設定為 3/2。當 x 同 y 所接收既信號係 0 既時候，因為 0 + 0 &lt; 3/2, 所以 S 係 0，同電腦既運算結果一樣。"},{"pid":"6400ce9d478739f0bc18dc1a5a226cdf86ddced1","tid":257808,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T10:56:51.000Z","msg":"我一直好想知點解non-linear化要用relu 同sigmoid, 點解做呢個過程會令fitting有效d<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"4f3a01d4bd084c37f5ce6fe048f7778cbe1a570f","tid":257808,"uid":87557,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T11:40:29.000Z","msg":"lm"},{"pid":"bcf5c33cbbee48351a9e7e4fb8a3510577d133d8","tid":257808,"uid":81268,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T11:42:52.000Z","msg":"留名學野"},{"pid":"0a5787834b8cdbd6311c287bbcced2f0673ae0be","tid":257808,"uid":28280,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T12:09:33.000Z","msg":"<blockquote>我一直好想知點解non-linear化要用relu 同sigmoid, 點解做呢個過程會令fitting有效d<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n你用identity function 整幾多層都係linear<br />\n<br />\n同埋唔一定要relu, sigmoid只係呢兩種比多常用啫<br />\n<br />\n點解有效啲你當啲input同outcome個關係係曲線/面, 如果你容許non-linearity, 咁個prediction既hyper plane咪可以描述個關係好d<br />\n<br />\n其實我都只係識少少, 等其他巴打再補充"},{"pid":"5b121b4e9ae1a068ba4658b78d05c63c6d238b27","tid":257808,"uid":115167,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T12:55:56.000Z","msg":"lm"},{"pid":"89105a2f804f1796f85ed55e5d903c3cf8441f32","tid":257808,"uid":67356,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T14:12:16.000Z","msg":"已經唔講你本身知識水平<br />\n你係連咩叫 <strong><span style=\"color: red;\">簡單</span></strong> 都唔知，仲想學人寫科普文章<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n同埋用logic gate做示範係一開始教計數先至會咁用<br />\n講概念一定唔會用logic gate<br />\n<br />\n唔知你係抄自己以前d notes定上網抄資料，但係真係唔掂囉<br />\n<br />\n其實直接講cnn就得"},{"pid":"b0d14cf8a7ab4af39cf7534e02b26fdd5e7b8f6e","tid":257808,"uid":19113,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-26T16:27:12.000Z","msg":"<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" />"},{"pid":"f21cf903ee4930cc0601b5b5507c1bec45c533e8","tid":257808,"uid":48566,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T02:13:06.000Z","msg":"LM"},{"pid":"bfde2d9bffeba0b60067f436af4dfe5904fa94ab","tid":257808,"uid":76358,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-27T02:24:07.000Z","msg":"<a href=\"https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fadeshpande3.github.io%2Fadeshpande3.github.io%2FA-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks%2F&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=d7ea96b1\" target=\"_blank\">https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/</a><br />\n<br />\n睇呢篇好似明明地<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" />"},{"pid":"b8d089be739d6667f85236e9e60bdcf95f4c2b69","tid":257808,"uid":38550,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T02:35:46.000Z","msg":"CNN 我就唔熟<br />\n不過ANN 以前都玩過下其實係好難做得好嘅嘢嚟<br />\n<br />\n人工神經網絡嘅設計重點至少有幾個<br />\n一係網絡嘅結構<br />\n有幾多個神經元有幾多個節點分幾個層級<br />\n每個層級又點分配<br />\n唔係愈深愈廣就一定好<br />\n結構不當會出然過度學習變成只懂得做 mapping 嘅數據庫<br />\n<br />\n二係神經元入面嘅方程<br />\nCNN 從名嚟睇我估係用convolution formula 去做比對<br />\n但因應唔同需可能要放唔同方程入去<br />\n唔係所有事情都可以有效地用比對去學習<br />\n<br />\n三係學習嘅數據同分析點<br />\n數據必需龐大而有代表性<br />\n如果參考一個傻人嘅數據出嚟嘅 AI 都會傻<br />\n好似人工對話<br />\n開發嘅人一生加埋講嘅句子可能都唔夠教<br />\n但倚賴向普通受眾學又會學埋啲錯嘢衰嘢返嚟<br />\n講到呢度你就會發覺真人嘅學習能力其實仍然高好多好多<br />\n<br />\n但有一啲嘢我地只知解決方向但唔知實際辦法同答案嘅時候 AI 就非常幫得手<br />\n<br />\n我試過用 Genetic algorithm 去解 travelling  salesman problem 係極快極輕易就得到優質答案<br />\n用人腦去諗諗到頭到爆都唔掂"},{"pid":"57dd5da6da4c53535d3ee275606a36fe8134aec0","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T02:37:51.000Z","msg":"<blockquote>CNN 我就唔熟<br />\n不過ANN 以前都玩過下其實係好難做得好嘅嘢嚟<br />\n<br />\n人工神經網絡嘅設計重點至少有幾個<br />\n一係網絡嘅結構<br />\n有幾多個神經元有幾多個節點分幾個層級<br />\n每個層級又點分配<br />\n唔係愈深愈廣就一定好<br />\n結構不當會出然過度學習變成只懂得做 mapping 嘅數據庫<br />\n<br />\n二係神經元入面嘅方程<br />\nCNN 從名嚟睇我估係用convolution formula 去做比對<br />\n但因應唔同需可能要放唔同方程入去<br />\n唔係所有事情都可以有效地用比對去學習<br />\n<br />\n三係學習嘅數據同分析點<br />\n數據必需龐大而有代表性<br />\n如果參考一個傻人嘅數據出嚟嘅 AI 都會傻<br />\n好似人工對話<br />\n開發嘅人一生加埋講嘅句子可能都唔夠教<br />\n但倚賴向普通受眾學又會學埋啲錯嘢衰嘢返嚟<br />\n講到呢度你就會發覺真人嘅學習能力其實仍然高好多好多<br />\n<br />\n但有一啲嘢我地只知解決方向但唔知實際辦法同答案嘅時候 AI 就非常幫得手<br />\n<br />\n我試過用 Genetic algorithm 去解 travelling  salesman problem 係極快極輕易就得到優質答案<br />\n用人腦去諗諗到頭到爆都唔掂</blockquote><br />\n如果你跟住個algorithm人手solve sales佬問題都得架<br />\n不過唔會有人想人手做<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"f7969d92937d9805d8ef77ce4dc85ce7043e13f8","tid":257808,"uid":38316,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T02:49:00.000Z","msg":"<blockquote>CNN 我就唔熟<br />\n不過ANN 以前都玩過下其實係好難做得好嘅嘢嚟<br />\n<br />\n人工神經網絡嘅設計重點至少有幾個<br />\n一係網絡嘅結構<br />\n有幾多個神經元有幾多個節點分幾個層級<br />\n每個層級又點分配<br />\n唔係愈深愈廣就一定好<br />\n結構不當會出然過度學習變成只懂得做 mapping 嘅數據庫<br />\n<br />\n二係神經元入面嘅方程<br />\nCNN 從名嚟睇我估係用convolution formula 去做比對<br />\n但因應唔同需可能要放唔同方程入去<br />\n唔係所有事情都可以有效地用比對去學習<br />\n<br />\n三係學習嘅數據同分析點<br />\n數據必需龐大而有代表性<br />\n如果參考一個傻人嘅數據出嚟嘅 AI 都會傻<br />\n好似人工對話<br />\n開發嘅人一生加埋講嘅句子可能都唔夠教<br />\n但倚賴向普通受眾學又會學埋啲錯嘢衰嘢返嚟<br />\n講到呢度你就會發覺真人嘅學習能力其實仍然高好多好多<br />\n<br />\n但有一啲嘢我地只知解決方向但唔知實際辦法同答案嘅時候 AI 就非常幫得手<br />\n<br />\n我試過用 Genetic algorithm 去解 travelling  salesman problem 係極快極輕易就得到優質答案<br />\n用人腦去諗諗到頭到爆都唔掂</blockquote><br />\n中文唔好，想問咩叫優質答案<img src=\"/assets/faces/normal/wonder.gif\" class=\"hkgmoji\" />"},{"pid":"cda6c7d4bce97b3235c8ca5fe0d555d7190f44cd","tid":257808,"uid":38550,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T03:06:40.000Z","msg":"<blockquote><blockquote>CNN 我就唔熟<br />\n不過ANN 以前都玩過下其實係好難做得好嘅嘢嚟<br />\n<br />\n人工神經網絡嘅設計重點至少有幾個<br />\n一係網絡嘅結構<br />\n有幾多個神經元有幾多個節點分幾個層級<br />\n每個層級又點分配<br />\n唔係愈深愈廣就一定好<br />\n結構不當會出然過度學習變成只懂得做 mapping 嘅數據庫<br />\n<br />\n二係神經元入面嘅方程<br />\nCNN 從名嚟睇我估係用convolution formula 去做比對<br />\n但因應唔同需可能要放唔同方程入去<br />\n唔係所有事情都可以有效地用比對去學習<br />\n<br />\n三係學習嘅數據同分析點<br />\n數據必需龐大而有代表性<br />\n如果參考一個傻人嘅數據出嚟嘅 AI 都會傻<br />\n好似人工對話<br />\n開發嘅人一生加埋講嘅句子可能都唔夠教<br />\n但倚賴向普通受眾學又會學埋啲錯嘢衰嘢返嚟<br />\n講到呢度你就會發覺真人嘅學習能力其實仍然高好多好多<br />\n<br />\n但有一啲嘢我地只知解決方向但唔知實際辦法同答案嘅時候 AI 就非常幫得手<br />\n<br />\n我試過用 Genetic algorithm 去解 travelling  salesman problem 係極快極輕易就得到優質答案<br />\n用人腦去諗諗到頭到爆都唔掂</blockquote><br />\n中文唔好，想問咩叫優質答案<img src=\"/assets/faces/normal/wonder.gif\" class=\"hkgmoji\" /></blockquote><br />\nGood answer <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n唔知係咪答到你<br />\n<br />\n對於一啲無或者唔知準確答案嘅問題<br />\n一個可接受範圍內嘅答案就係 Good answer<br />\n<br />\n以自動駕駛為例<br />\n幾時開始剎制停車要準確計算需要好多不可能即時得到嘅數據<br />\n人係憑經驗直覺同即時修正去判斷<br />\n只要做到指定位置範圍停到就合格都稱為Good answer<br />\n<br />\n或者有學術名稱<br />\n不過太耐以前嘅嘢已經無乜印象 <img src=\"/assets/faces/normal/chicken.gif\" class=\"hkgmoji\" />"},{"pid":"1de1de5756a3596f97e0eef0aec8512be4b0a1cc","tid":257808,"uid":48679,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T03:09:51.000Z","msg":"<blockquote><blockquote><br />\n數據量問題其實我已經答左你, 人腦收集左好多數據而不自知, 而果D 數據正正就係人腦能夠加上少量supervised learning 既sample 之後就能夠得到高accuracy 既原因, 詳細你可以睇下One-shot learning 既相關資料<br />\n你講既分狗既品種, 依家CNN 係咪做唔到呢? 其實又唔係, 而且仲準過人, 不過事先你輸入既要過濾清晒一定全部都係狗, 人腦既強大之處係generalized problem solving<br />\n<br />\n人腦你可以想像佢係chain of cluster of nodes.而每組cluster 所做既野都唔同, 呢野大把paper 講過,至於分到幾細就未有人知, 呢D 等科學家去研究<br />\n但佢地研究到之前我地係咪就唔能夠作出一D 大膽既假設呢?<br />\n點解你覺得bayesian brain/probabilistic graphical model  唔係AI 呢?<br />\nAI 點解唔可以係neural network of value evaluation nodes/cluster of nodes, 而個value evaluation method 點解要局限左係ANN/CNN, regression?bayesian brain/probabilistic graphical model, arithmetic equation一樣可以係其中一種value evaluation 既方法<br />\n而因為人腦既neural 數目實在係太多,其實佢係一次過chain 埋左classification, value evaluation, inference, prediction而且好可能係每樣都反覆咁做N 次,所以佢先可以咁generalized<br />\nAI 係咪做唔到呢?依家所有puzzle 都有齊, 之不過係有無咁多computing power 姐....<br />\n<br />\nunderfitting not  necessary 係因為complexity 太低, 你諗野太practical,退後D 睇<br />\nunderfitting 係因為對問題/答案over generalization</blockquote><br />\n<br />\n簡單講其實即係 train 好多個唔同嘅network group埋一齊？而家都有啲類似做法。<br />\n<br />\n<img src=\"/assets/faces/lomoji/26.png\" class=\"hkgmoji\" /> 如果話人腦係 underfitting 咁其實個原因係因為人腦即使得好少data都可以 generally apply去其他地方？其實唔係好明你嘅意思，可否解釋下？如果得好少data而去做一啲task，over generalization 似乎係必然結果？<img src=\"/assets/faces/lomoji/26.png\" class=\"hkgmoji\" /></blockquote><br />\n係，你可以想像係首先有一個classification 既network 去分類問題，不過呢個classification 好大機會都唔係我地平時用開得0/1既classification, 而係weighted probability, 然後依照個weight 去相應地加權response 既rating, 最後依rating決定答案<br />\n例如一個簡化到極端既例子，&ldquo;1+1&rdquo;，首先要做既就係分類到式呢個係普通算式運算定係number theory，一般人會可能有0.9weight 分類去計數，即係答案係2, 而0.1 weight 係諗點prove1+1=2<br />\n然後個weighted 既答案又會feedback 番落去semantic analysis前文後理得出&ldquo;2&rdquo;先係&ldquo;正確答案&rdquo;<br />\n<br />\n要做到人腦咁generalized 既problem solving 似乎係無可避免地 over generalized, 至少人類進化左幾千年都未解決到呢個問題<br />\n你見到某d 領域既天才其本上就係dna 選擇去特化果個領域既結果，而可惜地呢d 天才多數地係其他方面都有一d 缺陷<br />\ngeneralized/ accuracy 係specific problem 似乎係無辦法公存既"},{"pid":"32535707e23098490621f0d1ed8eec886fc4ede1","tid":257808,"uid":38550,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T03:38:55.000Z","msg":"<blockquote><blockquote><blockquote><br />\n數據量問題其實我已經答左你, 人腦收集左好多數據而不自知, 而果D 數據正正就係人腦能夠加上少量supervised learning 既sample 之後就能夠得到高accuracy 既原因, 詳細你可以睇下One-shot learning 既相關資料<br />\n你講既分狗既品種, 依家CNN 係咪做唔到呢? 其實又唔係, 而且仲準過人, 不過事先你輸入既要過濾清晒一定全部都係狗, 人腦既強大之處係generalized problem solving<br />\n<br />\n人腦你可以想像佢係chain of cluster of nodes.而每組cluster 所做既野都唔同, 呢野大把paper 講過,至於分到幾細就未有人知, 呢D 等科學家去研究<br />\n但佢地研究到之前我地係咪就唔能夠作出一D 大膽既假設呢?<br />\n點解你覺得bayesian brain/probabilistic graphical model  唔係AI 呢?<br />\nAI 點解唔可以係neural network of value evaluation nodes/cluster of nodes, 而個value evaluation method 點解要局限左係ANN/CNN, regression?bayesian brain/probabilistic graphical model, arithmetic equation一樣可以係其中一種value evaluation 既方法<br />\n而因為人腦既neural 數目實在係太多,其實佢係一次過chain 埋左classification, value evaluation, inference, prediction而且好可能係每樣都反覆咁做N 次,所以佢先可以咁generalized<br />\nAI 係咪做唔到呢?依家所有puzzle 都有齊, 之不過係有無咁多computing power 姐....<br />\n<br />\nunderfitting not  necessary 係因為complexity 太低, 你諗野太practical,退後D 睇<br />\nunderfitting 係因為對問題/答案over generalization</blockquote><br />\n<br />\n簡單講其實即係 train 好多個唔同嘅network group埋一齊？而家都有啲類似做法。<br />\n<br />\n<img src=\"/assets/faces/lomoji/26.png\" class=\"hkgmoji\" /> 如果話人腦係 underfitting 咁其實個原因係因為人腦即使得好少data都可以 generally apply去其他地方？其實唔係好明你嘅意思，可否解釋下？如果得好少data而去做一啲task，over generalization 似乎係必然結果？<img src=\"/assets/faces/lomoji/26.png\" class=\"hkgmoji\" /></blockquote><br />\n係，你可以想像係首先有一個classification 既network 去分類問題，不過呢個classification 好大機會都唔係我地平時用開得0/1既classification, 而係weighted probability, 然後依照個weight 去相應地加權response 既rating, 最後依rating決定答案<br />\n例如一個簡化到極端既例子，&ldquo;1+1&rdquo;，首先要做既就係分類到式呢個係普通算式運算定係number theory，一般人會可能有0.9weight 分類去計數，即係答案係2, 而0.1 weight 係諗點prove1+1=2<br />\n然後個weighted 既答案又會feedback 番落去semantic analysis前文後理得出&ldquo;2&rdquo;先係&ldquo;正確答案&rdquo;<br />\n<br />\n要做到人腦咁generalized 既problem solving 似乎係無可避免地 over generalized, 至少人類進化左幾千年都未解決到呢個問題<br />\n你見到某d 領域既天才其本上就係dna 選擇去特化果個領域既結果，而可惜地呢d 天才多數地係其他方面都有一d 缺陷<br />\ngeneralized/ accuracy 係specific problem 似乎係無辦法公存既</blockquote><br />\n除咗解決問題嘅能力本身<br />\n人有想像力<br />\n可以超乎現實、經驗同理解範圍<br />\n呢個係自學啟發嘅關鍵<br />\n<br />\n一個人就算打坐瞓覺痾屎都可以悟出道理辦法<br />\n然後透過實證去確立再延伸學習<br />\n<br />\n今日嘅AI 充其量只有記憶運算同驗證能力<br />\n要啟發都只可循某固定模式或者更多係以隨機方法<br />\n要好似牛頓從見到蘋果落地悟出地心吸力近乎係無可能嘅事"},{"pid":"93f3e09f073a5884daef8a5123ff79a1e32b8d9f","tid":257808,"uid":43570,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T03:55:58.000Z","msg":"<blockquote><br />\n數據量問題其實我已經答左你, 人腦收集左好多數據而不自知, 而果D 數據正正就係人腦能夠加上少量supervised learning 既sample 之後就能夠得到高accuracy 既原因, 詳細你可以睇下One-shot learning 既相關資料<br />\n你講既分狗既品種, 依家CNN 係咪做唔到呢? 其實又唔係, 而且仲準過人, 不過事先你輸入既要過濾清晒一定全部都係狗, 人腦既強大之處係generalized problem solving<br />\n<br />\n人腦你可以想像佢係chain of cluster of nodes.而每組cluster 所做既野都唔同, 呢野大把paper 講過,至於分到幾細就未有人知, 呢D 等科學家去研究<br />\n但佢地研究到之前我地係咪就唔能夠作出一D 大膽既假設呢?<br />\n點解你覺得bayesian brain/probabilistic graphical model  唔係AI 呢?<br />\nAI 點解唔可以係neural network of value evaluation nodes/cluster of nodes, 而個value evaluation method 點解要局限左係ANN/CNN, regression?bayesian brain/probabilistic graphical model, arithmetic equation一樣可以係其中一種value evaluation 既方法<br />\n而因為人腦既neural 數目實在係太多,其實佢係一次過chain 埋左classification, value evaluation, inference, prediction而且好可能係每樣都反覆咁做N 次,所以佢先可以咁generalized<br />\nAI 係咪做唔到呢?依家所有puzzle 都有齊, 之不過係有無咁多computing power 姐....<br />\n<br />\nunderfitting not  necessary 係因為complexity 太低, 你諗野太practical,退後D 睇<br />\nunderfitting 係因為對問題/答案over generalization</blockquote><br />\n<br />\n如果話人黎講少sample都高accuracy因為我地不自覺間收集到好左好多數據。咁即係話人類高accuracy 係都係因為多sample, 但連deepmind 而家做one-shot都唔洗多sample 喎。咁即係話多sample唔係accuracy高嘅必然條件，係同個architecture有關。仲有我唔係話CNN分唔到狗嘅品種，我係話train左黎識別人面嘅唔可以同時用黎分辨狗嘅品種，ANN而家transfer learning仲係好差。呢個係architectural 層次問題。<br />\n<br />\n其實我邊度有講過graphical model 唔係AI <img src=\"/assets/faces/normal/wonder.gif\" class=\"hkgmoji\" /> 我由頭到尾只係講ANN唔可以完全解釋人腦，intelligence system有好多種，ANN只係其中一種。ANN係一個數學模型，唔係描述人腦嘅模型。<br />\n<br />\n你所謂cluster of nodes理論有一個致命問題，就係假設左node同node之間係冇重疊的。但人腦就正正相反，同一個neuron係唔同task底下都有可能被activate。而且如果每個task都有一個cluster, 咁係咪用手拎杯一個cluster, 拎碟一個cluster, 拎波又另一個cluster呢，世事何其多，要幾多個cluster先夠...而train新cluster又要幾多時間？學識拎杯嘅人，亦會好快學識拎茶壺，咁又點理解呢？再者，大腦構造係可以靈活改變。盲人visual cortex可以repurpose做聽覺，觸覺，visual同聽覺根本需要唔同architecture, 前者CNN後者RNN，點解同一個node可以變得咁快？<br />\n<br />\nOver-generalize 同複雜度唔夠其實咪同一樣野？咁都可以話下人諗野太practical...<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> 其實我地都係純粹學術討論，點解最後都要加句人身攻擊呢...<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"033e7ae5f5a5f831a995e942525bb98a4e3a409d","tid":257808,"uid":62935,"like":3,"dislike":1,"score":2,"citedBy":0,"replyTime":"2017-05-27T04:26:43.000Z","msg":"屌<br />\n又話簡單了解<br />\n愈講愈唔知你地up 乜<br />\n<br />\n成班係到鬥反駁，show quali<br />\n寫野又無structure<br />\n<br />\n入撚錯post on99"},{"pid":"55d9bd090605cad1ff57daadb93e748f4513a323","tid":257808,"uid":43570,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T04:30:26.000Z","msg":"<blockquote><blockquote><blockquote><br />\n數據量問題其實我已經答左你, 人腦收集左好多數據而不自知, 而果D 數據正正就係人腦能夠加上少量supervised learning 既sample 之後就能夠得到高accuracy 既原因, 詳細你可以睇下One-shot learning 既相關資料<br />\n你講既分狗既品種, 依家CNN 係咪做唔到呢? 其實又唔係, 而且仲準過人, 不過事先你輸入既要過濾清晒一定全部都係狗, 人腦既強大之處係generalized problem solving<br />\n<br />\n人腦你可以想像佢係chain of cluster of nodes.而每組cluster 所做既野都唔同, 呢野大把paper 講過,至於分到幾細就未有人知, 呢D 等科學家去研究<br />\n但佢地研究到之前我地係咪就唔能夠作出一D 大膽既假設呢?<br />\n點解你覺得bayesian brain/probabilistic graphical model  唔係AI 呢?<br />\nAI 點解唔可以係neural network of value evaluation nodes/cluster of nodes, 而個value evaluation method 點解要局限左係ANN/CNN, regression?bayesian brain/probabilistic graphical model, arithmetic equation一樣可以係其中一種value evaluation 既方法<br />\n而因為人腦既neural 數目實在係太多,其實佢係一次過chain 埋左classification, value evaluation, inference, prediction而且好可能係每樣都反覆咁做N 次,所以佢先可以咁generalized<br />\nAI 係咪做唔到呢?依家所有puzzle 都有齊, 之不過係有無咁多computing power 姐....<br />\n<br />\nunderfitting not  necessary 係因為complexity 太低, 你諗野太practical,退後D 睇<br />\nunderfitting 係因為對問題/答案over generalization</blockquote><br />\n<br />\n簡單講其實即係 train 好多個唔同嘅network group埋一齊？而家都有啲類似做法。<br />\n<br />\n<img src=\"/assets/faces/lomoji/26.png\" class=\"hkgmoji\" /> 如果話人腦係 underfitting 咁其實個原因係因為人腦即使得好少data都可以 generally apply去其他地方？其實唔係好明你嘅意思，可否解釋下？如果得好少data而去做一啲task，over generalization 似乎係必然結果？<img src=\"/assets/faces/lomoji/26.png\" class=\"hkgmoji\" /></blockquote><br />\n係，你可以想像係首先有一個classification 既network 去分類問題，不過呢個classification 好大機會都唔係我地平時用開得0/1既classification, 而係weighted probability, 然後依照個weight 去相應地加權response 既rating, 最後依rating決定答案<br />\n例如一個簡化到極端既例子，&ldquo;1+1&rdquo;，首先要做既就係分類到式呢個係普通算式運算定係number theory，一般人會可能有0.9weight 分類去計數，即係答案係2, 而0.1 weight 係諗點prove1+1=2<br />\n然後個weighted 既答案又會feedback 番落去semantic analysis前文後理得出&ldquo;2&rdquo;先係&ldquo;正確答案&rdquo;<br />\n<br />\n要做到人腦咁generalized 既problem solving 似乎係無可避免地 over generalized, 至少人類進化左幾千年都未解決到呢個問題<br />\n你見到某d 領域既天才其本上就係dna 選擇去特化果個領域既結果，而可惜地呢d 天才多數地係其他方面都有一d 缺陷<br />\ngeneralized/ accuracy 係specific problem 似乎係無辦法公存既</blockquote><br />\n<br />\n呢種層層classify嘅理論可以從實驗層面就可以證明唔係完全正確。例如一啲講求速度嘅運動，e.g. 乒乓球，羽毛球，劍擊etc .，運動員可以係極短時間內作出反應。依照層層classify嘅理論，見到一個乒乓波飛埋黎要作出反應要經過好多步，例如先要辨別到係一個乒乓波，分辨眼前係現實定電視形像，判斷自己係咪應該打佢，然後估算佢嘅位置同方向，估算自己手嘅位置同方向，計算出手嘅trajectory，判斷呢個trajectory係咪合理，計算各肌肉的用力的比例，然後打出去。如果層層classify,呢度每一個判斷都涉及極複雜的network，而neuron同neuron之間有synaptic delay，速度係上限的，計得黎個波一早就飛走左。但其實研究顯示，對一個task越純熟，大腦就越少activation。大腦會自動調節去specialize一個task, 唔係一定要層層去classify.<br />\n<br />\n而天才都有好多種，全能型天才都有。達文西就工程畫畫樣樣精，愛因斯坦都好有音樂天份，亦唔見佢有咩其他明顯缺陷。人腦並唔係over-generalize,而係佢本身有個好靈活嘅架構，令到佢可以適應唔同嘅環境。世上有好多超乎常人嘅運動家，音樂家，數學家，呢啲都係人可以specialize嘅證明。有缺陷嘅天才好多只係因為佢地被報道得多，其實果啲缺陷唔係天才都一樣會有，亦有好多天才一生正正常常。因為就大概而論話人腦over-generalize未必太過武斷。"},{"pid":"f806e25056dc1352807434798e65444076e9beae","tid":257808,"uid":43570,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T04:42:33.000Z","msg":"<blockquote>我一直好想知點解non-linear化要用relu 同sigmoid, 點解做呢個過程會令fitting有效d<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n可能其中一個原因係唔想某個weight dominate晒成個network. <br />\n<br />\n而ReLU好似係同解決vanishing gradient 有關..."},{"pid":"588d0259391aae4f5ca2e3131397cdbe302c6f97","tid":257808,"uid":12510,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T04:46:32.000Z","msg":"高質 post LM"},{"pid":"b12b708dd4b44da27f1eaa252bbd894b19e4426b","tid":257808,"uid":43570,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T04:46:41.000Z","msg":"<blockquote><blockquote><blockquote><br />\n仲有人腦隨時有能力form新既neural pathway，老左又可以將某啲functions由frontal 轉去parietal<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n仲有一個人如果盲左，visual cortex會被轉去俾觸覺用，好癲 ....<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n仲有一樣我都覺得神奇既係手語activate既brain region同speech係極度相似<img src=\"/assets/faces/lomoji/42.png\" class=\"hkgmoji\" /><br />\n<br />\n巴打讀neuro但又了解咁多neural network野<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /></blockquote><br />\n只係因為研究同neural coding有啲關係，所以都係成日同machine learning呀，information theory ar打交道...<br />\n<br />\n想當年我岩岩開始做research時根本冇乜人會用ANN, 個個都覺得佢又慢又廢，估唔到而家時移世易<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"72c2881ce08c4fc1652c1a0674461e9151819fd5","tid":257808,"uid":15823,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T04:57:18.000Z","msg":"有無可能用低運算能力去做得返大概alpha go 個學習能力<img src=\"/assets/faces/normal/wonder.gif\" class=\"hkgmoji\" />"},{"pid":"04a4ada394717712751c254723489363cfdec7c8","tid":257808,"uid":91973,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T04:59:47.000Z","msg":"lm"},{"pid":"8d1960e1fdaf333296c2128f8724ecba7a02ced3","tid":257808,"uid":12510,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T05:15:11.000Z","msg":"<blockquote>屌<br />\n又話簡單了解<br />\n愈講愈唔知你地up 乜<br />\n<br />\n成班係到鬥反駁，show quali<br />\n寫野又無structure<br />\n<br />\n入撚錯post on99</blockquote><br />\n<br />\n係<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n樓主去左邊"},{"pid":"81375f1e2bdfab375102251f0cab55bdec2e274d","tid":257808,"uid":38550,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T05:29:03.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><br />\n仲有人腦隨時有能力form新既neural pathway，老左又可以將某啲functions由frontal 轉去parietal<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n仲有一個人如果盲左，visual cortex會被轉去俾觸覺用，好癲 ....<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n仲有一樣我都覺得神奇既係手語activate既brain region同speech係極度相似<img src=\"/assets/faces/lomoji/42.png\" class=\"hkgmoji\" /><br />\n<br />\n巴打讀neuro但又了解咁多neural network野<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /></blockquote><br />\n只係因為研究同neural coding有啲關係，所以都係成日同machine learning呀，information theory ar打交道...<br />\n<br />\n想當年我岩岩開始做research時根本冇乜人會用ANN, 個個都覺得佢又慢又廢，估唔到而家時移世易<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n好多年前鐵路 auto pilot 好似已經係 ANN + fuzzy logic 咁做<br />\n又唔講得上又慢又廢<br />\n只係搞一舖嘢就只係識做一件事好多時都唔值得所以唔係咁多機會用"},{"pid":"34e1602bce10f31fe0c3a51beb308a0720bd5d5a","tid":257808,"uid":113076,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T06:29:02.000Z","msg":"電子撚留名"},{"pid":"5a68d42c299f969e4e7795f8bccf28edd09f0c69","tid":257808,"uid":10780,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T06:31:49.000Z","msg":"高質post"},{"pid":"3ed5c9584a4d4eab40ba0a5d2d0b3dddc6ce286d","tid":257808,"uid":57088,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T06:37:47.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><br />\n仲有人腦隨時有能力form新既neural pathway，老左又可以將某啲functions由frontal 轉去parietal<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n仲有一個人如果盲左，visual cortex會被轉去俾觸覺用，好癲 ....<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n仲有一樣我都覺得神奇既係手語activate既brain region同speech係極度相似<img src=\"/assets/faces/lomoji/42.png\" class=\"hkgmoji\" /><br />\n<br />\n巴打讀neuro但又了解咁多neural network野<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /></blockquote><br />\n只係因為研究同neural coding有啲關係，所以都係成日同machine learning呀，information theory ar打交道...<br />\n<br />\n想當年我岩岩開始做research時根本冇乜人會用ANN, 個個都覺得佢又慢又廢，估唔到而家時移世易<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n好多年前鐵路 auto pilot 好似已經係 ANN + fuzzy logic 咁做<br />\n又唔講得上又慢又廢<br />\n只係搞一舖嘢就只係識做一件事好多時都唔值得所以唔係咁多機會用</blockquote><br />\n fuzzy logic <img src=\"/assets/faces/normal/tongue.gif\" class=\"hkgmoji\" />"},{"pid":"c5b3d567a28440e91d8e54811feb04ea9f5ac689","tid":257808,"uid":95102,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T07:47:38.000Z","msg":"splm"},{"pid":"d53b142183f2a9a18a44e19226b68e8622ec4239","tid":257808,"uid":37782,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T08:10:21.000Z","msg":"留明"},{"pid":"7486c6b863d405580a7c7a935d7ea11f02cc90e9","tid":257808,"uid":34301,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T08:31:47.000Z","msg":"<blockquote>屌<br />\n又話簡單了解<br />\n愈講愈唔知你地up 乜<br />\n<br />\n成班係到鬥反駁，show quali<br />\n寫野又無structure<br />\n<br />\n入撚錯post on99</blockquote><br />\n突破盲腸<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n老實講我覺得一講到layer層面啲weights已經好難簡單了解<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"e1f5dde5640df79da2b8d71fe1a23e9660679517","tid":257808,"uid":103983,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T09:10:43.000Z","msg":"<blockquote><blockquote>屌<br />\n又話簡單了解<br />\n愈講愈唔知你地up 乜<br />\n<br />\n成班係到鬥反駁，show quali<br />\n寫野又無structure<br />\n<br />\n入撚錯post on99</blockquote><br />\n突破盲腸<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n老實講我覺得一講到layer層面啲weights已經好難簡單了解<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n講左由淺入深去解釋原理<br />\n一黎就跳左去做reserach level<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" />"},{"pid":"f1be6c5f85a74deadedf28aa28a3a99b2a06323d","tid":257808,"uid":31455,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T09:59:42.000Z","msg":"對IT人嚟呢啲可能簡單<br />\n對文科人嚟講呢就 <img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" />"},{"pid":"2f0ad3cf7c8786c1d2482d170839b15dfd572fb0","tid":257808,"uid":65424,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T10:10:40.000Z","msg":"留名"},{"pid":"98aac92ed9c0276031202e7ca56250b6249c0ae4","tid":257808,"uid":55598,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-27T10:50:56.000Z","msg":"<blockquote><blockquote><blockquote>屌<br />\n又話簡單了解<br />\n愈講愈唔知你地up 乜<br />\n<br />\n成班係到鬥反駁，show quali<br />\n寫野又無structure<br />\n<br />\n入撚錯post on99</blockquote><br />\n突破盲腸<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n老實講我覺得一講到layer層面啲weights已經好難簡單了解<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n講左由淺入深去解釋原理<br />\n一黎就跳左去做reserach level<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n樓主有樓主講，其他巴打討論下有咩問題<img src=\"/assets/faces/lomoji/37.png\" class=\"hkgmoji\" />"},{"pid":"24bdfad85645b75e82fd991e4581e52d8548daa2","tid":257808,"uid":34301,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T11:04:42.000Z","msg":"<blockquote><blockquote><blockquote><blockquote>屌<br />\n又話簡單了解<br />\n愈講愈唔知你地up 乜<br />\n<br />\n成班係到鬥反駁，show quali<br />\n寫野又無structure<br />\n<br />\n入撚錯post on99</blockquote><br />\n突破盲腸<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n老實講我覺得一講到layer層面啲weights已經好難簡單了解<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n講左由淺入深去解釋原理<br />\n一黎就跳左去做reserach level<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n樓主有樓主講，其他巴打討論下有咩問題<img src=\"/assets/faces/lomoji/37.png\" class=\"hkgmoji\" /></blockquote><br />\n問題而家題目嘅關鍵字係「簡單了解」<br />\n你喺度講一堆專業詞彙出來咁你叫啲初學者點睇得明<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n衰啲講句而家大部分人都係離題<br />\n你要深入討論就應該出去開多個post慢慢討論，唔好搞到個post啲內容咁難明<br />\n呢個post嘅內容應該要簡單易明先鼓勵到多啲人認識咩係AI<br />\n唔係學咩人叫簡單了解？<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"af370d44d304c012fcadae0d22a2b6e7566114ae","tid":257808,"uid":82234,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T11:16:56.000Z","msg":"簡單了解<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"8f9fd7e36c638465343e9385aac252fae147eaa0","tid":257808,"uid":38550,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T11:34:29.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote>屌<br />\n又話簡單了解<br />\n愈講愈唔知你地up 乜<br />\n<br />\n成班係到鬥反駁，show quali<br />\n寫野又無structure<br />\n<br />\n入撚錯post on99</blockquote><br />\n突破盲腸<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n老實講我覺得一講到layer層面啲weights已經好難簡單了解<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n講左由淺入深去解釋原理<br />\n一黎就跳左去做reserach level<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n樓主有樓主講，其他巴打討論下有咩問題<img src=\"/assets/faces/lomoji/37.png\" class=\"hkgmoji\" /></blockquote><br />\n問題而家題目嘅關鍵字係「簡單了解」<br />\n你喺度講一堆專業詞彙出來咁你叫啲初學者點睇得明<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n衰啲講句而家大部分人都係離題<br />\n你要深入討論就應該出去開多個post慢慢討論，唔好搞到個post啲內容咁難明<br />\n呢個post嘅內容應該要簡單易明先鼓勵到多啲人認識咩係AI<br />\n唔係學咩人叫簡單了解？<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n真係應該至少要用啲普通人睇得明o既字詞<br />\n唔係有興趣都睇到無癮 <img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /><br />\n<br />\n雖然內行人自己可以繼續傾<br />\n但多啲人一齊討論開心啲又有啟發多啲"},{"pid":"294199b73e0bf551da14d0570a7dafcacaa543dd","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T11:54:04.000Z","msg":"其實neural network好單純math driven, 我讀果陣都唔知個prof up乜鳩 <br />\n連researcher都形容係black box<br />\n<br />\n反而我覺得reinforcement learning會易講d 起碼有實例講下<br />\n<br />\nMCT我就唔太識 MC method就聽過下<br />\n<br />\n其他都好食數底 一講就寫到水蛇春果時就會too long didnt read<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"078d5c6a09cb9c267b27878ac3e4b5d5b57760bd","tid":257808,"uid":55598,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T12:09:17.000Z","msg":"<blockquote>其實neural network好單純math driven, 我讀果陣都唔知個prof up乜鳩 <br />\n連researcher都形容係black box<br />\n<br />\n反而我覺得reinforcement learning會易講d 起碼有實例講下<br />\n<br />\nMCT我就唔太識 MC method就聽過下<br />\n<br />\n其他都好食數底 一講就寫到水蛇春果時就會too long didnt read<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\ndesign其實都要諗好多野，但train and test就真係black box<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n所以我覺得介紹multilayer perceptron係最簡單易明"},{"pid":"cb0ca626141803679c98954401f8c0b1503cccea","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T12:12:30.000Z","msg":"<blockquote><blockquote>其實neural network好單純math driven, 我讀果陣都唔知個prof up乜鳩 <br />\n連researcher都形容係black box<br />\n<br />\n反而我覺得reinforcement learning會易講d 起碼有實例講下<br />\n<br />\nMCT我就唔太識 MC method就聽過下<br />\n<br />\n其他都好食數底 一講就寫到水蛇春果時就會too long didnt read<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\ndesign其實都要諗好多野，但train and test就真係black box<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n所以我覺得介紹multilayer perceptron係最簡單易明</blockquote><br />\n係呀 同埋我記tf有個網玩下mlp"},{"pid":"2af8db010b13eb129da7db6413d40636a11db8f6","tid":257808,"uid":73004,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T12:39:26.000Z","msg":"留名慢慢睇<img src=\"/assets/faces/normal/hoho.gif\" class=\"hkgmoji\" />"},{"pid":"c097918a3a15ffa5d663ae724bc19e61abb086e9","tid":257808,"uid":9062,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T12:42:29.000Z","msg":"高汁<img src=\"/assets/faces/big/agree.gif\" class=\"hkgmoji\" />"},{"pid":"0e8d62de09628d0f2d5bccee24e845e0ba2c07d1","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T13:05:46.000Z","msg":"<a href=\"http://playground.tensorflow.org/\" data-sr-url=\"https://r.lihkg.com/link?u=http%3A%2F%2Fplayground.tensorflow.org%2F&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=79dd169d\" target=\"_blank\">http://playground.tensorflow.org/</a><br />\n大家可以鳩玩下<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"1f0603843f377a505f27bfe9e18a5bf18cbb57c9","tid":257808,"uid":48679,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T13:30:27.000Z","msg":"<blockquote><blockquote><br />\n數據量問題其實我已經答左你, 人腦收集左好多數據而不自知, 而果D 數據正正就係人腦能夠加上少量supervised learning 既sample 之後就能夠得到高accuracy 既原因, 詳細你可以睇下One-shot learning 既相關資料<br />\n你講既分狗既品種, 依家CNN 係咪做唔到呢? 其實又唔係, 而且仲準過人, 不過事先你輸入既要過濾清晒一定全部都係狗, 人腦既強大之處係generalized problem solving<br />\n<br />\n人腦你可以想像佢係chain of cluster of nodes.而每組cluster 所做既野都唔同, 呢野大把paper 講過,至於分到幾細就未有人知, 呢D 等科學家去研究<br />\n但佢地研究到之前我地係咪就唔能夠作出一D 大膽既假設呢?<br />\n點解你覺得bayesian brain/probabilistic graphical model  唔係AI 呢?<br />\nAI 點解唔可以係neural network of value evaluation nodes/cluster of nodes, 而個value evaluation method 點解要局限左係ANN/CNN, regression?bayesian brain/probabilistic graphical model, arithmetic equation一樣可以係其中一種value evaluation 既方法<br />\n而因為人腦既neural 數目實在係太多,其實佢係一次過chain 埋左classification, value evaluation, inference, prediction而且好可能係每樣都反覆咁做N 次,所以佢先可以咁generalized<br />\nAI 係咪做唔到呢?依家所有puzzle 都有齊, 之不過係有無咁多computing power 姐....<br />\n<br />\nunderfitting not  necessary 係因為complexity 太低, 你諗野太practical,退後D 睇<br />\nunderfitting 係因為對問題/答案over generalization</blockquote><br />\n<br />\n如果話人黎講少sample都高accuracy因為我地不自覺間收集到好左好多數據。咁即係話人類高accuracy 係都係因為多sample, 但連deepmind 而家做one-shot都唔洗多sample 喎。咁即係話多sample唔係accuracy高嘅必然條件，係同個architecture有關。仲有我唔係話CNN分唔到狗嘅品種，我係話train左黎識別人面嘅唔可以同時用黎分辨狗嘅品種，ANN而家transfer learning仲係好差。呢個係architectural 層次問題。<br />\n<br />\n其實我邊度有講過graphical model 唔係AI <img src=\"/assets/faces/normal/wonder.gif\" class=\"hkgmoji\" /> 我由頭到尾只係講ANN唔可以完全解釋人腦，intelligence system有好多種，ANN只係其中一種。ANN係一個數學模型，唔係描述人腦嘅模型。<br />\n<br />\n你所謂cluster of nodes理論有一個致命問題，就係假設左node同node之間係冇重疊的。但人腦就正正相反，同一個neuron係唔同task底下都有可能被activate。而且如果每個task都有一個cluster, 咁係咪用手拎杯一個cluster, 拎碟一個cluster, 拎波又另一個cluster呢，世事何其多，要幾多個cluster先夠...而train新cluster又要幾多時間？學識拎杯嘅人，亦會好快學識拎茶壺，咁又點理解呢？再者，大腦構造係可以靈活改變。盲人visual cortex可以repurpose做聽覺，觸覺，visual同聽覺根本需要唔同architecture, 前者CNN後者RNN，點解同一個node可以變得咁快？<br />\n<br />\nOver-generalize 同複雜度唔夠其實咪同一樣野？咁都可以話下人諗野太practical...<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> 其實我地都係純粹學術討論，點解最後都要加句人身攻擊呢...<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n咁你都要覺得係人身攻擊<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n依家d 人到底有幾玻璃?<br />\n政治正確同大愛呢家野真係害人不淺<br />\n學術討論唔加埋點解覺得對方既想法有不足又點樣互相提點?如果只係大家輪流掉theory 出嚟<br />\n不如自己去睇paper仲efficient?"},{"pid":"21ee465fae437ab80508e387f86a53a071926480","tid":257808,"uid":48679,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T13:41:23.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote>屌<br />\n又話簡單了解<br />\n愈講愈唔知你地up 乜<br />\n<br />\n成班係到鬥反駁，show quali<br />\n寫野又無structure<br />\n<br />\n入撚錯post on99</blockquote><br />\n突破盲腸<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n老實講我覺得一講到layer層面啲weights已經好難簡單了解<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n講左由淺入深去解釋原理<br />\n一黎就跳左去做reserach level<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n樓主有樓主講，其他巴打討論下有咩問題<img src=\"/assets/faces/lomoji/37.png\" class=\"hkgmoji\" /></blockquote><br />\n問題而家題目嘅關鍵字係「簡單了解」<br />\n你喺度講一堆專業詞彙出來咁你叫啲初學者點睇得明<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n衰啲講句而家大部分人都係離題<br />\n你要深入討論就應該出去開多個post慢慢討論，唔好搞到個post啲內容咁難明<br />\n呢個post嘅內容應該要簡單易明先鼓勵到多啲人認識咩係AI<br />\n唔係學咩人叫簡單了解？<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n無計架, 講D 簡單既/abstract version 就會有人跳出黎用terms/ PHD 黎撻你話你ON9 又俾膠咁<br />\n對住呢D人只能越講越technical, 你唔理呢班人又驚初學者入黎見到呢D 留言就以為呢個POST真係流既而出番去<br />\n你見我都絕大部份都補番個日常生活例子黎照顧下初學者, 不過果堆PHD就又會左搵右搵呢D 例子係details 上有乜不足...<br />\n初學者就等下樓主啦,暫時黎講我見樓主講既都適合初學者程度而且無乜致命既錯處"},{"pid":"58cb726f5e2701dda6e4a757990e0ecb2bf284ab","tid":257808,"uid":48796,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T13:43:05.000Z","msg":"<blockquote><blockquote><blockquote>屌<br />\n又話簡單了解<br />\n愈講愈唔知你地up 乜<br />\n<br />\n成班係到鬥反駁，show quali<br />\n寫野又無structure<br />\n<br />\n入撚錯post on99</blockquote><br />\n突破盲腸<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n老實講我覺得一講到layer層面啲weights已經好難簡單了解<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n講左由淺入深去解釋原理<br />\n一黎就跳左去做reserach level<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /></blockquote><br />\nAggr<br />\n一啲都唔layman <br />\n平時喺學術界拗嘅嘢，喺度拗多次有乜謂<br />\n研究人員應該幫手講下基本概念，AI而家發展情況，可見將來會發展成點，做到乜嘢，有乜限制，人類應該點自處 <img src=\"/assets/faces/normal/chicken.gif\" class=\"hkgmoji\" />"},{"pid":"f7bcb8ebfbb0929e82b4432741c42833fe52a216","tid":257808,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T13:50:10.000Z","msg":"<blockquote><blockquote><blockquote>屌<br />\n又話簡單了解<br />\n愈講愈唔知你地up 乜<br />\n<br />\n成班係到鬥反駁，show quali<br />\n寫野又無structure<br />\n<br />\n入撚錯post on99</blockquote><br />\n突破盲腸<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n老實講我覺得一講到layer層面啲weights已經好難簡單了解<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n講左由淺入深去解釋原理<br />\n一黎就跳左去做reserach level<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /></blockquote><br />\n要講weight要先講matrix係乜來，再extend去tensor<br />\n<br />\ninner product又可以點樣深入淺出呢<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n呢d係linear algebra 基本skills但要簡單去講真係唔易<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\nconvolution之前我都係一個post講過，CNN 呢樣野係CV入面好有效，但kernel, gradient呢類野又有排講<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"62c4490372caa78738b3f5bb0350b98564033b99","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T13:55:39.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>屌<br />\n又話簡單了解<br />\n愈講愈唔知你地up 乜<br />\n<br />\n成班係到鬥反駁，show quali<br />\n寫野又無structure<br />\n<br />\n入撚錯post on99</blockquote><br />\n突破盲腸<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n老實講我覺得一講到layer層面啲weights已經好難簡單了解<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n講左由淺入深去解釋原理<br />\n一黎就跳左去做reserach level<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n樓主有樓主講，其他巴打討論下有咩問題<img src=\"/assets/faces/lomoji/37.png\" class=\"hkgmoji\" /></blockquote><br />\n問題而家題目嘅關鍵字係「簡單了解」<br />\n你喺度講一堆專業詞彙出來咁你叫啲初學者點睇得明<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n衰啲講句而家大部分人都係離題<br />\n你要深入討論就應該出去開多個post慢慢討論，唔好搞到個post啲內容咁難明<br />\n呢個post嘅內容應該要簡單易明先鼓勵到多啲人認識咩係AI<br />\n唔係學咩人叫簡單了解？<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n無計架, 講D 簡單既/abstract version 就會有人跳出黎用terms/ PHD 黎撻你話你ON9 又俾膠咁<br />\n對住呢D人只能越講越technical, 你唔理呢班人又驚初學者入黎見到呢D 留言就以為呢個POST真係流既而出番去<br />\n你見我都絕大部份都補番個日常生活例子黎照顧下初學者, 不過果堆PHD就又會左搵右搵呢D 例子係details 上有乜不足...<br />\n初學者就等下樓主啦,暫時黎講我見樓主講既都適合初學者程度而且無乜致命既錯處</blockquote><br />\n但係而家係連樓主都比人屌唔夠layman<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"a3e54dc60b24e8ca6384a320c1fffda91e8ba993","tid":257808,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T13:59:50.000Z","msg":"簡單講下CNN結構，主要由幾款layer組成<br />\n<br />\n1. convolution (提取feature)<br />\n2. pooling (feature 簡化)<br />\n3. non-linearalize function (提高系統的非線性程度)<br />\n4. fully connected/ inner product (傳統ANN)<br />\n5. classification/ deconcolution(output最終結果)<br />\n<br />\n以前NN 唔可行因為所有pixel間都要行neuron connection, 10*10 的圖 output 去10*10 的layer就有 100^2 咁撚多的組合，CNN 用左convolution同pooling的技改提取image faeture進行簡化，咁一層layer用10個faeture kernel, 每個kernel 3*3, 咁只係得3*3*10 個neuron要fit, 令到NN 呢個想法實際可行"},{"pid":"53bdd134929a77d8d4672b4cdc8cc157f28f75f3","tid":257808,"uid":70989,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-27T14:01:08.000Z","msg":"<blockquote><blockquote><br />\n數據量問題其實我已經答左你, 人腦收集左好多數據而不自知, 而果D 數據正正就係人腦能夠加上少量supervised learning 既sample 之後就能夠得到高accuracy 既原因, 詳細你可以睇下One-shot learning 既相關資料<br />\n你講既分狗既品種, 依家CNN 係咪做唔到呢? 其實又唔係, 而且仲準過人, 不過事先你輸入既要過濾清晒一定全部都係狗, 人腦既強大之處係generalized problem solving<br />\n<br />\n人腦你可以想像佢係chain of cluster of nodes.而每組cluster 所做既野都唔同, 呢野大把paper 講過,至於分到幾細就未有人知, 呢D 等科學家去研究<br />\n但佢地研究到之前我地係咪就唔能夠作出一D 大膽既假設呢?<br />\n點解你覺得bayesian brain/probabilistic graphical model  唔係AI 呢?<br />\nAI 點解唔可以係neural network of value evaluation nodes/cluster of nodes, 而個value evaluation method 點解要局限左係ANN/CNN, regression?bayesian brain/probabilistic graphical model, arithmetic equation一樣可以係其中一種value evaluation 既方法<br />\n而因為人腦既neural 數目實在係太多,其實佢係一次過chain 埋左classification, value evaluation, inference, prediction而且好可能係每樣都反覆咁做N 次,所以佢先可以咁generalized<br />\nAI 係咪做唔到呢?依家所有puzzle 都有齊, 之不過係有無咁多computing power 姐....<br />\n<br />\nunderfitting not  necessary 係因為complexity 太低, 你諗野太practical,退後D 睇<br />\nunderfitting 係因為對問題/答案over generalization</blockquote><br />\n<br />\n如果話人黎講少sample都高accuracy因為我地不自覺間收集到好左好多數據。咁即係話人類高accuracy 係都係因為多sample, 但連deepmind 而家做one-shot都唔洗多sample 喎。咁即係話多sample唔係accuracy高嘅必然條件，係同個architecture有關。仲有我唔係話CNN分唔到狗嘅品種，我係話train左黎識別人面嘅唔可以同時用黎分辨狗嘅品種，ANN而家transfer learning仲係好差。呢個係architectural 層次問題。<br />\n<br />\n其實我邊度有講過graphical model 唔係AI <img src=\"/assets/faces/normal/wonder.gif\" class=\"hkgmoji\" /> 我由頭到尾只係講ANN唔可以完全解釋人腦，intelligence system有好多種，ANN只係其中一種。ANN係一個數學模型，唔係描述人腦嘅模型。<br />\n<br />\n你所謂cluster of nodes理論有一個致命問題，就係假設左node同node之間係冇重疊的。但人腦就正正相反，同一個neuron係唔同task底下都有可能被activate。而且如果每個task都有一個cluster, 咁係咪用手拎杯一個cluster, 拎碟一個cluster, 拎波又另一個cluster呢，世事何其多，要幾多個cluster先夠...而train新cluster又要幾多時間？學識拎杯嘅人，亦會好快學識拎茶壺，咁又點理解呢？再者，大腦構造係可以靈活改變。盲人visual cortex可以repurpose做聽覺，觸覺，visual同聽覺根本需要唔同architecture, 前者CNN後者RNN，點解同一個node可以變得咁快？<br />\n<br />\nOver-generalize 同複雜度唔夠其實咪同一樣野？咁都可以話下人諗野太practical...<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> 其實我地都係純粹學術討論，點解最後都要加句人身攻擊呢...<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n唔洗理呢條友，根本呢度係俾大家開放討論，佢成日驚俾堆PhD串就係度拋書包，根本PhD係唔會咁樣，我地做真實學術討論都會尊重人地既觀點，唔會下下人身攻擊人地。根本佢就係捉住你講既一兩句然後好唔logical咁criticize。既然佢咁鍾意講就講晒佢，廢事俾人話晒命啦。"},{"pid":"1758f5c01b02dfb69a64c86ca584a925630a7986","tid":257808,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:03:14.000Z","msg":"同傳統imaging AI 不同，algo designer唔使preset一堆image faeture去做fitting, CNN 可以自己搵到大堆image faeaure，並且比人類所能理解梗要抽象幾百倍，呢個就係CNN 的強大之處"},{"pid":"4e89452ada2dea45a227c188194f7ce650cd4203","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:03:43.000Z","msg":"<blockquote><blockquote><blockquote><blockquote>屌<br />\n又話簡單了解<br />\n愈講愈唔知你地up 乜<br />\n<br />\n成班係到鬥反駁，show quali<br />\n寫野又無structure<br />\n<br />\n入撚錯post on99</blockquote><br />\n突破盲腸<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n老實講我覺得一講到layer層面啲weights已經好難簡單了解<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n講左由淺入深去解釋原理<br />\n一黎就跳左去做reserach level<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /></blockquote><br />\nAggr<br />\n一啲都唔layman <br />\n平時喺學術界拗嘅嘢，喺度拗多次有乜謂<br />\n研究人員應該幫手講下基本概念，AI而家發展情況，可見將來會發展成點，做到乜嘢，有乜限制，人類應該點自處 <img src=\"/assets/faces/normal/chicken.gif\" class=\"hkgmoji\" /></blockquote><br />\n<img src=\"http://img.eservice-hk.net/upload/2017/05/27/215648_193c8ddf86b5f44e7b428ab257b5d70f.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fimg.eservice-hk.net%2Fupload%2F2017%2F05%2F27%2F215648_193c8ddf86b5f44e7b428ab257b5d70f.jpg&h=6e340c9d&s={SIZE}\" /><br />\n而家d人檢討緊點解決上面既error<br />\n<br />\n少量雜訊(noise)就搞到ai認柒左<br />\n<br />\n之前主流比較多都係object classification(字面解)<br />\n<br />\n而家就開始多左generative model<br />\n<br />\n即係講緊我同電腦講比張J圖我 要長腿大波<br />\n<br />\n咁電腦就generate一d長腿大波妹既相比你<br />\n(而唔係係harddisk入面搵佢珍藏既j圖比你)<br />\n<br />\n不過so far呢方面都未成熟 仲係好多一睇都知唔係路既j圖會出左黎"},{"pid":"4d71033a0922f3f2093d0346a4b2ede097594ef1","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:05:13.000Z","msg":"<blockquote>簡單講下CNN結構，主要由幾款layer組成<br />\n<br />\n1. convolution (提取feature)<br />\n2. pooling (feature 簡化)<br />\n3. non-linearalize function (提高系統的非線性程度)<br />\n4. fully connected/ inner product (傳統ANN)<br />\n5. classification/ deconcolution(output最終結果)<br />\n<br />\n以前NN 唔可行因為所有pixel間都要行neuron connection, 10*10 的圖 output 去10*10 的layer就有 100^2 咁撚多的組合，CNN 用左convolution同pooling的技改提取image faeture進行簡化，咁一層layer用10個faeture kernel, 每個kernel 3*3, 咁只係得3*3*10 個neuron要fit, 令到NN 呢個想法實際可行</blockquote><br />\n<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> 太複雜啦 等比人屌啦你"},{"pid":"06b2bbd8847230fe99307cef321ff92daca8dc94","tid":257808,"uid":28279,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:08:04.000Z","msg":"<img src=\"/assets/faces/lomoji/28.png\" class=\"hkgmoji\" /> 簡單嚟講即係點？"},{"pid":"d456b3931172723a8191d8509ea069f22c89d846","tid":257808,"uid":48679,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:08:27.000Z","msg":"<blockquote><br />\n呢種層層classify嘅理論可以從實驗層面就可以證明唔係完全正確。例如一啲講求速度嘅運動，e.g. 乒乓球，羽毛球，劍擊etc .，運動員可以係極短時間內作出反應。依照層層classify嘅理論，見到一個乒乓波飛埋黎要作出反應要經過好多步，例如先要辨別到係一個乒乓波，分辨眼前係現實定電視形像，判斷自己係咪應該打佢，然後估算佢嘅位置同方向，估算自己手嘅位置同方向，計算出手嘅trajectory，判斷呢個trajectory係咪合理，計算各肌肉的用力的比例，然後打出去。如果層層classify,呢度每一個判斷都涉及極複雜的network，而neuron同neuron之間有synaptic delay，速度係上限的，計得黎個波一早就飛走左。但其實研究顯示，對一個task越純熟，大腦就越少activation。大腦會自動調節去specialize一個task, 唔係一定要層層去classify.<br />\n<br />\n而天才都有好多種，全能型天才都有。達文西就工程畫畫樣樣精，愛因斯坦都好有音樂天份，亦唔見佢有咩其他明顯缺陷。人腦並唔係over-generalize,而係佢本身有個好靈活嘅架構，令到佢可以適應唔同嘅環境。世上有好多超乎常人嘅運動家，音樂家，數學家，呢啲都係人可以specialize嘅證明。有缺陷嘅天才好多只係因為佢地被報道得多，其實果啲缺陷唔係天才都一樣會有，亦有好多天才一生正正常常。因為就大概而論話人腦over-generalize未必太過武斷。</blockquote><br />\n<br />\nsynaptic delay does not exist in electrotonic transmission. 所以你例子不成立<br />\n不過唔再講太technical 啦, 廢事班初學者睇到一舊舊<br />\n<br />\n你要睇係真係天才定係後天努力既&quot;天才&quot;<br />\n假如個腦本身構造已經係特化左, 一定係幾歲就顯現到出黎,依呢D 基本上好大部份都係有缺陷<br />\ngenius 同austim 係positive correlation 係已經Proof 左<br />\n而後天既天才, 佢地只係train 佢地既network train 得好好, 而唔係佢個network 本身就特化左<br />\n所以佢地其他能力普遍都同其他人相若"},{"pid":"f0b04f85764116f012ab4a8cddc6f38aa75743d5","tid":257808,"uid":104138,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:08:42.000Z","msg":"<blockquote><blockquote>屌<br />\n又話簡單了解<br />\n愈講愈唔知你地up 乜<br />\n<br />\n成班係到鬥反駁，show quali<br />\n寫野又無structure<br />\n<br />\n入撚錯post on99</blockquote><br />\n突破盲腸<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n老實講我覺得一講到layer層面啲weights已經好難簡單了解<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n連登D 學術post係咁,入到黎就見到此post已完同咩收皮真係好撚on9,邊度錯又唔講, 大佬呀show quali 就咁打句此post已完唔會覺得你,只會覺得你on9"},{"pid":"7005b68df670e78ae8b5f9607adc67a20886b375","tid":257808,"uid":45094,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:09:54.000Z","msg":"<img src=\"/assets/faces/normal/hoho.gif\" class=\"hkgmoji\" />"},{"pid":"e561c54b1e1a146a181a4bff0af267e17fabcadf","tid":257808,"uid":48679,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:12:27.000Z","msg":"<blockquote><blockquote><blockquote><br />\n數據量問題其實我已經答左你, 人腦收集左好多數據而不自知, 而果D 數據正正就係人腦能夠加上少量supervised learning 既sample 之後就能夠得到高accuracy 既原因, 詳細你可以睇下One-shot learning 既相關資料<br />\n你講既分狗既品種, 依家CNN 係咪做唔到呢? 其實又唔係, 而且仲準過人, 不過事先你輸入既要過濾清晒一定全部都係狗, 人腦既強大之處係generalized problem solving<br />\n<br />\n人腦你可以想像佢係chain of cluster of nodes.而每組cluster 所做既野都唔同, 呢野大把paper 講過,至於分到幾細就未有人知, 呢D 等科學家去研究<br />\n但佢地研究到之前我地係咪就唔能夠作出一D 大膽既假設呢?<br />\n點解你覺得bayesian brain/probabilistic graphical model  唔係AI 呢?<br />\nAI 點解唔可以係neural network of value evaluation nodes/cluster of nodes, 而個value evaluation method 點解要局限左係ANN/CNN, regression?bayesian brain/probabilistic graphical model, arithmetic equation一樣可以係其中一種value evaluation 既方法<br />\n而因為人腦既neural 數目實在係太多,其實佢係一次過chain 埋左classification, value evaluation, inference, prediction而且好可能係每樣都反覆咁做N 次,所以佢先可以咁generalized<br />\nAI 係咪做唔到呢?依家所有puzzle 都有齊, 之不過係有無咁多computing power 姐....<br />\n<br />\nunderfitting not  necessary 係因為complexity 太低, 你諗野太practical,退後D 睇<br />\nunderfitting 係因為對問題/答案over generalization</blockquote><br />\n<br />\n如果話人黎講少sample都高accuracy因為我地不自覺間收集到好左好多數據。咁即係話人類高accuracy 係都係因為多sample, 但連deepmind 而家做one-shot都唔洗多sample 喎。咁即係話多sample唔係accuracy高嘅必然條件，係同個architecture有關。仲有我唔係話CNN分唔到狗嘅品種，我係話train左黎識別人面嘅唔可以同時用黎分辨狗嘅品種，ANN而家transfer learning仲係好差。呢個係architectural 層次問題。<br />\n<br />\n其實我邊度有講過graphical model 唔係AI <img src=\"/assets/faces/normal/wonder.gif\" class=\"hkgmoji\" /> 我由頭到尾只係講ANN唔可以完全解釋人腦，intelligence system有好多種，ANN只係其中一種。ANN係一個數學模型，唔係描述人腦嘅模型。<br />\n<br />\n你所謂cluster of nodes理論有一個致命問題，就係假設左node同node之間係冇重疊的。但人腦就正正相反，同一個neuron係唔同task底下都有可能被activate。而且如果每個task都有一個cluster, 咁係咪用手拎杯一個cluster, 拎碟一個cluster, 拎波又另一個cluster呢，世事何其多，要幾多個cluster先夠...而train新cluster又要幾多時間？學識拎杯嘅人，亦會好快學識拎茶壺，咁又點理解呢？再者，大腦構造係可以靈活改變。盲人visual cortex可以repurpose做聽覺，觸覺，visual同聽覺根本需要唔同architecture, 前者CNN後者RNN，點解同一個node可以變得咁快？<br />\n<br />\nOver-generalize 同複雜度唔夠其實咪同一樣野？咁都可以話下人諗野太practical...<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> 其實我地都係純粹學術討論，點解最後都要加句人身攻擊呢...<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n唔洗理呢條友，根本呢度係俾大家開放討論，佢成日驚俾堆PhD串就係度拋書包，根本PhD係唔會咁樣，我地做真實學術討論都會尊重人地既觀點，唔會下下人身攻擊人地。根本佢就係捉住你講既一兩句然後好唔logical咁criticize。既然佢咁鍾意講就講晒佢，廢事俾人話晒命啦。</blockquote><br />\n開放討論但係掉低舊膠and留個PHD 就走左去, 你D討論真係<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"b6b4e22823876d2f4471d894e39e0b32c3dea50d","tid":257808,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:13:38.000Z","msg":"<blockquote><blockquote>簡單講下CNN結構，主要由幾款layer組成<br />\n<br />\n1. convolution (提取feature)<br />\n2. pooling (feature 簡化)<br />\n3. non-linearalize function (提高系統的非線性程度)<br />\n4. fully connected/ inner product (傳統ANN)<br />\n5. classification/ deconcolution(output最終結果)<br />\n<br />\n以前NN 唔可行因為所有pixel間都要行neuron connection, 10*10 的圖 output 去10*10 的layer就有 100^2 咁撚多的組合，CNN 用左convolution同pooling的技改提取image faeture進行簡化，咁一層layer用10個faeture kernel, 每個kernel 3*3, 咁只係得3*3*10 個neuron要fit, 令到NN 呢個想法實際可行</blockquote><br />\n<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> 太複雜啦 等比人屌啦你</blockquote><br />\n屌你咩已經無得再淺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n一係由咩叫kernel，咩係convolution開始講<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n簡單d就係俾一堆圖(e.g. 車)俾電腦睇，淨係同佢講呢樣野叫車，電腦自動搵&quot;車&quot;的feature，e.g. 有碌，有窗，有bumper, 再深多兩層就人都理解唔到的東西，叫feature vector，搵一大堆數function去描述d 特徵<br />\n<br />\nCNN 同人類局域視角好有關，但我唔想講人類點樣去理解世界呢樣野<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"c022cf91b09eacbf07eb703f40b1d4ec422bf4c4","tid":257808,"uid":34457,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:16:04.000Z","msg":"lm"},{"pid":"e806efa5e98a8d73cdac8cdbe3da4844850e6a85","tid":257808,"uid":48679,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:16:10.000Z","msg":"<blockquote><blockquote><blockquote>屌<br />\n又話簡單了解<br />\n愈講愈唔知你地up 乜<br />\n<br />\n成班係到鬥反駁，show quali<br />\n寫野又無structure<br />\n<br />\n入撚錯post on99</blockquote><br />\n突破盲腸<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n老實講我覺得一講到layer層面啲weights已經好難簡單了解<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n連登D 學術post係咁,入到黎就見到此post已完同咩收皮真係好撚on9,邊度錯又唔講, 大佬呀show quali 就咁打句此post已完唔會覺得你,只會覺得你on9</blockquote><br />\n留名等睇你俾人屌你人身攻擊"},{"pid":"eaa1f9034c44bb8fdb88c20b06cf9c4c1aba5e64","tid":257808,"uid":28279,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:17:27.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote>屌<br />\n又話簡單了解<br />\n愈講愈唔知你地up 乜<br />\n<br />\n成班係到鬥反駁，show quali<br />\n寫野又無structure<br />\n<br />\n入撚錯post on99</blockquote><br />\n突破盲腸<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n老實講我覺得一講到layer層面啲weights已經好難簡單了解<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n講左由淺入深去解釋原理<br />\n一黎就跳左去做reserach level<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /></blockquote><br />\nAggr<br />\n一啲都唔layman <br />\n平時喺學術界拗嘅嘢，喺度拗多次有乜謂<br />\n研究人員應該幫手講下基本概念，AI而家發展情況，可見將來會發展成點，做到乜嘢，有乜限制，人類應該點自處 <img src=\"/assets/faces/normal/chicken.gif\" class=\"hkgmoji\" /></blockquote><br />\n<img src=\"http://img.eservice-hk.net/upload/2017/05/27/215648_193c8ddf86b5f44e7b428ab257b5d70f.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fimg.eservice-hk.net%2Fupload%2F2017%2F05%2F27%2F215648_193c8ddf86b5f44e7b428ab257b5d70f.jpg&h=6e340c9d&s={SIZE}\" /><br />\n而家d人檢討緊點解決上面既error<br />\n<br />\n少量雜訊(noise)就搞到ai認柒左<br />\n<br />\n之前主流比較多都係object classification(字面解)<br />\n<br />\n而家就開始多左generative model<br />\n<br />\n即係講緊我同電腦講比張J圖我 要長腿大波<br />\n<br />\n咁電腦就generate一d長腿大波妹既相比你<br />\n(而唔係係harddisk入面搵佢珍藏既j圖比你)<br />\n<br />\n不過so far呢方面都未成熟 仲係好多一睇都知唔係路既j圖會出左黎</blockquote><br />\n<img src=\"/assets/faces/lomoji/26.png\" class=\"hkgmoji\" /> 係咪因為電腦係逐粒pixel睇？但我記得好似有人整到電腦識得睇圖搵啲圓圈直線既形狀出嚟<br />\n加上google又好似整咗個你畫簡單線條公仔就識認係乜東東圖，係咪已經解決咗問題？"},{"pid":"642e3ecd07b013d2033fcfd6f9dab0f73d83c635","tid":257808,"uid":104138,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:18:02.000Z","msg":"<blockquote><blockquote><blockquote>簡單講下CNN結構，主要由幾款layer組成<br />\n<br />\n1. convolution (提取feature)<br />\n2. pooling (feature 簡化)<br />\n3. non-linearalize function (提高系統的非線性程度)<br />\n4. fully connected/ inner product (傳統ANN)<br />\n5. classification/ deconcolution(output最終結果)<br />\n<br />\n以前NN 唔可行因為所有pixel間都要行neuron connection, 10*10 的圖 output 去10*10 的layer就有 100^2 咁撚多的組合，CNN 用左convolution同pooling的技改提取image faeture進行簡化，咁一層layer用10個faeture kernel, 每個kernel 3*3, 咁只係得3*3*10 個neuron要fit, 令到NN 呢個想法實際可行</blockquote><br />\n<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> 太複雜啦 等比人屌啦你</blockquote><br />\n屌你咩已經無得再淺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n一係由咩叫kernel，咩係convolution開始講<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n簡單d就係俾一堆圖(e.g. 車)俾電腦睇，淨係同佢講呢樣野叫車，電腦自動搵&quot;車&quot;的feature，e.g. 有碌，有窗，有bumper, 再深多兩層就人都理解唔到的東西，叫feature vector，搵一大堆數function去描述d 特徵<br />\n<br />\nCNN 同人類局域視角好有關，但我唔想講人類點樣去理解世界呢樣野<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n巴打我記得你,你係香港定外國讀PG? 諗緊想出國讀書"},{"pid":"c01cde39c335aacb1ed321e5d12b5f197f785b95","tid":257808,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:20:56.000Z","msg":"CNN 原理同你俾一大堆中文字個鬼佬去背但唔教佢點解差唔多，佢最後會歸納到&quot;人&quot;呢隻字有一捌同捺兩樣野，呢個寫法係&quot;人&quot;字，但唔會知背後意思<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"768c3a09dd972c97a10a4a4ed9fe9590a7416bbb","tid":257808,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:21:51.000Z","msg":"<blockquote><blockquote><blockquote><blockquote>簡單講下CNN結構，主要由幾款layer組成<br />\n<br />\n1. convolution (提取feature)<br />\n2. pooling (feature 簡化)<br />\n3. non-linearalize function (提高系統的非線性程度)<br />\n4. fully connected/ inner product (傳統ANN)<br />\n5. classification/ deconcolution(output最終結果)<br />\n<br />\n以前NN 唔可行因為所有pixel間都要行neuron connection, 10*10 的圖 output 去10*10 的layer就有 100^2 咁撚多的組合，CNN 用左convolution同pooling的技改提取image faeture進行簡化，咁一層layer用10個faeture kernel, 每個kernel 3*3, 咁只係得3*3*10 個neuron要fit, 令到NN 呢個想法實際可行</blockquote><br />\n<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> 太複雜啦 等比人屌啦你</blockquote><br />\n屌你咩已經無得再淺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n一係由咩叫kernel，咩係convolution開始講<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n簡單d就係俾一堆圖(e.g. 車)俾電腦睇，淨係同佢講呢樣野叫車，電腦自動搵&quot;車&quot;的feature，e.g. 有碌，有窗，有bumper, 再深多兩層就人都理解唔到的東西，叫feature vector，搵一大堆數function去描述d 特徵<br />\n<br />\nCNN 同人類局域視角好有關，但我唔想講人類點樣去理解世界呢樣野<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n巴打我記得你,你係香港定外國讀PG? 諗緊想出國讀書</blockquote><br />\n香港讀完MPhil, Phd 唔諗住因為有份好的offer<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"35d0b8f039cba961ad5661d0af5262acb6f2ea78","tid":257808,"uid":74327,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:22:50.000Z","msg":"Lm"},{"pid":"f213ff80f6b47ef82b71ea9ca89064ce1c5e1f0e","tid":257808,"uid":38550,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:28:10.000Z","msg":"<blockquote>同傳統imaging AI 不同，algo designer唔使preset一堆image faeture去做fitting, CNN 可以自己搵到大堆image faeaure，並且比人類所能理解梗要抽象幾百倍，呢個就係CNN 的強大之處</blockquote><br />\n講 image processing 我又可以撘少少咀<br />\n<br />\n其實就算e+ 大部分圖像識別都係要先做一定處理程序提取有用而精簡資訊先<br />\n<br />\n例如視象監測系統<br />\n最常見先將圖像轉為黑白<br />\n然後透過high pass filter 搵出物件邊緣<br />\n再用一系列轉換搵出各種圖形資訊<br />\n最後先將資訊放入learning machine <br />\n<br />\n問題就係前期處理為固定情境度身訂造<br />\n只要環境或者要求有變化就全盤用唔到<br />\n但好處係效率高<br />\n只要用啱方法只要少量資源就可以實現<br />\n<br />\n由我呢啲門外漢講可能多啲人明 <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n又唔怕俾人插<br />\n插我當學嘢 <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"5a6db82a1afcd74fab271d35f678088db4150a43","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:36:48.000Z","msg":"<img src=\"/assets/faces/normal/fuck.gif\" class=\"hkgmoji\" /><br />\n呢一個course instructor係ImageNet既founder, 我剩係想show下佢最上面果個prediction<br />\n<br />\n而ImageNet就係一個project講緊收左大量image同埋佢地相應既label, 再比ai作為一個learning resources (或者我地叫groundtruth(?), training data)<br />\n<br />\nAI就會根據返自己predict出黎既result同training data對答案，錯曬就大改，唔岩少少就改少d，咁至於點定義「錯幾多」就係另一個數學問題啦<br />\n<br />\n咁同埋你唔能夠確保你學既野(data)一定係岩，即係你做問卷調查都會有人鳩填啦，咁所以有一個叫learning rate去決定你每一次學野，改變你自己幾多，<br />\nlearning rate高，意味住「左耳入右耳出」，即係你每學一個data就洗曬之前d野(有錯請指出 好似唔係講得幾好)<br />\nlearning rate低，就慢工出細貨，真係真係會好慢<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\nIT狗角度：<br />\n真心果句, CS field已經係最易google到答案 打d keyword真係一大堆ans,真係再唔明就睇youtube,想知多d就一定係用research既方法,whether你係phd定industry<br />\n<br />\n至於數底,唯有自己睇多d linear algebra, 要知道成個whole picture就避唔開數<br />\n<br />\n對文科人黎講 我諗得d visualization到既先易講得明<br />\n可惜既係neural network visualize左都冇乜用 所以先比人話係black box"},{"pid":"ebcc10c09a7176481c367ed6c8c125c04de3c1e5","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:39:46.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>屌<br />\n又話簡單了解<br />\n愈講愈唔知你地up 乜<br />\n<br />\n成班係到鬥反駁，show quali<br />\n寫野又無structure<br />\n<br />\n入撚錯post on99</blockquote><br />\n突破盲腸<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n老實講我覺得一講到layer層面啲weights已經好難簡單了解<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n講左由淺入深去解釋原理<br />\n一黎就跳左去做reserach level<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /></blockquote><br />\nAggr<br />\n一啲都唔layman <br />\n平時喺學術界拗嘅嘢，喺度拗多次有乜謂<br />\n研究人員應該幫手講下基本概念，AI而家發展情況，可見將來會發展成點，做到乜嘢，有乜限制，人類應該點自處 <img src=\"/assets/faces/normal/chicken.gif\" class=\"hkgmoji\" /></blockquote><br />\n<img src=\"http://img.eservice-hk.net/upload/2017/05/27/215648_193c8ddf86b5f44e7b428ab257b5d70f.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fimg.eservice-hk.net%2Fupload%2F2017%2F05%2F27%2F215648_193c8ddf86b5f44e7b428ab257b5d70f.jpg&h=6e340c9d&s={SIZE}\" /><br />\n而家d人檢討緊點解決上面既error<br />\n<br />\n少量雜訊(noise)就搞到ai認柒左<br />\n<br />\n之前主流比較多都係object classification(字面解)<br />\n<br />\n而家就開始多左generative model<br />\n<br />\n即係講緊我同電腦講比張J圖我 要長腿大波<br />\n<br />\n咁電腦就generate一d長腿大波妹既相比你<br />\n(而唔係係harddisk入面搵佢珍藏既j圖比你)<br />\n<br />\n不過so far呢方面都未成熟 仲係好多一睇都知唔係路既j圖會出左黎</blockquote><br />\n<img src=\"/assets/faces/lomoji/26.png\" class=\"hkgmoji\" /> 係咪因為電腦係逐粒pixel睇？但我記得好似有人整到電腦識得睇圖搵啲圓圈直線既形狀出嚟<br />\n加上google又好似整咗個你畫簡單線條公仔就識認係乜東東圖，係咪已經解決咗問題？</blockquote><br />\n第二同第三行我諗太似樣<br />\n<br />\n第一張我估係當左cell/microscopy image<br />\n<br />\n唔係逐粒逐粒睇，會睇埋隔離d pixel，that's why there is convolution"},{"pid":"c2666c3791dfd454eaf210e18a3333f9d8284124","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:43:15.000Z","msg":"<blockquote><blockquote>同傳統imaging AI 不同，algo designer唔使preset一堆image faeture去做fitting, CNN 可以自己搵到大堆image faeaure，並且比人類所能理解梗要抽象幾百倍，呢個就係CNN 的強大之處</blockquote><br />\n講 image processing 我又可以撘少少咀<br />\n<br />\n其實就算e+ 大部分圖像識別都係要先做一定處理程序提取有用而精簡資訊先<br />\n<br />\n<span style=\"color: red;\">例如視象監測系統</span><br />\n最常見先將圖像轉為黑白<br />\n然後透過high pass filter 搵出物件邊緣<br />\n再用一系列轉換搵出各種圖形資訊<br />\n最後先將資訊放入learning machine <br />\n<br />\n問題就係前期處理為固定情境度身訂造<br />\n只要環境或者要求有變化就全盤用唔到<br />\n但好處係效率高<br />\n只要用啱方法只要少量資源就可以實現<br />\n<br />\n由我呢啲門外漢講可能多啲人明 <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n又唔怕俾人插<br />\n插我當學嘢 <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /></blockquote><br />\n你講緊motion estimation?"},{"pid":"a3d5b7aa4407f8f3b13298b1f2ac11d92f4b0020","tid":257808,"uid":43570,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:50:27.000Z","msg":"<blockquote><blockquote><br />\n呢種層層classify嘅理論可以從實驗層面就可以證明唔係完全正確。例如一啲講求速度嘅運動，e.g. 乒乓球，羽毛球，劍擊etc .，運動員可以係極短時間內作出反應。依照層層classify嘅理論，見到一個乒乓波飛埋黎要作出反應要經過好多步，例如先要辨別到係一個乒乓波，分辨眼前係現實定電視形像，判斷自己係咪應該打佢，然後估算佢嘅位置同方向，估算自己手嘅位置同方向，計算出手嘅trajectory，判斷呢個trajectory係咪合理，計算各肌肉的用力的比例，然後打出去。如果層層classify,呢度每一個判斷都涉及極複雜的network，而neuron同neuron之間有synaptic delay，速度係上限的，計得黎個波一早就飛走左。但其實研究顯示，對一個task越純熟，大腦就越少activation。大腦會自動調節去specialize一個task, 唔係一定要層層去classify.<br />\n<br />\n而天才都有好多種，全能型天才都有。達文西就工程畫畫樣樣精，愛因斯坦都好有音樂天份，亦唔見佢有咩其他明顯缺陷。人腦並唔係over-generalize,而係佢本身有個好靈活嘅架構，令到佢可以適應唔同嘅環境。世上有好多超乎常人嘅運動家，音樂家，數學家，呢啲都係人可以specialize嘅證明。有缺陷嘅天才好多只係因為佢地被報道得多，其實果啲缺陷唔係天才都一樣會有，亦有好多天才一生正正常常。因為就大概而論話人腦over-generalize未必太過武斷。</blockquote><br />\n<br />\nsynaptic delay does not exist in electrotonic transmission. 所以你例子不成立<br />\n不過唔再講太technical 啦, 廢事班初學者睇到一舊舊<br />\n<br />\n你要睇係真係天才定係後天努力既&quot;天才&quot;<br />\n假如個腦本身構造已經係特化左, 一定係幾歲就顯現到出黎,依呢D 基本上好大部份都係有缺陷<br />\ngenius 同austim 係positive correlation 係已經Proof 左<br />\n而後天既天才, 佢地只係train 佢地既network train 得好好, 而唔係佢個network 本身就特化左<br />\n所以佢地其他能力普遍都同其他人相若</blockquote><br />\n我都有諗過electrical synapse, 但electrical synapse係冇gain, 即係無辦法做複雜運算，佢主要係用黎synchronize一堆post-synaptic neurons. 人腦大部份都係chemical synapse.<br />\n<br />\n好多神童都好正常喎，Mozart同Terence Tao 呢？其實autism中間天才好少，有研究指出大部份都有智力問題，只有3%係above average:<br />\n<a href=\"https://www.ncbi.nlm.nih.gov/pubmed/21272389\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpubmed%2F21272389&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=6f4ac9dd\" data-auto-link target=\"_blank\">https://www.ncbi.nlm.nih.gov/pubmed/21272389</a><br />\n<br />\n關於事實性的錯誤，我覺得自己有責任指出，唔想有人誤會。<br />\n<br />\nTo其他巴打：<br />\n我都知自己hijack左個post嫁啦，各位巴打對唔住！只係一時興起所以多口左。多有得罪唔好意思 <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> 繼續潛水做番CD-ROM..."},{"pid":"d9f16d0b19c1f24f05b7590fd916ac37c850d39b","tid":257808,"uid":28279,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:55:36.000Z","msg":"<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> btw原來lihkg咁多研究生"},{"pid":"5588d5b8ac189a1ed5ae1abe37ae002b6f667de2","tid":257808,"uid":30710,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:56:41.000Z","msg":"好深<img src=\"/assets/faces/normal/cry.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/cry.gif\" class=\"hkgmoji\" /> 唔明"},{"pid":"e4bd71e6b752e1a42ad6d579f3bcf762884ed80b","tid":257808,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:58:50.000Z","msg":"<blockquote><blockquote><blockquote>同傳統imaging AI 不同，algo designer唔使preset一堆image faeture去做fitting, CNN 可以自己搵到大堆image faeaure，並且比人類所能理解梗要抽象幾百倍，呢個就係CNN 的強大之處</blockquote><br />\n講 image processing 我又可以撘少少咀<br />\n<br />\n其實就算e+ 大部分圖像識別都係要先做一定處理程序提取有用而精簡資訊先<br />\n<br />\n<span style=\"color: red;\">例如視象監測系統</span><br />\n最常見先將圖像轉為黑白<br />\n然後透過high pass filter 搵出物件邊緣<br />\n再用一系列轉換搵出各種圖形資訊<br />\n最後先將資訊放入learning machine <br />\n<br />\n問題就係前期處理為固定情境度身訂造<br />\n只要環境或者要求有變化就全盤用唔到<br />\n但好處係效率高<br />\n只要用啱方法只要少量資源就可以實現<br />\n<br />\n由我呢啲門外漢講可能多啲人明 <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n又唔怕俾人插<br />\n插我當學嘢 <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /></blockquote><br />\n你講緊motion estimation?</blockquote><br />\n做到segmentation tracking就好易<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"95bef156523e30fb9fea53a928cc949bddf9ab12","tid":257808,"uid":83084,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T14:59:29.000Z","msg":"留名學野<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"d288fa14896b0ff7dd22df83839bdc17d59b0821","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T15:02:57.000Z","msg":"<blockquote><blockquote><blockquote><blockquote>同傳統imaging AI 不同，algo designer唔使preset一堆image faeture去做fitting, CNN 可以自己搵到大堆image faeaure，並且比人類所能理解梗要抽象幾百倍，呢個就係CNN 的強大之處</blockquote><br />\n講 image processing 我又可以撘少少咀<br />\n<br />\n其實就算e+ 大部分圖像識別都係要先做一定處理程序提取有用而精簡資訊先<br />\n<br />\n<span style=\"color: red;\">例如視象監測系統</span><br />\n最常見先將圖像轉為黑白<br />\n然後透過high pass filter 搵出物件邊緣<br />\n再用一系列轉換搵出各種圖形資訊<br />\n最後先將資訊放入learning machine <br />\n<br />\n問題就係前期處理為固定情境度身訂造<br />\n只要環境或者要求有變化就全盤用唔到<br />\n但好處係效率高<br />\n只要用啱方法只要少量資源就可以實現<br />\n<br />\n由我呢啲門外漢講可能多啲人明 <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n又唔怕俾人插<br />\n插我當學嘢 <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /></blockquote><br />\n你講緊motion estimation?</blockquote><br />\n做到segmentation tracking就好易<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n聽過prof講有d情況反而係用返motion tracking黎做segmentation<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"97e9011c8194f428457135552622769169065395","tid":257808,"uid":9769,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T15:04:41.000Z","msg":"( Show Blocked User - 老舉唔怕柒大 )"},{"pid":"daea2858776f0cc35a0caa330b0a3a1bab720e9d","tid":257808,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T15:05:02.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote>同傳統imaging AI 不同，algo designer唔使preset一堆image faeture去做fitting, CNN 可以自己搵到大堆image faeaure，並且比人類所能理解梗要抽象幾百倍，呢個就係CNN 的強大之處</blockquote><br />\n講 image processing 我又可以撘少少咀<br />\n<br />\n其實就算e+ 大部分圖像識別都係要先做一定處理程序提取有用而精簡資訊先<br />\n<br />\n<span style=\"color: red;\">例如視象監測系統</span><br />\n最常見先將圖像轉為黑白<br />\n然後透過high pass filter 搵出物件邊緣<br />\n再用一系列轉換搵出各種圖形資訊<br />\n最後先將資訊放入learning machine <br />\n<br />\n問題就係前期處理為固定情境度身訂造<br />\n只要環境或者要求有變化就全盤用唔到<br />\n但好處係效率高<br />\n只要用啱方法只要少量資源就可以實現<br />\n<br />\n由我呢啲門外漢講可能多啲人明 <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n又唔怕俾人插<br />\n插我當學嘢 <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /></blockquote><br />\n你講緊motion estimation?</blockquote><br />\n做到segmentation tracking就好易<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n聽過prof講有d情況反而係用返motion tracking黎做segmentation<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\noptical flow segmentation? 聽過但未用過<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"3ce009ca0352f39f9f6451735980c4d122b0abd1","tid":257808,"uid":30535,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T15:06:11.000Z","msg":"<blockquote><blockquote><blockquote>簡單講下CNN結構，主要由幾款layer組成<br />\n<br />\n1. convolution (提取feature)<br />\n2. pooling (feature 簡化)<br />\n3. non-linearalize function (提高系統的非線性程度)<br />\n4. fully connected/ inner product (傳統ANN)<br />\n5. classification/ deconcolution(output最終結果)<br />\n<br />\n以前NN 唔可行因為所有pixel間都要行neuron connection, 10*10 的圖 output 去10*10 的layer就有 100^2 咁撚多的組合，CNN 用左convolution同pooling的技改提取image faeture進行簡化，咁一層layer用10個faeture kernel, 每個kernel 3*3, 咁只係得3*3*10 個neuron要fit, 令到NN 呢個想法實際可行</blockquote><br />\n<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> 太複雜啦 等比人屌啦你</blockquote><br />\n屌你咩已經無得再淺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n一係由咩叫kernel，咩係convolution開始講<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n簡單d就係俾一堆圖(e.g. 車)俾電腦睇，淨係同佢講呢樣野叫車，電腦自動搵&quot;車&quot;的feature，e.g. 有碌，有窗，有bumper, 再深多兩層就人都理解唔到的東西，叫feature vector，搵一大堆數function去描述d 特徵<br />\n<br />\nCNN 同人類局域視角好有關，但我唔想講人類點樣去理解世界呢樣野<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n可唔可以講詳細少少點用convolution嚟搵feature?<img src=\"/assets/faces/normal/wonder.gif\" class=\"hkgmoji\" /> <br />\n<br />\n利申：知convolution同kernel乜嚟，講得數學啲都ok"},{"pid":"28d508db8b8225a846d7bd61b2e3a3c8407e9029","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T15:08:39.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>同傳統imaging AI 不同，algo designer唔使preset一堆image faeture去做fitting, CNN 可以自己搵到大堆image faeaure，並且比人類所能理解梗要抽象幾百倍，呢個就係CNN 的強大之處</blockquote><br />\n講 image processing 我又可以撘少少咀<br />\n<br />\n其實就算e+ 大部分圖像識別都係要先做一定處理程序提取有用而精簡資訊先<br />\n<br />\n<span style=\"color: red;\">例如視象監測系統</span><br />\n最常見先將圖像轉為黑白<br />\n然後透過high pass filter 搵出物件邊緣<br />\n再用一系列轉換搵出各種圖形資訊<br />\n最後先將資訊放入learning machine <br />\n<br />\n問題就係前期處理為固定情境度身訂造<br />\n只要環境或者要求有變化就全盤用唔到<br />\n但好處係效率高<br />\n只要用啱方法只要少量資源就可以實現<br />\n<br />\n由我呢啲門外漢講可能多啲人明 <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n又唔怕俾人插<br />\n插我當學嘢 <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /></blockquote><br />\n你講緊motion estimation?</blockquote><br />\n做到segmentation tracking就好易<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n聽過prof講有d情況反而係用返motion tracking黎做segmentation<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\noptical flow segmentation? 聽過但未用過<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\nultrasound motion tracking好難做 所以好似要reverse咁做"},{"pid":"62064bbaab0b46b99b8bf1035482c70a592081e6","tid":257808,"uid":70989,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-27T15:10:20.000Z","msg":"<blockquote><br />\n開放討論但係掉低舊膠and留個PHD 就走左去, 你D討論真係<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n我邊有掉低舊膠呀，我話支持樓主你有冇見到呀？宜家係話自己讀PhD已經俾你屌鳩我晒命，心諗我有咩得罪你？我冇時間貢獻討論係我唔啱，但係上面都有一大堆巴打貢獻緊啦。我都未見過咁撚串既人係度教人。"},{"pid":"5e250319f2f0e5450fda4035bdf752f12a637e2b","tid":257808,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T15:10:29.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>同傳統imaging AI 不同，algo designer唔使preset一堆image faeture去做fitting, CNN 可以自己搵到大堆image faeaure，並且比人類所能理解梗要抽象幾百倍，呢個就係CNN 的強大之處</blockquote><br />\n講 image processing 我又可以撘少少咀<br />\n<br />\n其實就算e+ 大部分圖像識別都係要先做一定處理程序提取有用而精簡資訊先<br />\n<br />\n<span style=\"color: red;\">例如視象監測系統</span><br />\n最常見先將圖像轉為黑白<br />\n然後透過high pass filter 搵出物件邊緣<br />\n再用一系列轉換搵出各種圖形資訊<br />\n最後先將資訊放入learning machine <br />\n<br />\n問題就係前期處理為固定情境度身訂造<br />\n只要環境或者要求有變化就全盤用唔到<br />\n但好處係效率高<br />\n只要用啱方法只要少量資源就可以實現<br />\n<br />\n由我呢啲門外漢講可能多啲人明 <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n又唔怕俾人插<br />\n插我當學嘢 <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /></blockquote><br />\n你講緊motion estimation?</blockquote><br />\n做到segmentation tracking就好易<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n聽過prof講有d情況反而係用返motion tracking黎做segmentation<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\noptical flow segmentation? 聽過但未用過<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\nultrasound motion tracking好難做 所以好似要reverse咁做</blockquote><br />\nUS track blood flow?<br />\n<br />\n你係中大orthopedics?"},{"pid":"faf512671aac968d623d358ddbe76987bd4d4446","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T15:12:29.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>同傳統imaging AI 不同，algo designer唔使preset一堆image faeture去做fitting, CNN 可以自己搵到大堆image faeaure，並且比人類所能理解梗要抽象幾百倍，呢個就係CNN 的強大之處</blockquote><br />\n講 image processing 我又可以撘少少咀<br />\n<br />\n其實就算e+ 大部分圖像識別都係要先做一定處理程序提取有用而精簡資訊先<br />\n<br />\n<span style=\"color: red;\">例如視象監測系統</span><br />\n最常見先將圖像轉為黑白<br />\n然後透過high pass filter 搵出物件邊緣<br />\n再用一系列轉換搵出各種圖形資訊<br />\n最後先將資訊放入learning machine <br />\n<br />\n問題就係前期處理為固定情境度身訂造<br />\n只要環境或者要求有變化就全盤用唔到<br />\n但好處係效率高<br />\n只要用啱方法只要少量資源就可以實現<br />\n<br />\n由我呢啲門外漢講可能多啲人明 <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n又唔怕俾人插<br />\n插我當學嘢 <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /></blockquote><br />\n你講緊motion estimation?</blockquote><br />\n做到segmentation tracking就好易<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n聽過prof講有d情況反而係用返motion tracking黎做segmentation<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\noptical flow segmentation? 聽過但未用過<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\nultrasound motion tracking好難做 所以好似要reverse咁做</blockquote><br />\nUS track blood flow?<br />\n<br />\n你係中大orthopedics?</blockquote><br />\n<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"f35b064c58faedcaa043a1e0e5d609b7843626a5","tid":257808,"uid":440,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T15:13:15.000Z","msg":"講d 唔講d 一次過出哂佢啦"},{"pid":"8b10722b32c80bca99509def896c4f6a32afcfce","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T15:17:23.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>同傳統imaging AI 不同，algo designer唔使preset一堆image faeture去做fitting, CNN 可以自己搵到大堆image faeaure，並且比人類所能理解梗要抽象幾百倍，呢個就係CNN 的強大之處</blockquote><br />\n講 image processing 我又可以撘少少咀<br />\n<br />\n其實就算e+ 大部分圖像識別都係要先做一定處理程序提取有用而精簡資訊先<br />\n<br />\n<span style=\"color: red;\">例如視象監測系統</span><br />\n最常見先將圖像轉為黑白<br />\n然後透過high pass filter 搵出物件邊緣<br />\n再用一系列轉換搵出各種圖形資訊<br />\n最後先將資訊放入learning machine <br />\n<br />\n問題就係前期處理為固定情境度身訂造<br />\n只要環境或者要求有變化就全盤用唔到<br />\n但好處係效率高<br />\n只要用啱方法只要少量資源就可以實現<br />\n<br />\n由我呢啲門外漢講可能多啲人明 <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n又唔怕俾人插<br />\n插我當學嘢 <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /></blockquote><br />\n你講緊motion estimation?</blockquote><br />\n做到segmentation tracking就好易<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n聽過prof講有d情況反而係用返motion tracking黎做segmentation<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\noptical flow segmentation? 聽過但未用過<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\nultrasound motion tracking好難做 所以好似要reverse咁做</blockquote><br />\nUS track blood flow?<br />\n<br />\n你係中大orthopedics?</blockquote><br />\n<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n唔一定係blood flow啦 blood flow好似難d同doppler imaging鬥<br />\n<br />\n最主要係US個PSF好煩 phase difference影響好大<br />\n<br />\n我本身諗住跟果位老細係做us姐 不過後尾冇跟到佢"},{"pid":"b88702ba8f8705baaef7d59a35332adea20fe62d","tid":257808,"uid":43570,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T15:18:14.000Z","msg":"<blockquote><blockquote><blockquote><blockquote>簡單講下CNN結構，主要由幾款layer組成<br />\n<br />\n1. convolution (提取feature)<br />\n2. pooling (feature 簡化)<br />\n3. non-linearalize function (提高系統的非線性程度)<br />\n4. fully connected/ inner product (傳統ANN)<br />\n5. classification/ deconcolution(output最終結果)<br />\n<br />\n以前NN 唔可行因為所有pixel間都要行neuron connection, 10*10 的圖 output 去10*10 的layer就有 100^2 咁撚多的組合，CNN 用左convolution同pooling的技改提取image faeture進行簡化，咁一層layer用10個faeture kernel, 每個kernel 3*3, 咁只係得3*3*10 個neuron要fit, 令到NN 呢個想法實際可行</blockquote><br />\n<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> 太複雜啦 等比人屌啦你</blockquote><br />\n屌你咩已經無得再淺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n一係由咩叫kernel，咩係convolution開始講<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n簡單d就係俾一堆圖(e.g. 車)俾電腦睇，淨係同佢講呢樣野叫車，電腦自動搵&quot;車&quot;的feature，e.g. 有碌，有窗，有bumper, 再深多兩層就人都理解唔到的東西，叫feature vector，搵一大堆數function去描述d 特徵<br />\n<br />\nCNN 同人類局域視角好有關，但我唔想講人類點樣去理解世界呢樣野<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n可唔可以講詳細少少點用convolution嚟搵feature?<img src=\"/assets/faces/normal/wonder.gif\" class=\"hkgmoji\" /> <br />\n<br />\n利申：知convolution同kernel乜嚟，講得數學啲都ok</blockquote><br />\nConvolution其實係一個spatial filter, 例如最簡單嘅Sobel kernel:<br />\n[1 0 -1;<br />\n2  0 -2;<br />\n1 0 -1]<br />\n就可以搵到圖形的邊界：<br />\nbefore:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_original_(1).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_original_%281%29.PNG&h=33d303a3&s={SIZE}\" /><br />\nafter:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_sobel_(3).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_sobel_%283%29.PNG&h=ee5959a1&s={SIZE}\" /><br />\n唔同kernel 會產生唔同feature, 所以CNN個kernel都係要由data train出黎。"},{"pid":"696977c906e4c072ecf0edc0d3b5bf3e75b67ca1","tid":257808,"uid":43570,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T15:19:27.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote>簡單講下CNN結構，主要由幾款layer組成<br />\n<br />\n1. convolution (提取feature)<br />\n2. pooling (feature 簡化)<br />\n3. non-linearalize function (提高系統的非線性程度)<br />\n4. fully connected/ inner product (傳統ANN)<br />\n5. classification/ deconcolution(output最終結果)<br />\n<br />\n以前NN 唔可行因為所有pixel間都要行neuron connection, 10*10 的圖 output 去10*10 的layer就有 100^2 咁撚多的組合，CNN 用左convolution同pooling的技改提取image faeture進行簡化，咁一層layer用10個faeture kernel, 每個kernel 3*3, 咁只係得3*3*10 個neuron要fit, 令到NN 呢個想法實際可行</blockquote><br />\n<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> 太複雜啦 等比人屌啦你</blockquote><br />\n屌你咩已經無得再淺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n一係由咩叫kernel，咩係convolution開始講<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n簡單d就係俾一堆圖(e.g. 車)俾電腦睇，淨係同佢講呢樣野叫車，電腦自動搵&quot;車&quot;的feature，e.g. 有碌，有窗，有bumper, 再深多兩層就人都理解唔到的東西，叫feature vector，搵一大堆數function去描述d 特徵<br />\n<br />\nCNN 同人類局域視角好有關，但我唔想講人類點樣去理解世界呢樣野<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n可唔可以講詳細少少點用convolution嚟搵feature?<img src=\"/assets/faces/normal/wonder.gif\" class=\"hkgmoji\" /> <br />\n<br />\n利申：知convolution同kernel乜嚟，講得數學啲都ok</blockquote><br />\nConvolution其實係一個spatial filter, 例如最簡單嘅Sobel kernel:<br />\n[1 0 -1;<br />\n2  0 -2;<br />\n1 0 -1]<br />\n就可以搵到圖形的邊界：<br />\nbefore:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_original_(1).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_original_%281%29.PNG&h=33d303a3&s={SIZE}\" /><br />\nafter:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_sobel_(3).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_sobel_%283%29.PNG&h=ee5959a1&s={SIZE}\" /><br />\n唔同kernel 會產生唔同feature, 所以CNN個kernel都係要由data train出黎。</blockquote><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f0/Valve_original_%281%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Ff%2Ff0%2FValve_original_%25281%2529.PNG&h=2d10e9ed&s={SIZE}\" /><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Valve_sobel_%283%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fd%2Fd4%2FValve_sobel_%25283%2529.PNG&h=d64282a3&s={SIZE}\" />"},{"pid":"c7d53753b9c26fef3b8e13731d7dae1781470351","tid":257808,"uid":28279,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T15:19:38.000Z","msg":"<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> 嫌樓主太慢嗰啲其實可以睇下其他link，舊年已經有人貼過，我慢慢等睇lin登仔用自己文字表述過癮啲<br />\n<br />\n淺談Alpha Go所涉及的深度學習技術<br />\n[url]<a href=\"https://www.bnext.com.tw/article/38923/BN-2016-03-14-120814-178\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.bnext.com.tw%2Farticle%2F38923%2FBN-2016-03-14-120814-178&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=37702875\" data-auto-link target=\"_blank\">https://www.bnext.com.tw/article/38923/BN-2016-03-14-120814-178</a><br />\n[/url]<br />\n用30分鐘深入瞭解《AlphaGo圍棋程式的設計原理》<br />\n<a href=\"https://www.slideshare.net/ccckmit/30alphago\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.slideshare.net%2Fccckmit%2F30alphago&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=a338c269\" target=\"_blank\">https://www.slideshare.net/ccckmit/30alphago</a>"},{"pid":"ffa9961011ba7e6ca0ebe251eabb4ccef5159553","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T15:25:57.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>簡單講下CNN結構，主要由幾款layer組成<br />\n<br />\n1. convolution (提取feature)<br />\n2. pooling (feature 簡化)<br />\n3. non-linearalize function (提高系統的非線性程度)<br />\n4. fully connected/ inner product (傳統ANN)<br />\n5. classification/ deconcolution(output最終結果)<br />\n<br />\n以前NN 唔可行因為所有pixel間都要行neuron connection, 10*10 的圖 output 去10*10 的layer就有 100^2 咁撚多的組合，CNN 用左convolution同pooling的技改提取image faeture進行簡化，咁一層layer用10個faeture kernel, 每個kernel 3*3, 咁只係得3*3*10 個neuron要fit, 令到NN 呢個想法實際可行</blockquote><br />\n<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> 太複雜啦 等比人屌啦你</blockquote><br />\n屌你咩已經無得再淺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n一係由咩叫kernel，咩係convolution開始講<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n簡單d就係俾一堆圖(e.g. 車)俾電腦睇，淨係同佢講呢樣野叫車，電腦自動搵&quot;車&quot;的feature，e.g. 有碌，有窗，有bumper, 再深多兩層就人都理解唔到的東西，叫feature vector，搵一大堆數function去描述d 特徵<br />\n<br />\nCNN 同人類局域視角好有關，但我唔想講人類點樣去理解世界呢樣野<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n可唔可以講詳細少少點用convolution嚟搵feature?<img src=\"/assets/faces/normal/wonder.gif\" class=\"hkgmoji\" /> <br />\n<br />\n利申：知convolution同kernel乜嚟，講得數學啲都ok</blockquote><br />\nConvolution其實係一個spatial filter, 例如最簡單嘅Sobel kernel:<br />\n[1 0 -1;<br />\n2  0 -2;<br />\n1 0 -1]<br />\n就可以搵到圖形的邊界：<br />\nbefore:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_original_(1).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_original_%281%29.PNG&h=33d303a3&s={SIZE}\" /><br />\nafter:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_sobel_(3).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_sobel_%283%29.PNG&h=ee5959a1&s={SIZE}\" /><br />\n唔同kernel 會產生唔同feature, 所以CNN個kernel都係要由data train出黎。</blockquote><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f0/Valve_original_%281%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Ff%2Ff0%2FValve_original_%25281%2529.PNG&h=2d10e9ed&s={SIZE}\" /><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Valve_sobel_%283%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fd%2Fd4%2FValve_sobel_%25283%2529.PNG&h=d64282a3&s={SIZE}\" /></blockquote><br />\n數學d咁講 low pass filter就係filter走不斷升跌既pattern<br />\n<br />\n咁反觀high pass filter就係filter走太flat(?)既wave, 留返d high perturbation(?) (即係d 幼既線)"},{"pid":"3879eedca856fc4508ba35f71bd664b6515bf884","tid":257808,"uid":29804,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T15:26:36.000Z","msg":"呢個post<br />\n出現左唔同類型嘅人<br />\n<br />\n1. 以無知為榮的人<br />\n2. 傲慢自以為高人一等的知識份子"},{"pid":"2b0597c2b8c37d0a2747d21719a11aa1346fc74f","tid":257808,"uid":30535,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-27T15:37:14.000Z","msg":"<blockquote><blockquote><br />\n開放討論但係掉低舊膠and留個PHD 就走左去, 你D討論真係<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n我邊有掉低舊膠呀，我話支持樓主你有冇見到呀？宜家係話自己讀PhD已經俾你屌鳩我晒命，心諗我有咩得罪你？我冇時間貢獻討論係我唔啱，但係上面都有一大堆巴打貢獻緊啦。我都未見過咁撚串既人係度教人。</blockquote><br />\n無謂咁上心<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"a2c9b5475852262fb518d8e0928beff2c016f5d8","tid":257808,"uid":30535,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T15:38:41.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>簡單講下CNN結構，主要由幾款layer組成<br />\n<br />\n1. convolution (提取feature)<br />\n2. pooling (feature 簡化)<br />\n3. non-linearalize function (提高系統的非線性程度)<br />\n4. fully connected/ inner product (傳統ANN)<br />\n5. classification/ deconcolution(output最終結果)<br />\n<br />\n以前NN 唔可行因為所有pixel間都要行neuron connection, 10*10 的圖 output 去10*10 的layer就有 100^2 咁撚多的組合，CNN 用左convolution同pooling的技改提取image faeture進行簡化，咁一層layer用10個faeture kernel, 每個kernel 3*3, 咁只係得3*3*10 個neuron要fit, 令到NN 呢個想法實際可行</blockquote><br />\n<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> 太複雜啦 等比人屌啦你</blockquote><br />\n屌你咩已經無得再淺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n一係由咩叫kernel，咩係convolution開始講<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n簡單d就係俾一堆圖(e.g. 車)俾電腦睇，淨係同佢講呢樣野叫車，電腦自動搵&quot;車&quot;的feature，e.g. 有碌，有窗，有bumper, 再深多兩層就人都理解唔到的東西，叫feature vector，搵一大堆數function去描述d 特徵<br />\n<br />\nCNN 同人類局域視角好有關，但我唔想講人類點樣去理解世界呢樣野<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n可唔可以講詳細少少點用convolution嚟搵feature?<img src=\"/assets/faces/normal/wonder.gif\" class=\"hkgmoji\" /> <br />\n<br />\n利申：知convolution同kernel乜嚟，講得數學啲都ok</blockquote><br />\nConvolution其實係一個spatial filter, 例如最簡單嘅Sobel kernel:<br />\n[1 0 -1;<br />\n2  0 -2;<br />\n1 0 -1]<br />\n就可以搵到圖形的邊界：<br />\nbefore:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_original_(1).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_original_%281%29.PNG&h=33d303a3&s={SIZE}\" /><br />\nafter:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_sobel_(3).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_sobel_%283%29.PNG&h=ee5959a1&s={SIZE}\" /><br />\n唔同kernel 會產生唔同feature, 所以CNN個kernel都係要由data train出黎。</blockquote><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f0/Valve_original_%281%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Ff%2Ff0%2FValve_original_%25281%2529.PNG&h=2d10e9ed&s={SIZE}\" /><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Valve_sobel_%283%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fd%2Fd4%2FValve_sobel_%25283%2529.PNG&h=d64282a3&s={SIZE}\" /></blockquote><br />\n數學d咁講 low pass filter就係filter走不斷升跌既pattern<br />\n<br />\n咁反觀high pass filter就係filter走太flat(?)既wave, 留返d high perturbation(?) (即係d 幼既線)</blockquote><br />\n只係咁嘅話令我有種用啲高深字眼包裝一啲已知好耐嘅嘢嘅感覺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"00941cdf4d801469d45e52fff3c119ce7bb8db4d","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T15:43:00.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>簡單講下CNN結構，主要由幾款layer組成<br />\n<br />\n1. convolution (提取feature)<br />\n2. pooling (feature 簡化)<br />\n3. non-linearalize function (提高系統的非線性程度)<br />\n4. fully connected/ inner product (傳統ANN)<br />\n5. classification/ deconcolution(output最終結果)<br />\n<br />\n以前NN 唔可行因為所有pixel間都要行neuron connection, 10*10 的圖 output 去10*10 的layer就有 100^2 咁撚多的組合，CNN 用左convolution同pooling的技改提取image faeture進行簡化，咁一層layer用10個faeture kernel, 每個kernel 3*3, 咁只係得3*3*10 個neuron要fit, 令到NN 呢個想法實際可行</blockquote><br />\n<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> 太複雜啦 等比人屌啦你</blockquote><br />\n屌你咩已經無得再淺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n一係由咩叫kernel，咩係convolution開始講<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n簡單d就係俾一堆圖(e.g. 車)俾電腦睇，淨係同佢講呢樣野叫車，電腦自動搵&quot;車&quot;的feature，e.g. 有碌，有窗，有bumper, 再深多兩層就人都理解唔到的東西，叫feature vector，搵一大堆數function去描述d 特徵<br />\n<br />\nCNN 同人類局域視角好有關，但我唔想講人類點樣去理解世界呢樣野<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n可唔可以講詳細少少點用convolution嚟搵feature?<img src=\"/assets/faces/normal/wonder.gif\" class=\"hkgmoji\" /> <br />\n<br />\n利申：知convolution同kernel乜嚟，講得數學啲都ok</blockquote><br />\nConvolution其實係一個spatial filter, 例如最簡單嘅Sobel kernel:<br />\n[1 0 -1;<br />\n2  0 -2;<br />\n1 0 -1]<br />\n就可以搵到圖形的邊界：<br />\nbefore:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_original_(1).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_original_%281%29.PNG&h=33d303a3&s={SIZE}\" /><br />\nafter:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_sobel_(3).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_sobel_%283%29.PNG&h=ee5959a1&s={SIZE}\" /><br />\n唔同kernel 會產生唔同feature, 所以CNN個kernel都係要由data train出黎。</blockquote><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f0/Valve_original_%281%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Ff%2Ff0%2FValve_original_%25281%2529.PNG&h=2d10e9ed&s={SIZE}\" /><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Valve_sobel_%283%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fd%2Fd4%2FValve_sobel_%25283%2529.PNG&h=d64282a3&s={SIZE}\" /></blockquote><br />\n數學d咁講 low pass filter就係filter走不斷升跌既pattern<br />\n<br />\n咁反觀high pass filter就係filter走太flat(?)既wave, 留返d high perturbation(?) (即係d 幼既線)</blockquote><br />\n只係咁嘅話令我有種用啲高深字眼包裝一啲已知好耐嘅嘢嘅感覺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n咁你數學好<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n你可以google下sift,surf,brief,orb feature既 呢d係cv比較出名既feature extraction"},{"pid":"def18ed93a4d7f8d3d720faa11e67bc1c0d5cf96","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T15:49:56.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>簡單講下CNN結構，主要由幾款layer組成<br />\n<br />\n1. convolution (提取feature)<br />\n2. pooling (feature 簡化)<br />\n3. non-linearalize function (提高系統的非線性程度)<br />\n4. fully connected/ inner product (傳統ANN)<br />\n5. classification/ deconcolution(output最終結果)<br />\n<br />\n以前NN 唔可行因為所有pixel間都要行neuron connection, 10*10 的圖 output 去10*10 的layer就有 100^2 咁撚多的組合，CNN 用左convolution同pooling的技改提取image faeture進行簡化，咁一層layer用10個faeture kernel, 每個kernel 3*3, 咁只係得3*3*10 個neuron要fit, 令到NN 呢個想法實際可行</blockquote><br />\n<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> 太複雜啦 等比人屌啦你</blockquote><br />\n屌你咩已經無得再淺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n一係由咩叫kernel，咩係convolution開始講<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n簡單d就係俾一堆圖(e.g. 車)俾電腦睇，淨係同佢講呢樣野叫車，電腦自動搵&quot;車&quot;的feature，e.g. 有碌，有窗，有bumper, 再深多兩層就人都理解唔到的東西，叫feature vector，搵一大堆數function去描述d 特徵<br />\n<br />\nCNN 同人類局域視角好有關，但我唔想講人類點樣去理解世界呢樣野<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n可唔可以講詳細少少點用convolution嚟搵feature?<img src=\"/assets/faces/normal/wonder.gif\" class=\"hkgmoji\" /> <br />\n<br />\n利申：知convolution同kernel乜嚟，講得數學啲都ok</blockquote><br />\nConvolution其實係一個spatial filter, 例如最簡單嘅Sobel kernel:<br />\n[1 0 -1;<br />\n2  0 -2;<br />\n1 0 -1]<br />\n就可以搵到圖形的邊界：<br />\nbefore:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_original_(1).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_original_%281%29.PNG&h=33d303a3&s={SIZE}\" /><br />\nafter:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_sobel_(3).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_sobel_%283%29.PNG&h=ee5959a1&s={SIZE}\" /><br />\n唔同kernel 會產生唔同feature, 所以CNN個kernel都係要由data train出黎。</blockquote><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f0/Valve_original_%281%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Ff%2Ff0%2FValve_original_%25281%2529.PNG&h=2d10e9ed&s={SIZE}\" /><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Valve_sobel_%283%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fd%2Fd4%2FValve_sobel_%25283%2529.PNG&h=d64282a3&s={SIZE}\" /></blockquote><br />\n數學d咁講 low pass filter就係filter走不斷升跌既pattern<br />\n<br />\n咁反觀high pass filter就係filter走太flat(?)既wave, 留返d high perturbation(?) (即係d 幼既線)</blockquote><br />\n只係咁嘅話令我有種用啲高深字眼包裝一啲已知好耐嘅嘢嘅感覺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n咁你數學好<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n你可以google下sift,surf,brief,orb feature既 呢d係cv比較出名既feature extraction</blockquote><br />\n同埋math果邊就好興用TV,應該算係pde approach(?)<br />\n<br />\nTony Chan佢個model用左個果時黎講好特別既approach所以好出名<br />\n<br />\nNormally segmentation係based on edge咁剪<br />\n但係佢就proposed minimize (pixel既intensity - 屬於同一個region既mean intensity) (我都唔係好識講 不過睇返佢果份paper應該難唔到你<img src=\"/assets/faces/normal/wink.gif\" class=\"hkgmoji\" />"},{"pid":"81540dda55ecd07d235837b09ff07423a908ed5a","tid":257808,"uid":69125,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T15:54:22.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>簡單講下CNN結構，主要由幾款layer組成<br />\n<br />\n1. convolution (提取feature)<br />\n2. pooling (feature 簡化)<br />\n3. non-linearalize function (提高系統的非線性程度)<br />\n4. fully connected/ inner product (傳統ANN)<br />\n5. classification/ deconcolution(output最終結果)<br />\n<br />\n以前NN 唔可行因為所有pixel間都要行neuron connection, 10*10 的圖 output 去10*10 的layer就有 100^2 咁撚多的組合，CNN 用左convolution同pooling的技改提取image faeture進行簡化，咁一層layer用10個faeture kernel, 每個kernel 3*3, 咁只係得3*3*10 個neuron要fit, 令到NN 呢個想法實際可行</blockquote><br />\n<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> 太複雜啦 等比人屌啦你</blockquote><br />\n屌你咩已經無得再淺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n一係由咩叫kernel，咩係convolution開始講<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n簡單d就係俾一堆圖(e.g. 車)俾電腦睇，淨係同佢講呢樣野叫車，電腦自動搵&quot;車&quot;的feature，e.g. 有碌，有窗，有bumper, 再深多兩層就人都理解唔到的東西，叫feature vector，搵一大堆數function去描述d 特徵<br />\n<br />\nCNN 同人類局域視角好有關，但我唔想講人類點樣去理解世界呢樣野<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n可唔可以講詳細少少點用convolution嚟搵feature?<img src=\"/assets/faces/normal/wonder.gif\" class=\"hkgmoji\" /> <br />\n<br />\n利申：知convolution同kernel乜嚟，講得數學啲都ok</blockquote><br />\nConvolution其實係一個spatial filter, 例如最簡單嘅Sobel kernel:<br />\n[1 0 -1;<br />\n2  0 -2;<br />\n1 0 -1]<br />\n就可以搵到圖形的邊界：<br />\nbefore:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_original_(1).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_original_%281%29.PNG&h=33d303a3&s={SIZE}\" /><br />\nafter:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_sobel_(3).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_sobel_%283%29.PNG&h=ee5959a1&s={SIZE}\" /><br />\n唔同kernel 會產生唔同feature, 所以CNN個kernel都係要由data train出黎。</blockquote><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f0/Valve_original_%281%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Ff%2Ff0%2FValve_original_%25281%2529.PNG&h=2d10e9ed&s={SIZE}\" /><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Valve_sobel_%283%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fd%2Fd4%2FValve_sobel_%25283%2529.PNG&h=d64282a3&s={SIZE}\" /></blockquote><br />\n數學d咁講 low pass filter就係filter走不斷升跌既pattern<br />\n<br />\n咁反觀high pass filter就係filter走太flat(?)既wave, 留返d high perturbation(?) (即係d 幼既線)</blockquote><br />\n只係咁嘅話令我有種用啲高深字眼包裝一啲已知好耐嘅嘢嘅感覺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n咁你數學好<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n你可以google下sift,surf,brief,orb feature既 呢d係cv比較出名既feature extraction</blockquote><br />\n同埋math果邊就好興用TV,應該算係pde approach(?)<br />\n<br />\nTony Chan佢個model用左個果時黎講好特別既approach所以好出名<br />\n<br />\nNormally segmentation係based on edge咁剪<br />\n但係佢就proposed minimize (pixel既intensity - 屬於同一個region既mean intensity) (我都唔係好識講 不過睇返佢果份paper應該難唔到你<img src=\"/assets/faces/normal/wink.gif\" class=\"hkgmoji\" /></blockquote><br />\ntony chan=陳繁昌?ee仔lm,唔記得1D signal個convolution點做<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" />"},{"pid":"611cedbcd9efc10080a292f477d59f05d868ff25","tid":257808,"uid":67752,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-05-27T16:01:08.000Z","msg":"<blockquote><img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> btw原來lihkg咁多研究生</blockquote><br />\n係d中小學雞太多 令其他人唔多顯眼 <br />\n其實呢d post 先有返d上x登既感覺 d小朋友睇兩眼唔明就係到嘈<br />\n<br />\n利申 想學下野<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"272d1d38b750f31aa15819cba2e9849c3c679029","tid":257808,"uid":30535,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T16:10:25.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><br />\n屌你咩已經無得再淺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n一係由咩叫kernel，咩係convolution開始講<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n簡單d就係俾一堆圖(e.g. 車)俾電腦睇，淨係同佢講呢樣野叫車，電腦自動搵&quot;車&quot;的feature，e.g. 有碌，有窗，有bumper, 再深多兩層就人都理解唔到的東西，叫feature vector，搵一大堆數function去描述d 特徵<br />\n<br />\nCNN 同人類局域視角好有關，但我唔想講人類點樣去理解世界呢樣野<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n可唔可以講詳細少少點用convolution嚟搵feature?<img src=\"/assets/faces/normal/wonder.gif\" class=\"hkgmoji\" /> <br />\n<br />\n利申：知convolution同kernel乜嚟，講得數學啲都ok</blockquote><br />\nConvolution其實係一個spatial filter, 例如最簡單嘅Sobel kernel:<br />\n[1 0 -1;<br />\n2  0 -2;<br />\n1 0 -1]<br />\n就可以搵到圖形的邊界：<br />\nbefore:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_original_(1).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_original_%281%29.PNG&h=33d303a3&s={SIZE}\" /><br />\nafter:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_sobel_(3).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_sobel_%283%29.PNG&h=ee5959a1&s={SIZE}\" /><br />\n唔同kernel 會產生唔同feature, 所以CNN個kernel都係要由data train出黎。</blockquote><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f0/Valve_original_%281%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Ff%2Ff0%2FValve_original_%25281%2529.PNG&h=2d10e9ed&s={SIZE}\" /><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Valve_sobel_%283%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fd%2Fd4%2FValve_sobel_%25283%2529.PNG&h=d64282a3&s={SIZE}\" /></blockquote><br />\n數學d咁講 low pass filter就係filter走不斷升跌既pattern<br />\n<br />\n咁反觀high pass filter就係filter走太flat(?)既wave, 留返d high perturbation(?) (即係d 幼既線)</blockquote><br />\n只係咁嘅話令我有種用啲高深字眼包裝一啲已知好耐嘅嘢嘅感覺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n咁你數學好<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n你可以google下sift,surf,brief,orb feature既 呢d係cv比較出名既feature extraction</blockquote><br />\n原來係我睇錯<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> 我以為淨係用死某隻filter,所以覺得用convolutional依隻字有啲misleading<br />\n<br />\n睇返你講嗰啲例子幾有趣<img src=\"/assets/faces/normal/hoho.gif\" class=\"hkgmoji\" />"},{"pid":"4c2287cb48a25a45a3ef88465b750ebe772e837c","tid":257808,"uid":112176,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T16:13:50.000Z","msg":"留名"},{"pid":"9ea8e5d82de1f5a363b351dfa93e724e65cc1999","tid":257808,"uid":28279,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T16:20:43.000Z","msg":"<blockquote><blockquote><img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> btw原來lihkg咁多研究生</blockquote><br />\n係d中小學雞太多 令其他人唔多顯眼 <br />\n其實呢d post 先有返d上x登既感覺 d小朋友睇兩眼唔明就係到嘈<br />\n<br />\n利申 想學下野<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n<img src=\"/assets/faces/normal/no.gif\" class=\"hkgmoji\" /> 舊登社會人仕比較多，分享得多啲社會經驗，少啲呢啲學術<br />\n<br />\n不過可能因為社會人仕多所以大家通常都係直接互屌老母，呢度相比之下嘈都嘈得比較和諧<img src=\"/assets/faces/lomoji/05.png\" class=\"hkgmoji\" />"},{"pid":"7ccf6b0721768e4dd8e428f96b1097cc903d8166","tid":257808,"uid":30535,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T16:21:58.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><br />\n可唔可以講詳細少少點用convolution嚟搵feature?<img src=\"/assets/faces/normal/wonder.gif\" class=\"hkgmoji\" /> <br />\n<br />\n利申：知convolution同kernel乜嚟，講得數學啲都ok</blockquote><br />\nConvolution其實係一個spatial filter, 例如最簡單嘅Sobel kernel:<br />\n[1 0 -1;<br />\n2  0 -2;<br />\n1 0 -1]<br />\n就可以搵到圖形的邊界：<br />\nbefore:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_original_(1).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_original_%281%29.PNG&h=33d303a3&s={SIZE}\" /><br />\nafter:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_sobel_(3).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_sobel_%283%29.PNG&h=ee5959a1&s={SIZE}\" /><br />\n唔同kernel 會產生唔同feature, 所以CNN個kernel都係要由data train出黎。</blockquote><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f0/Valve_original_%281%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Ff%2Ff0%2FValve_original_%25281%2529.PNG&h=2d10e9ed&s={SIZE}\" /><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Valve_sobel_%283%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fd%2Fd4%2FValve_sobel_%25283%2529.PNG&h=d64282a3&s={SIZE}\" /></blockquote><br />\n數學d咁講 low pass filter就係filter走不斷升跌既pattern<br />\n<br />\n咁反觀high pass filter就係filter走太flat(?)既wave, 留返d high perturbation(?) (即係d 幼既線)</blockquote><br />\n只係咁嘅話令我有種用啲高深字眼包裝一啲已知好耐嘅嘢嘅感覺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n咁你數學好<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n你可以google下sift,surf,brief,orb feature既 呢d係cv比較出名既feature extraction</blockquote><br />\n同埋math果邊就好興用TV,應該算係pde approach(?)<br />\n<br />\nTony Chan佢個model用左個果時黎講好特別既approach所以好出名<br />\n<br />\nNormally segmentation係based on edge咁剪<br />\n但係佢就proposed minimize (pixel既intensity - 屬於同一個region既mean intensity) (我都唔係好識講 不過睇返佢果份paper應該難唔到你<img src=\"/assets/faces/normal/wink.gif\" class=\"hkgmoji\" /></blockquote><br />\n有聽過下，都可以話係PDE approach嘅，但意念上minimize TV同apply個low pass filter分別唔大（但方法當然唔同）"},{"pid":"42285ec227060d611f39fb94d142e2b45b8f5dbf","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T16:27:26.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><br />\nConvolution其實係一個spatial filter, 例如最簡單嘅Sobel kernel:<br />\n[1 0 -1;<br />\n2  0 -2;<br />\n1 0 -1]<br />\n就可以搵到圖形的邊界：<br />\nbefore:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_original_(1).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_original_%281%29.PNG&h=33d303a3&s={SIZE}\" /><br />\nafter:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_sobel_(3).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_sobel_%283%29.PNG&h=ee5959a1&s={SIZE}\" /><br />\n唔同kernel 會產生唔同feature, 所以CNN個kernel都係要由data train出黎。</blockquote><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f0/Valve_original_%281%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Ff%2Ff0%2FValve_original_%25281%2529.PNG&h=2d10e9ed&s={SIZE}\" /><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Valve_sobel_%283%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fd%2Fd4%2FValve_sobel_%25283%2529.PNG&h=d64282a3&s={SIZE}\" /></blockquote><br />\n數學d咁講 low pass filter就係filter走不斷升跌既pattern<br />\n<br />\n咁反觀high pass filter就係filter走太flat(?)既wave, 留返d high perturbation(?) (即係d 幼既線)</blockquote><br />\n只係咁嘅話令我有種用啲高深字眼包裝一啲已知好耐嘅嘢嘅感覺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n咁你數學好<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n你可以google下sift,surf,brief,orb feature既 呢d係cv比較出名既feature extraction</blockquote><br />\n同埋math果邊就好興用TV,應該算係pde approach(?)<br />\n<br />\nTony Chan佢個model用左個果時黎講好特別既approach所以好出名<br />\n<br />\nNormally segmentation係based on edge咁剪<br />\n但係佢就proposed minimize (pixel既intensity - 屬於同一個region既mean intensity) (我都唔係好識講 不過睇返佢果份paper應該難唔到你<img src=\"/assets/faces/normal/wink.gif\" class=\"hkgmoji\" /></blockquote><br />\n有聽過下，都可以話係PDE approach嘅，但意念上minimize TV同apply個low pass filter分別唔大（但方法當然唔同）</blockquote><br />\n如果用2norm就同low pass差唔多 好似係<br />\n<br />\n不過而家成日興sparse所以先1norm<br />\n<br />\n仲有image impainting同netflix prize(即係recommendation system) 實例就係fb news feed, youtube, google suggestion"},{"pid":"fe70844fc813ff16a29bf593862b660305a0504c","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T16:28:13.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><br />\nConvolution其實係一個spatial filter, 例如最簡單嘅Sobel kernel:<br />\n[1 0 -1;<br />\n2  0 -2;<br />\n1 0 -1]<br />\n就可以搵到圖形的邊界：<br />\nbefore:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_original_(1).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_original_%281%29.PNG&h=33d303a3&s={SIZE}\" /><br />\nafter:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_sobel_(3).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_sobel_%283%29.PNG&h=ee5959a1&s={SIZE}\" /><br />\n唔同kernel 會產生唔同feature, 所以CNN個kernel都係要由data train出黎。</blockquote><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f0/Valve_original_%281%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Ff%2Ff0%2FValve_original_%25281%2529.PNG&h=2d10e9ed&s={SIZE}\" /><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Valve_sobel_%283%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fd%2Fd4%2FValve_sobel_%25283%2529.PNG&h=d64282a3&s={SIZE}\" /></blockquote><br />\n數學d咁講 low pass filter就係filter走不斷升跌既pattern<br />\n<br />\n咁反觀high pass filter就係filter走太flat(?)既wave, 留返d high perturbation(?) (即係d 幼既線)</blockquote><br />\n只係咁嘅話令我有種用啲高深字眼包裝一啲已知好耐嘅嘢嘅感覺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n咁你數學好<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n你可以google下sift,surf,brief,orb feature既 呢d係cv比較出名既feature extraction</blockquote><br />\n同埋math果邊就好興用TV,應該算係pde approach(?)<br />\n<br />\nTony Chan佢個model用左個果時黎講好特別既approach所以好出名<br />\n<br />\nNormally segmentation係based on edge咁剪<br />\n但係佢就proposed minimize (pixel既intensity - 屬於同一個region既mean intensity) (我都唔係好識講 不過睇返佢果份paper應該難唔到你<img src=\"/assets/faces/normal/wink.gif\" class=\"hkgmoji\" /></blockquote><br />\n<span style=\"color: red;\">tony chan=陳繁昌?</span>ee仔lm,唔記得1D signal個convolution點做<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\nYes"},{"pid":"e98a003265501ba33578bcf9d48c33f8de693296","tid":257808,"uid":69125,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T16:29:44.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><br />\nConvolution其實係一個spatial filter, 例如最簡單嘅Sobel kernel:<br />\n[1 0 -1;<br />\n2  0 -2;<br />\n1 0 -1]<br />\n就可以搵到圖形的邊界：<br />\nbefore:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_original_(1).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_original_%281%29.PNG&h=33d303a3&s={SIZE}\" /><br />\nafter:<br />\n<img src=\"https://en.wikipedia.org/wiki/File:Valve_sobel_(3).PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3AValve_sobel_%283%29.PNG&h=ee5959a1&s={SIZE}\" /><br />\n唔同kernel 會產生唔同feature, 所以CNN個kernel都係要由data train出黎。</blockquote><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f0/Valve_original_%281%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Ff%2Ff0%2FValve_original_%25281%2529.PNG&h=2d10e9ed&s={SIZE}\" /><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Valve_sobel_%283%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fd%2Fd4%2FValve_sobel_%25283%2529.PNG&h=d64282a3&s={SIZE}\" /></blockquote><br />\n數學d咁講 low pass filter就係filter走不斷升跌既pattern<br />\n<br />\n咁反觀high pass filter就係filter走太flat(?)既wave, 留返d high perturbation(?) (即係d 幼既線)</blockquote><br />\n只係咁嘅話令我有種用啲高深字眼包裝一啲已知好耐嘅嘢嘅感覺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n咁你數學好<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n你可以google下sift,surf,brief,orb feature既 呢d係cv比較出名既feature extraction</blockquote><br />\n同埋math果邊就好興用TV,應該算係pde approach(?)<br />\n<br />\nTony Chan佢個model用左個果時黎講好特別既approach所以好出名<br />\n<br />\nNormally segmentation係based on edge咁剪<br />\n但係佢就proposed minimize (pixel既intensity - 屬於同一個region既mean intensity) (我都唔係好識講 不過睇返佢果份paper應該難唔到你<img src=\"/assets/faces/normal/wink.gif\" class=\"hkgmoji\" /></blockquote><br />\n<span style=\"color: red;\">tony chan=陳繁昌?</span>ee仔lm,唔記得1D signal個convolution點做<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\nYes</blockquote><br />\n原來佢係做依d<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />讀ust讀到grad都唔知佢做乜,淨係知applied math<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"842b7e95c04b8c7d9ca1c96ebc82d55b8fa2d62a","tid":257808,"uid":30535,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T16:31:51.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f0/Valve_original_%281%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Ff%2Ff0%2FValve_original_%25281%2529.PNG&h=2d10e9ed&s={SIZE}\" /><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Valve_sobel_%283%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fd%2Fd4%2FValve_sobel_%25283%2529.PNG&h=d64282a3&s={SIZE}\" /></blockquote><br />\n數學d咁講 low pass filter就係filter走不斷升跌既pattern<br />\n<br />\n咁反觀high pass filter就係filter走太flat(?)既wave, 留返d high perturbation(?) (即係d 幼既線)</blockquote><br />\n只係咁嘅話令我有種用啲高深字眼包裝一啲已知好耐嘅嘢嘅感覺<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n咁你數學好<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n你可以google下sift,surf,brief,orb feature既 呢d係cv比較出名既feature extraction</blockquote><br />\n同埋math果邊就好興用TV,應該算係pde approach(?)<br />\n<br />\nTony Chan佢個model用左個果時黎講好特別既approach所以好出名<br />\n<br />\nNormally segmentation係based on edge咁剪<br />\n但係佢就proposed minimize (pixel既intensity - 屬於同一個region既mean intensity) (我都唔係好識講 不過睇返佢果份paper應該難唔到你<img src=\"/assets/faces/normal/wink.gif\" class=\"hkgmoji\" /></blockquote><br />\n有聽過下，都可以話係PDE approach嘅，但意念上minimize TV同apply個low pass filter分別唔大（但方法當然唔同）</blockquote><br />\n如果用2norm就同low pass差唔多 好似係<br />\n<br />\n不過而家成日興sparse所以先1norm<br />\n<br />\n仲有image impainting同netflix prize(即係recommendation system) 實例就係fb news feed, youtube, google suggestion</blockquote><br />\n因為2-norm corresponds to Laplacian,而Laplacian locally其實根本就係low pass filter<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" />"},{"pid":"0b3452bd447f149b51952e6bd78f6723a1027828","tid":257808,"uid":40839,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T16:45:36.000Z","msg":"<blockquote><blockquote><blockquote><a href=\"https://lihkg.com/thread/257709/page/3?ref=ios\" target=\"_blank\">https://lihkg.com/thread/257709/page/3?ref=ios</a><br />\n<br />\n研發Alpha Go嘅人真係會知自己部機個極限係咩？<br />\n佢話柯潔頭50步接近完美 <br />\n但又代表即使柯潔無犯錯Alpha Go都可能會輸<br />\n會唔會事實係AlphaGo本身都係plan緊幾十步之後去拆解個局？<br />\n<br />\n定係純粹係佢謙虛咁講？</blockquote><br />\n我諗係因為alpha go可以output一個predict自己贏既probi 而頭50move佢都predict自己勝率唔高</blockquote><br />\n應該係盤轉角色係人的個方eval出黎個move用柯潔行的一樣</blockquote><br />\n我估柯潔頭50步 alphago計到係當時最高勝率或者係AlphaGo都會選擇既下法"},{"pid":"37213596ca38db90e4b888753c5e88bdb7681b74","tid":257808,"uid":158,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T17:43:07.000Z","msg":"留名學野"},{"pid":"8ac14203e430850aad55cbcdc8886ba735862b2f","tid":257808,"uid":49935,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T17:52:57.000Z","msg":"飛已彿"},{"pid":"8b1d186555adfbb3236a0dedd991e52057efe152","tid":257808,"uid":17008,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T20:04:48.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><a href=\"https://lihkg.com/thread/257709/page/3?ref=ios\" target=\"_blank\">https://lihkg.com/thread/257709/page/3?ref=ios</a><br />\n<br />\n研發Alpha Go嘅人真係會知自己部機個極限係咩？<br />\n佢話柯潔頭50步接近完美 <br />\n但又代表即使柯潔無犯錯Alpha Go都可能會輸<br />\n會唔會事實係AlphaGo本身都係plan緊幾十步之後去拆解個局？<br />\n<br />\n定係純粹係佢謙虛咁講？</blockquote><br />\n我諗係因為alpha go可以output一個predict自己贏既probi 而頭50move佢都predict自己勝率唔高</blockquote><br />\n應該係盤轉角色係人的個方eval出黎個move用柯潔行的一樣</blockquote><br />\n我估柯潔頭50步 alphago計到係當時最高勝率或者係AlphaGo都會選擇既下法</blockquote><br />\nAlpha go表現可能平均d<br />\n人表現一失準可能衰到之前幾好都維持唔到<br />\n輸集中力既持久度<br />\n<br />\n柯同李世石版既alphago鬥唔知會唔會兩勝"},{"pid":"7693bde380d8ad8584020c8089c26bff31f3f05b","tid":257808,"uid":37303,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T20:09:34.000Z","msg":"<img src=\"/assets/faces/normal/z.gif\" class=\"hkgmoji\" />"},{"pid":"354fdc79fa18dccd0e8eb4ebdd3a9402f6a533b8","tid":257808,"uid":118360,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-27T23:33:54.000Z","msg":"今日上下網睇<br />\n先知道原來 AlphaGo 有幾粒叫做 TPU (Tensor processing unit, 張量處理器) 的野, 專門用黎做 machine learning"},{"pid":"123758bfc99ed85bfb835114d3dfefa4da0e0021","tid":257808,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-28T02:19:59.000Z","msg":"<blockquote>今日上下網睇<br />\n先知道原來 AlphaGo 有幾粒叫做 TPU (Tensor processing unit, 張量處理器) 的野, 專門用黎做 machine learning</blockquote><br />\nTPU 出左2代啦，但唔開放俾公眾，遲d可能有google cloud version<br />\n<br />\n不同計CV野主流唔會用tensorflow<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"e2b8ddc96d81ffff38aa222c69348000fc660d93","tid":257808,"uid":48679,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-28T03:34:05.000Z","msg":"<blockquote><blockquote><blockquote><br />\n呢種層層classify嘅理論可以從實驗層面就可以證明唔係完全正確。例如一啲講求速度嘅運動，e.g. 乒乓球，羽毛球，劍擊etc .，運動員可以係極短時間內作出反應。依照層層classify嘅理論，見到一個乒乓波飛埋黎要作出反應要經過好多步，例如先要辨別到係一個乒乓波，分辨眼前係現實定電視形像，判斷自己係咪應該打佢，然後估算佢嘅位置同方向，估算自己手嘅位置同方向，計算出手嘅trajectory，判斷呢個trajectory係咪合理，計算各肌肉的用力的比例，然後打出去。如果層層classify,呢度每一個判斷都涉及極複雜的network，而neuron同neuron之間有synaptic delay，速度係上限的，計得黎個波一早就飛走左。但其實研究顯示，對一個task越純熟，大腦就越少activation。大腦會自動調節去specialize一個task, 唔係一定要層層去classify.<br />\n<br />\n而天才都有好多種，全能型天才都有。達文西就工程畫畫樣樣精，愛因斯坦都好有音樂天份，亦唔見佢有咩其他明顯缺陷。人腦並唔係over-generalize,而係佢本身有個好靈活嘅架構，令到佢可以適應唔同嘅環境。世上有好多超乎常人嘅運動家，音樂家，數學家，呢啲都係人可以specialize嘅證明。有缺陷嘅天才好多只係因為佢地被報道得多，其實果啲缺陷唔係天才都一樣會有，亦有好多天才一生正正常常。因為就大概而論話人腦over-generalize未必太過武斷。</blockquote><br />\n<br />\nsynaptic delay does not exist in electrotonic transmission. 所以你例子不成立<br />\n不過唔再講太technical 啦, 廢事班初學者睇到一舊舊<br />\n<br />\n你要睇係真係天才定係後天努力既&quot;天才&quot;<br />\n假如個腦本身構造已經係特化左, 一定係幾歲就顯現到出黎,依呢D 基本上好大部份都係有缺陷<br />\ngenius 同austim 係positive correlation 係已經Proof 左<br />\n而後天既天才, 佢地只係train 佢地既network train 得好好, 而唔係佢個network 本身就特化左<br />\n所以佢地其他能力普遍都同其他人相若</blockquote><br />\n我都有諗過electrical synapse, 但electrical synapse係冇gain, 即係無辦法做複雜運算，佢主要係用黎synchronize一堆post-synaptic neurons. 人腦大部份都係chemical synapse.<br />\n<br />\n好多神童都好正常喎，Mozart同Terence Tao 呢？其實autism中間天才好少，有研究指出大部份都有智力問題，只有3%係above average:<br />\n<a href=\"https://www.ncbi.nlm.nih.gov/pubmed/21272389\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpubmed%2F21272389&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=6f4ac9dd\" data-auto-link target=\"_blank\">https://www.ncbi.nlm.nih.gov/pubmed/21272389</a><br />\n<br />\n關於事實性的錯誤，我覺得自己有責任指出，唔想有人誤會。<br />\n<br />\nTo其他巴打：<br />\n我都知自己hijack左個post嫁啦，各位巴打對唔住！只係一時興起所以多口左。多有得罪唔好意思 <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> 繼續潛水做番CD-ROM...</blockquote><br />\nomfg ,<br />\n我同你講a=&gt;b ，你答我b=&gt;a 既機會都唔大黎反證我?你科logic 101係個腦到delete 左?<br />\n<br />\n如果我咁又人身攻擊左你，係到講句唔好意思先"},{"pid":"e06415f2bf22c6f92d08143c4ca5fdef127d043f","tid":257808,"uid":48679,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-28T03:44:14.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><br />\n呢種層層classify嘅理論可以從實驗層面就可以證明唔係完全正確。例如一啲講求速度嘅運動，e.g. 乒乓球，羽毛球，劍擊etc .，運動員可以係極短時間內作出反應。依照層層classify嘅理論，見到一個乒乓波飛埋黎要作出反應要經過好多步，例如先要辨別到係一個乒乓波，分辨眼前係現實定電視形像，判斷自己係咪應該打佢，然後估算佢嘅位置同方向，估算自己手嘅位置同方向，計算出手嘅trajectory，判斷呢個trajectory係咪合理，計算各肌肉的用力的比例，然後打出去。如果層層classify,呢度每一個判斷都涉及極複雜的network，而neuron同neuron之間有synaptic delay，速度係上限的，計得黎個波一早就飛走左。但其實研究顯示，對一個task越純熟，大腦就越少activation。大腦會自動調節去specialize一個task, 唔係一定要層層去classify.<br />\n<br />\n而天才都有好多種，全能型天才都有。達文西就工程畫畫樣樣精，愛因斯坦都好有音樂天份，亦唔見佢有咩其他明顯缺陷。人腦並唔係over-generalize,而係佢本身有個好靈活嘅架構，令到佢可以適應唔同嘅環境。世上有好多超乎常人嘅運動家，音樂家，數學家，呢啲都係人可以specialize嘅證明。有缺陷嘅天才好多只係因為佢地被報道得多，其實果啲缺陷唔係天才都一樣會有，亦有好多天才一生正正常常。因為就大概而論話人腦over-generalize未必太過武斷。</blockquote><br />\n<br />\nsynaptic delay does not exist in electrotonic transmission. 所以你例子不成立<br />\n不過唔再講太technical 啦, 廢事班初學者睇到一舊舊<br />\n<br />\n你要睇係真係天才定係後天努力既&quot;天才&quot;<br />\n假如個腦本身構造已經係特化左, 一定係幾歲就顯現到出黎,依呢D 基本上好大部份都係有缺陷<br />\ngenius 同austim 係positive correlation 係已經Proof 左<br />\n而後天既天才, 佢地只係train 佢地既network train 得好好, 而唔係佢個network 本身就特化左<br />\n所以佢地其他能力普遍都同其他人相若</blockquote><br />\n我都有諗過electrical synapse, 但electrical synapse係冇gain, 即係無辦法做複雜運算，佢主要係用黎synchronize一堆post-synaptic neurons. 人腦大部份都係chemical synapse.<br />\n<br />\n好多神童都好正常喎，Mozart同Terence Tao 呢？其實autism中間天才好少，有研究指出大部份都有智力問題，只有3%係above average:<br />\n<a href=\"https://www.ncbi.nlm.nih.gov/pubmed/21272389\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpubmed%2F21272389&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=6f4ac9dd\" data-auto-link target=\"_blank\">https://www.ncbi.nlm.nih.gov/pubmed/21272389</a><br />\n<br />\n關於事實性的錯誤，我覺得自己有責任指出，唔想有人誤會。<br />\n<br />\nTo其他巴打：<br />\n我都知自己hijack左個post嫁啦，各位巴打對唔住！只係一時興起所以多口左。多有得罪唔好意思 <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> 繼續潛水做番CD-ROM...</blockquote><br />\nomfg ,<br />\n我同你講a=&gt;b ，你答我b=&gt;a 既機會都唔大黎反證我?你科logic 101係個腦到delete 左?<br />\n<br />\n如果我咁又人身攻擊左你，係到講句唔好意思先</blockquote><br />\n俾個例子D logic101 都未學既初學者<br />\n我話鑽石其實係碳, 只不過係粒子特別排列左<br />\n而佢既反證竟然係係碳之中其實得0.00000001% 係鑽石....."},{"pid":"ad40b91ae90593b2f0773fa1bafcd9ead019a33e","tid":257808,"uid":55598,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-28T03:48:05.000Z","msg":"趁星期日得閒少少，upload 少少computer vision嘅例子圖比大家睇，唔講理論，只用laymen語言，等大家知道下係咩嚟，另外各位巴打見到有錯請指正<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <br />\n<br />\n通常要整一個 system（system 呢個字好 abstract），個flow都係咁：<br />\ntask -&gt; design architecture -&gt; training -&gt; testing<br />\ntesting 個 accuracy 未如理想就由 design再loop過<br />\n<br />\n好，假設我而家嘅目的係係一堆圖入邊，我要將佢哋分成 5 個種類（呢樣嘢要話比電腦知），然後我整咗個 CNN 去做呢個 &quot;classification&quot; problem。<br />\n<br />\n我個 CNN 有好多層 layer，同以下呢個好相似（呢個係一個好出名嘅 archietecture，叫 VGG19）：<br />\n<img src=\"https://image.slidesharecdn.com/adl1103-161027023044/95/applied-deep-learning-1103-convolutional-neural-networks-60-638.jpg?cb=1479405398\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fimage.slidesharecdn.com%2Fadl1103-161027023044%2F95%2Fapplied-deep-learning-1103-convolutional-neural-networks-60-638.jpg%3Fcb%3D1479405398&h=b47dbd55&s={SIZE}\" /><br />\n<br />\n如果你問咩叫 2d convolution，最laymen嚟講就係每一粒 output pixel 係kernel 同 input 嘅 linear combination（當係乘數都同加數） 嘅結果。例如：<br />\n<br />\n<img src=\"https://i.stack.imgur.com/6zX2c.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fi.stack.imgur.com%2F6zX2c.png&h=8a3d0a24&s={SIZE}\" /><br />\n<br />\n你會見到 kernel 係 3x3，佢同 input convolute，3x3 最終會變返 1x1，而output嘅每一粒 pixel 都係咁嚟。上面vgg19嗰啲 CONV 其實就係 convolution layer。<br />\n<br />\n咁我而家 design完，要開始訓練個 system，咁點樣做呢？就係input一啲已經知道答案嘅圖比佢，例如：<br />\nimage class<br />\nimage1.jpg 1<br />\nimage2.jpg 5<br />\nimage3.jpg 4<br />\n...如此類推<br />\n<br />\ntraining 即係我要搵一個 function 去將 Input map 去 output 個 5 個 class<br />\ntraining嘅同時其實我哋係 minimize 緊一個 loss function, loss function其目的係估計我哋 train 出嚟嘅 &quot;mapping function&quot; 嘅prediction output 同真實 output 不一致嘅程度。所以 Minimize 佢即係 maximize 我哋最終嘅 accruacy。<br />\n<br />\n未完待續，嚟緊我會 post 返我做呢個 task 所訓練出嚟嘅 result。"},{"pid":"ea866d0b427fc72904e4dd80c833a165ab9830f5","tid":257808,"uid":30535,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-28T04:03:01.000Z","msg":"<blockquote>趁星期日得閒少少，upload 少少computer vision嘅例子圖比大家睇，唔講理論，只用laymen語言，等大家知道下係咩嚟，另外各位巴打見到有錯請指正<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <br />\n<br />\n通常要整一個 system（system 呢個字好 abstract），個flow都係咁：<br />\ntask -&gt; design architecture -&gt; training -&gt; testing<br />\ntesting 個 accuracy 未如理想就由 design再loop過<br />\n<br />\n好，假設我而家嘅目的係係一堆圖入邊，我要將佢哋分成 5 個種類（呢樣嘢要話比電腦知），然後我整咗個 CNN 去做呢個 &quot;classification&quot; problem。<br />\n<br />\n我個 CNN 有好多層 layer，同以下呢個好相似（呢個係一個好出名嘅 archietecture，叫 VGG19）：<br />\n<img src=\"https://image.slidesharecdn.com/adl1103-161027023044/95/applied-deep-learning-1103-convolutional-neural-networks-60-638.jpg?cb=1479405398\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fimage.slidesharecdn.com%2Fadl1103-161027023044%2F95%2Fapplied-deep-learning-1103-convolutional-neural-networks-60-638.jpg%3Fcb%3D1479405398&h=b47dbd55&s={SIZE}\" /><br />\n<br />\n如果你問咩叫 2d convolution，最laymen嚟講就係每一粒 output pixel 係kernel 同 input 嘅 linear combination（當係乘數都同加數） 嘅結果。例如：<br />\n<br />\n<img src=\"https://i.stack.imgur.com/6zX2c.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fi.stack.imgur.com%2F6zX2c.png&h=8a3d0a24&s={SIZE}\" /><br />\n<br />\n你會見到 kernel 係 3x3，佢同 input convolute，3x3 最終會變返 1x1，而output嘅每一粒 pixel 都係咁嚟。上面vgg19嗰啲 CONV 其實就係 convolution layer。<br />\n<br />\n咁我而家 design完，要開始訓練個 system，咁點樣做呢？就係input一啲已經知道答案嘅圖比佢，例如：<br />\nimage class<br />\nimage1.jpg 1<br />\nimage2.jpg 5<br />\nimage3.jpg 4<br />\n...如此類推<br />\n<br />\ntraining 即係我要搵一個 function 去將 Input map 去 output 個 5 個 class<br />\ntraining嘅同時其實我哋係 minimize 緊一個 loss function, loss function其目的係估計我哋 train 出嚟嘅 &quot;mapping function&quot; 嘅prediction output 同真實 output 不一致嘅程度。所以 Minimize 佢即係 maximize 我哋最終嘅 accruacy。<br />\n<br />\n未完待續，嚟緊我會 post 返我做呢個 task 所訓練出嚟嘅 result。</blockquote><br />\n<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /> <br />\nbetween係convolve唔係convolute"},{"pid":"192bf5092c9327f3b5c562435b14ca10f90793fe","tid":257808,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-28T04:15:58.000Z","msg":"<blockquote><blockquote>趁星期日得閒少少，upload 少少computer vision嘅例子圖比大家睇，唔講理論，只用laymen語言，等大家知道下係咩嚟，另外各位巴打見到有錯請指正<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <br />\n<br />\n通常要整一個 system（system 呢個字好 abstract），個flow都係咁：<br />\ntask -&gt; design architecture -&gt; training -&gt; testing<br />\ntesting 個 accuracy 未如理想就由 design再loop過<br />\n<br />\n好，假設我而家嘅目的係係一堆圖入邊，我要將佢哋分成 5 個種類（呢樣嘢要話比電腦知），然後我整咗個 CNN 去做呢個 &quot;classification&quot; problem。<br />\n<br />\n我個 CNN 有好多層 layer，同以下呢個好相似（呢個係一個好出名嘅 archietecture，叫 VGG19）：<br />\n<img src=\"https://image.slidesharecdn.com/adl1103-161027023044/95/applied-deep-learning-1103-convolutional-neural-networks-60-638.jpg?cb=1479405398\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fimage.slidesharecdn.com%2Fadl1103-161027023044%2F95%2Fapplied-deep-learning-1103-convolutional-neural-networks-60-638.jpg%3Fcb%3D1479405398&h=b47dbd55&s={SIZE}\" /><br />\n<br />\n如果你問咩叫 2d convolution，最laymen嚟講就係每一粒 output pixel 係kernel 同 input 嘅 linear combination（當係乘數都同加數） 嘅結果。例如：<br />\n<br />\n<img src=\"https://i.stack.imgur.com/6zX2c.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fi.stack.imgur.com%2F6zX2c.png&h=8a3d0a24&s={SIZE}\" /><br />\n<br />\n你會見到 kernel 係 3x3，佢同 input convolute，3x3 最終會變返 1x1，而output嘅每一粒 pixel 都係咁嚟。上面vgg19嗰啲 CONV 其實就係 convolution layer。<br />\n<br />\n咁我而家 design完，要開始訓練個 system，咁點樣做呢？就係input一啲已經知道答案嘅圖比佢，例如：<br />\nimage class<br />\nimage1.jpg 1<br />\nimage2.jpg 5<br />\nimage3.jpg 4<br />\n...如此類推<br />\n<br />\ntraining 即係我要搵一個 function 去將 Input map 去 output 個 5 個 class<br />\ntraining嘅同時其實我哋係 minimize 緊一個 loss function, loss function其目的係估計我哋 train 出嚟嘅 &quot;mapping function&quot; 嘅prediction output 同真實 output 不一致嘅程度。所以 Minimize 佢即係 maximize 我哋最終嘅 accruacy。<br />\n<br />\n未完待續，嚟緊我會 post 返我做呢個 task 所訓練出嚟嘅 result。</blockquote><br />\n<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /> <br />\nbetween係convolve唔係convolute</blockquote><br />\n其實pooling 都可以算convolution的一類<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\nVGG 設計就係不停repeat pooling，conv，conv 呢個組合，跟住不停加大深度，當中每層layer的depth就係net的設計重點，跟住有人就extend到百幾層的VGG<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n有人發現用隨機方法去決定每層間的neuron進唔進行activation,發現有效降低training時間(即係有部份layer pixel會隨機選擇用定唔用)<br />\n<br />\n另外CV的ai 主要有兩個output, 一係classification,另一種係deconcolution後的probabilistic image (alphago, segmentation等)<br />\n<br />\nRNN 就唔多熟"},{"pid":"b241299390bd24900e3e1182071db7cd398f25a1","tid":257808,"uid":55598,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-28T04:20:41.000Z","msg":"<blockquote><br />\n其實pooling 都可以算convolution的一類<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\nVGG 設計就係不停repeat pooling，conv，conv 呢個組合，跟住不停加大深度，當中每層layer的depth就係net的設計重點，跟住有人就extend到百幾層的VGG<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n有人發現用隨機方法去決定每層間的neuron進唔進行activation,發現有效降低training時間(即係有部份layer pixel會隨機選擇用定唔用)<br />\n<br />\n另外CV的ai 主要有兩個output, 一係classification,另一種係deconcolution後的probabilistic image (alphago, segmentation等)<br />\n<br />\nRNN 就唔多熟</blockquote><br />\n<br />\npooling 其實仲容易理解<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> 但我會覺得佢似 sampling 多過 convolution <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"5c25300199b7082bf9649e6803d847ec93e4ce06","tid":257808,"uid":49544,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-28T11:01:41.000Z","msg":"預左比人插<br />\n<br />\n成班唔知懶係勁定真係勁既人係度鬥深入討論騎劫人個Post，利申我就真係乜都唔識，諗住入黎睇下，點知一班專家係度簡單討論，覺得有D咩唔啱你講完，自己開個Post放條Link係度未得囉，你係有料啱既D人自然入你個Post嫁啦。<br />\n<br />\n成班係一個人地講明話係簡單了解既Post入面鬥勁<br />\n<br />\n同乞衣鬥有錢有咩分別，讀完咁多書仲係咁On9<br />\n<br />\nBTW其實樓主你講到最新果個，其實已經好深，冇讀過既人已經冇咩興趣睇<br />\n<br />\n一來本身好多專業名詞，二來又冇咩現實類比既例子好難明<br />\n<br />\n希望樓主繼續努力，唔好比D死On9打擊<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"d8c59851bb10a8416a3e1660a0482e3485d5fa86","tid":257808,"uid":82234,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-28T11:28:11.000Z","msg":"樓主我們到了你在哪裡"},{"pid":"0852f1ce32f19a95f884c228cc66bc54c78afde7","tid":257808,"uid":61967,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-28T11:48:55.000Z","msg":"少有正經post<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"7a5bbc2963d5b985ac917749877f14909cb74c0d","tid":257808,"uid":50616,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-05-28T12:01:04.000Z","msg":"<blockquote><blockquote><br />\n其實pooling 都可以算convolution的一類<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\nVGG 設計就係不停repeat pooling，conv，conv 呢個組合，跟住不停加大深度，當中每層layer的depth就係net的設計重點，跟住有人就extend到百幾層的VGG<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n有人發現用隨機方法去決定每層間的neuron進唔進行activation,發現有效降低training時間(即係有部份layer pixel會隨機選擇用定唔用)<br />\n<br />\n另外CV的ai 主要有兩個output, 一係classification,另一種係deconcolution後的probabilistic image (alphago, segmentation等)<br />\n<br />\nRNN 就唔多熟</blockquote><br />\n<br />\npooling 其實仲容易理解<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> 但我會覺得佢似 sampling 多過 convolution <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n <br />\nSampling 會𢵧<br />\nPooling 係summarize"},{"pid":"3b196f2251d92c76d918059cd80217b278ec9310","tid":257808,"uid":69125,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-06-09T15:50:15.000Z","msg":"有冇巴打可以講吓back propagation,k-nearest neighbor,support vector machine同decision tree,我ee仔冇ml background但依幾日要知佢做緊乜<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"0f59826164f05bd28d0939cd4d282bd0dada0412","tid":257808,"uid":28279,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-06-09T23:30:18.000Z","msg":"<blockquote>有冇巴打可以講吓back propagation,k-nearest neighbor,support vector machine同decision tree,我ee仔冇ml background但依幾日要知佢做緊乜<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\nBackpropagation即係一個Algorithm去計 neural network入面parameter，原理係用network output result個error 去層層逆推計parameter以達致最適error，行完結果係得到一個neural network，我諗做分類<br />\nk-nearest neighbor (K-means)做data分類，原理係每粒data附近k粒最近既data應該大部分係同類，行完結果係一堆分好類既data<br />\nSupport vector machine (SVM)做data分類，原理係用data投影喺條界上既位置計距離，行完結果係得到一條界cut係兩邊data既正中間<br />\nDecision tree我諗指分類 learning嗰隻，原理係逐個feature起個條件分類，得到一個跟條件既分類法，例子如下: 你身高夠唔夠180cm? 夠&gt;你會考有無30分? 有&gt;你賓周有無30cm長? 有&gt;...諸如此類...&gt;你係高登仔<br />\n<br />\n<br />\n利申: 其實我唔太識，係上網自學，我都無background，不過見好似無人答你先講兩句，有咩唔明大家研究下，下面係一啲舊年上coursera堂送既reference<br />\n<br />\nBackpropagation Algorithm<br />\n<a href=\"http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm\" data-sr-url=\"https://r.lihkg.com/link?u=http%3A%2F%2Fufldl.stanford.edu%2Fwiki%2Findex.php%2FBackpropagation_Algorithm&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=7872d0b3\" target=\"_blank\">http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm</a><br />\nAn Idiot&rsquo;s guide to Support vector machines (SVMs)<br />\n<a href=\"http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf\" data-sr-url=\"https://r.lihkg.com/link?u=http%3A%2F%2Fweb.mit.edu%2F6.034%2Fwwwbob%2Fsvm-notes-long-08.pdf&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=8302fea7\" target=\"_blank\">http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf</a>"},{"pid":"805ffa8b7a46f1f85e82faab8bda03b7dc6c7353","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-06-10T00:14:29.000Z","msg":"<blockquote><blockquote>有冇巴打可以講吓back propagation,k-nearest neighbor,support vector machine同decision tree,我ee仔冇ml background但依幾日要知佢做緊乜<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\nBackpropagation即係一個Algorithm去計 neural network入面parameter，原理係用network output result個error 去層層逆推計parameter以達致最適error，行完結果係得到一個neural network，我諗做分類<br />\nk-nearest neighbor (K-means)做data分類，原理係每粒data附近k粒最近既data應該大部分係同類，行完結果係一堆分好類既data<br />\nSupport vector machine (SVM)做data分類，原理係用data投影喺條界上既位置計距離，行完結果係得到一條界cut係兩邊data既正中間<br />\nDecision tree我諗指分類 learning嗰隻，原理係逐個feature起個條件分類，得到一個跟條件既分類法，例子如下: 你身高夠唔夠180cm? 夠&gt;你會考有無30分? 有&gt;你賓周有無30cm長? 有&gt;...諸如此類...&gt;你係高登仔<br />\n<br />\n<br />\n利申: 其實我唔太識，係上網自學，我都無background，不過見好似無人答你先講兩句，有咩唔明大家研究下，下面係一啲舊年上coursera堂送既reference<br />\n<br />\nBackpropagation Algorithm<br />\n<a href=\"http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm\" data-sr-url=\"https://r.lihkg.com/link?u=http%3A%2F%2Fufldl.stanford.edu%2Fwiki%2Findex.php%2FBackpropagation_Algorithm&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=7872d0b3\" target=\"_blank\">http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm</a><br />\nAn Idiot&rsquo;s guide to Support vector machines (SVMs)<br />\n<a href=\"http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf\" data-sr-url=\"https://r.lihkg.com/link?u=http%3A%2F%2Fweb.mit.edu%2F6.034%2Fwwwbob%2Fsvm-notes-long-08.pdf&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=8302fea7\" target=\"_blank\">http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf</a></blockquote><br />\nBp講緊由錯誤中學習 用自定既error measure(最簡單就係euclidean distance)去improve返neural network, 如果高既learning rate就會學曬呢一次既野 而冇左好多以前學左既野<br />\n<br />\n本意係minimize error, geometrically 睇就係要去到error curve既minimum point, 所以涉及patial d去搵呢個pt, 好似dse curve sketching搵minimum pt咁<br />\n<br />\n如果你有d numerical analysis底就可以理解為step size太大 去唔到optimal point<br />\nAs a signal黎講就係sample得太疏 唔會知道真正既最低點（唔係100%岩 只係類近d既講法）<br />\n<br />\nK-mean多數係做clustering 將最近(呢個distance又係要自己define, 多數係euclidean)既痴埋一齊<br />\n<br />\nSVM多數係用黎做classification 要睇你本身用咩kernel, 冇就係linear, 搵一條optimal既直線分開兩個group"},{"pid":"33a49637b3b8cfcef6a34e810fe9053a7194fb9d","tid":257808,"uid":28279,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-06-10T00:37:59.000Z","msg":"<blockquote><blockquote><blockquote>有冇巴打可以講吓back propagation,k-nearest neighbor,support vector machine同decision tree,我ee仔冇ml background但依幾日要知佢做緊乜<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\nBackpropagation即係一個Algorithm去計 neural network入面parameter，原理係用network output result個error 去層層逆推計parameter以達致最適error，行完結果係得到一個neural network，我諗做分類<br />\nk-nearest neighbor (K-means)做data分類，原理係每粒data附近k粒最近既data應該大部分係同類，行完結果係一堆分好類既data<br />\nSupport vector machine (SVM)做data分類，原理係用data投影喺條界上既位置計距離，行完結果係得到一條界cut係兩邊data既正中間<br />\nDecision tree我諗指分類 learning嗰隻，原理係逐個feature起個條件分類，得到一個跟條件既分類法，例子如下: 你身高夠唔夠180cm? 夠&gt;你會考有無30分? 有&gt;你賓周有無30cm長? 有&gt;...諸如此類...&gt;你係高登仔<br />\n<br />\n<br />\n利申: 其實我唔太識，係上網自學，我都無background，不過見好似無人答你先講兩句，有咩唔明大家研究下，下面係一啲舊年上coursera堂送既reference<br />\n<br />\nBackpropagation Algorithm<br />\n<a href=\"http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm\" data-sr-url=\"https://r.lihkg.com/link?u=http%3A%2F%2Fufldl.stanford.edu%2Fwiki%2Findex.php%2FBackpropagation_Algorithm&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=7872d0b3\" target=\"_blank\">http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm</a><br />\nAn Idiot&rsquo;s guide to Support vector machines (SVMs)<br />\n<a href=\"http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf\" data-sr-url=\"https://r.lihkg.com/link?u=http%3A%2F%2Fweb.mit.edu%2F6.034%2Fwwwbob%2Fsvm-notes-long-08.pdf&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=8302fea7\" target=\"_blank\">http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf</a></blockquote><br />\nBp講緊由錯誤中學習 用自定既error measure(最簡單就係euclidean distance)去improve返neural network, 如果高既learning rate就會學曬呢一次既野 而冇左好多以前學左既野<br />\n<br />\n本意係minimize error, geometrically 睇就係要去到error curve既minimum point, 所以涉及patial d去搵呢個pt, 好似dse curve sketching搵minimum pt咁<br />\n<br />\n如果你有d numerical analysis底就可以理解為step size太大 去唔到optimal point<br />\nAs a signal黎講就係sample得太疏 唔會知道真正既最低點（唔係100%岩 只係類近d既講法）<br />\n<br />\nK-mean多數係做clustering 將最近(呢個distance又係要自己define, 多數係euclidean)既痴埋一齊<br />\n<br />\nSVM多數係用黎做classification 要睇你本身用咩kernel, 冇就係linear, 搵一條optimal既直線分開兩個group</blockquote><br />\n<br />\n<img src=\"/assets/faces/fs/smile.gif\" class=\"hkgmoji\" /> 係喎，我係咪都叫分類，講得唔清楚<br />\n分類有分有監督同無監督，有監即係手頭上既sample一早已經知邊個打邊類，用現有預估將來，無監即係而家唔知邊個打邊個，靠估分返一群群<br />\n<br />\n<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> 巴打可唔可以講埋Decision tree learning"},{"pid":"fe935f9b6964591170f04ee87d109e5b07451eee","tid":257808,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-06-10T00:39:03.000Z","msg":"<blockquote><blockquote><blockquote><blockquote>有冇巴打可以講吓back propagation,k-nearest neighbor,support vector machine同decision tree,我ee仔冇ml background但依幾日要知佢做緊乜<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\nBackpropagation即係一個Algorithm去計 neural network入面parameter，原理係用network output result個error 去層層逆推計parameter以達致最適error，行完結果係得到一個neural network，我諗做分類<br />\nk-nearest neighbor (K-means)做data分類，原理係每粒data附近k粒最近既data應該大部分係同類，行完結果係一堆分好類既data<br />\nSupport vector machine (SVM)做data分類，原理係用data投影喺條界上既位置計距離，行完結果係得到一條界cut係兩邊data既正中間<br />\nDecision tree我諗指分類 learning嗰隻，原理係逐個feature起個條件分類，得到一個跟條件既分類法，例子如下: 你身高夠唔夠180cm? 夠&gt;你會考有無30分? 有&gt;你賓周有無30cm長? 有&gt;...諸如此類...&gt;你係高登仔<br />\n<br />\n<br />\n利申: 其實我唔太識，係上網自學，我都無background，不過見好似無人答你先講兩句，有咩唔明大家研究下，下面係一啲舊年上coursera堂送既reference<br />\n<br />\nBackpropagation Algorithm<br />\n<a href=\"http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm\" data-sr-url=\"https://r.lihkg.com/link?u=http%3A%2F%2Fufldl.stanford.edu%2Fwiki%2Findex.php%2FBackpropagation_Algorithm&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=7872d0b3\" target=\"_blank\">http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm</a><br />\nAn Idiot&rsquo;s guide to Support vector machines (SVMs)<br />\n<a href=\"http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf\" data-sr-url=\"https://r.lihkg.com/link?u=http%3A%2F%2Fweb.mit.edu%2F6.034%2Fwwwbob%2Fsvm-notes-long-08.pdf&d=Fi3RlZIS89NrQIGLncg3OvM6KyiA14vjm6WlJCMZo68%3D&h=8302fea7\" target=\"_blank\">http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf</a></blockquote><br />\nBp講緊由錯誤中學習 用自定既error measure(最簡單就係euclidean distance)去improve返neural network, 如果高既learning rate就會學曬呢一次既野 而冇左好多以前學左既野<br />\n<br />\n本意係minimize error, geometrically 睇就係要去到error curve既minimum point, 所以涉及patial d去搵呢個pt, 好似dse curve sketching搵minimum pt咁<br />\n<br />\n如果你有d numerical analysis底就可以理解為step size太大 去唔到optimal point<br />\nAs a signal黎講就係sample得太疏 唔會知道真正既最低點（唔係100%岩 只係類近d既講法）<br />\n<br />\nK-mean多數係做clustering 將最近(呢個distance又係要自己define, 多數係euclidean)既痴埋一齊<br />\n<br />\nSVM多數係用黎做classification 要睇你本身用咩kernel, 冇就係linear, 搵一條optimal既直線分開兩個group</blockquote><br />\n<br />\n<img src=\"/assets/faces/fs/smile.gif\" class=\"hkgmoji\" /> 係喎，我係咪都叫分類，講得唔清楚<br />\n分類有分有監督同無監督，有監即係手頭上既sample一早已經知邊個打邊類，用現有預估將來，無監即係而家唔知邊個打邊個，靠估分返一群群<br />\n<br />\n<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> 巴打可唔可以講埋Decision tree learning</blockquote><br />\nDecision tree我所知就差唔多"},{"pid":"aedcf6ab7d15c22252a84c04103974326870b281","tid":257808,"uid":28279,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-06-10T00:46:11.000Z","msg":"<img src=\"/assets/faces/lomoji/28.png\" class=\"hkgmoji\" /> 巴打禮拜六都起到身，我都想乘機問下euclidean distance係咪即係L2 norm? 一時一個名好混亂，幾時用邊個名?"},{"pid":"3ab4203dbad0c3973792890ff2f5d0e025cfa0fd","tid":257808,"uid":34301,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-06-10T00:50:58.000Z","msg":"<blockquote><img src=\"/assets/faces/lomoji/28.png\" class=\"hkgmoji\" /> 巴打禮拜六都起到身，我都想乘機問下euclidean distance係咪即係L2 norm? 一時一個名好混亂，幾時用邊個名?</blockquote><br />\n係<br />\nnorm係linear algebra入面嘅concept<br />\n其實用邊個都無所謂"},{"pid":"96b78edf7fc695398678d126aac8685e388717ff","tid":257808,"uid":28279,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-06-10T00:55:46.000Z","msg":"<blockquote><blockquote><img src=\"/assets/faces/lomoji/28.png\" class=\"hkgmoji\" /> 巴打禮拜六都起到身，我都想乘機問下euclidean distance係咪即係L2 norm? 一時一個名好混亂，幾時用邊個名?</blockquote><br />\n係<br />\nnorm係linear algebra入面嘅concept<br />\n其實用邊個都無所謂</blockquote><br />\n<img src=\"/assets/faces/lomoji/26.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/26.png\" class=\"hkgmoji\" /> 咁鬼多名，以前中學做數一日做到黑邊有咁多英文名"}]}