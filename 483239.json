{"tid":483239,"cid":18,"subCid":0,"title":"[ 人工智能 ]簡介下現代AI","createTime":"2017-11-29T02:53:14.000Z","updateTime":"2018-04-24T11:25:18.000Z","uid":12599,"like":180,"dislike":4,"uniUserReply":64,"replies":[{"pid":"2f2d32a3057ea610a9fa693b282590360c1ae326","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T02:53:14.000Z","msg":"細佬我做image processing打code打左4年左右, 一年前開始睇google同facebook d image processing 同拎佢地d code來自己做development<br />\n<br />\n其實香港真係好少人識當代最先進既AI,好多人仲以為係if else statement既年代,呢個post就簡單講解下AI係乜東西,其實AI唔止係alphago咁簡單架<br />\n<br />\n係睇呢個post前可以先重溫下兩個舊post:<br />\n<br />\n簡單了解AlphaGo 同今日既AI<br />\n<a href=\"https://lihkg.com/thread/257808/page/1\" target=\"_blank\">https://lihkg.com/thread/257808/page/1</a><br />\n<br />\n大數據深度學習 AI醫生診症快又準<br />\n<a href=\"https://lihkg.com/thread/404330/page/1\" target=\"_blank\">https://lihkg.com/thread/404330/page/1</a><br />\n<br />\n<span style=\"font-size: xx-large;\"><span style=\"color: red;\">定義</span></span><br />\n係初步了解咩係AI前,先睇下唔同的哲學家點定義人工智能呢一樣野,當中最出名就一定係電腦之父圖靈提出的圖靈測試<br />\n<br />\n<img src=\"http://pic.pimg.tw/uselesswood3/1425301678-2621973798_n.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fpic.pimg.tw%2Fuselesswood3%2F1425301678-2621973798_n.jpg&h=8301032d&s={SIZE}\" /><br />\n<del>電腦之父Alan Turing </del><br />\n<br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a1/Alan_Turing_Aged_16.jpg/220px-Alan_Turing_Aged_16.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fthumb%2Fa%2Fa1%2FAlan_Turing_Aged_16.jpg%2F220px-Alan_Turing_Aged_16.jpg&h=7c4cd302&s={SIZE}\" /><br />\nsorry 呢個先係 16歲的Alan Turing<br />\n<br />\n<span style=\"color: green;\"><span style=\"font-size: x-large;\">圖靈測試</span></span><br />\n呢個測試好簡單,你blind左咁去問一部機器,如果你無方法去分佢究竟係真人定係唔係,咁呢部機器就已經pass左圖靈測試,亦被認為係真正擁有人工智能<br />\n<br />\n而家係無一部機器係成功通過圖靈測試,最近睇左一套電影,對圖靈測試有更深入既探究,講緊係究竟只係語言上通過圖靈測試是否真係可以被證明為有人工智能:<br />\n<br />\n<a href=\"https://www.youtube.com/watch?v=15RZg4kp6qI\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D15RZg4kp6qI&d=tjzMqdWYAiG4enN%2Fcf3LS6NGKT0mDZcTD8WJHRevFdc%3D&h=f69da198\" target=\"_blank\">https://www.youtube.com/watch?v=15RZg4kp6qI</a><br />\nEx_Machina 智能叛侶, netflix同google play都有,劇情一般但係就對咩係AI有比較深入既探討<br />\n<br />\n係電腦岩岩發明既年代,所有同電算有關既基礎theory都好快速咁發展,當中最出名既要數人工智能之父約翰.麥卡錫1956年係第一次達特矛斯會議提出左「人工智能」一詞。佢既諗法同圖靈都係差唔多:<br />\n<br />\n<em><div style=\"text-align: center;\"><span style=\"color: blue;\">人工智能就是要讓機器的行為看起來就像是人所表現出的智能行為一樣。</span></div></em><br />\n<br />\n但咁樣係咪就係真正既人工智能呢? 係哲學上又有另一個思想實驗,令唯心論同唯物論者對人工智能有唔同既理解<br />\n<br />\n<span style=\"color: green;\"><span style=\"font-size: x-large;\">中文房間</span></span><br />\n如果麥卡錫同圖靈都係人工智能既唯物論派,咁約翰&middot;希爾勒就應該係唯心論既代表人物<br />\n<br />\n佢提出左中文房間呢個問題,呢個問題係關乎宇宙同人類既本質問題,不過呢樣野太複雜,呢度唔多講,介紹返個實驗先:<br />\n<br />\n<em>一個對漢語一竅不通，只說英語的人關在一間只有一個開口的封閉房間中。房間裏有一本用英文寫成的手冊，指示該如何處理收到的漢語訊息及如何以漢語相應地回覆。房外的人不斷向房間內遞進用中文寫成的問題。房內的人便按照手冊的說明，尋找到合適的指示，將相應的中文字元組合成對問題的解答，並將答案遞出房間。</em><br />\n<br />\n事實上,房入面既人係完全唔識中文,佢只係按一d規律去達成「智能」既假像,而家既google/ siri 之類軟件好大程度都係做緊類似既野,係人工智能定義上,呢兩派仍然係爭論不休。不過,哲學問題通常都係知道左都唔會影響世界運作既野,咁我地focus返去d solid d既AI題目上面<br />\n<br />\n<span style=\"color: red;\">[/size=6]強人工智能和弱人工智能</span>[/size=6]<br />\n<span style=\"color: green;\"><span style=\"font-size: x-large;\">強人工智能</span></span><br />\n呢樣野又可以叫做通用人工智能,講緊係一部機械可以完完全全做到人類先做到既野,亦即係通過到圖靈測試或其他類近既AI測試。呢樣野一度被認為不可能存在,但係近幾年既電腦運算力同人工神經網路理論有飛躍性進步,加上Big Data加持,令到呢一個topic又再heat起來。但事實上,強人工智能仍然未被發明<br />\n<br />\n<span style=\"color: green;\"><span style=\"font-size: x-large;\">弱人工智能</span></span><br />\n呢樣野已經存在,就係做特定任務的電腦軟件,都可以叫做弱人工智能。係2009年康乃爾大學俾左一d data入電腦入面,個program用左幾十個鐘就自己發掘出牛頓力學方程<br />\n<br />\n雖然睇上去已經非常地有智能,但由於一個program只能做到一件事,所以唔算係真正意義上既AI。而家聲音,圖像,情感之類既電腦處理已經好成熟,但係點樣完全整合仍然係一個大難題。<br />\n<br />\n==========================================================<br />\n大家可能睇左好多字too long didn't read, 但好可惜,之後1-2個chapter仍然係好多字好少圖 <br />\n<br />\n如果大家真係對現代AI有興趣可以等我寫到人工神經網絡先開始睇"},{"pid":"1dd5588a4b6ee340d394a71d69163e0af2d6d409","tid":483239,"uid":113745,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T03:08:07.000Z","msg":"快d等緊，唔該"},{"pid":"e759bfd891ba9ff71c3470ecb1e30d6985937925","tid":483239,"uid":125113,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T03:14:08.000Z","msg":"評已正"},{"pid":"5a8e8444cc4716db54ea992cae779df70a3f9699","tid":483239,"uid":12599,"like":7,"dislike":2,"score":5,"citedBy":0,"replyTime":"2017-11-29T03:17:27.000Z","msg":"現代google/facebook之類既AI之所以能夠成功,主要歸功於big data既時代到臨,大家都手持一部手機,部手機仲要上到網影到相拍到片錄到聲,呢樣野對於數據收集非常有用。<br />\n<br />\n但係單有big data係唔夠的,伴隨住big data而來既係機械學習技術。其實就已經唔係咩新野,不過由於big data既數據量實在非常之大,統計學係呢個時候就能夠有效發揮作用<br />\n<br />\n<span style=\"color: red;\"><span style=\"font-size: xx-large;\">機器學習</span></span><br />\n人工智能既發展有3個時期<br />\n<br />\n<div style=\"text-align: center;\"><span style=\"font-size: x-large;\"><span style=\"color: blue;\">推理 -&gt; 知識 -&gt; 學習</span>[/size5]</div><br />\n<br />\n[size=5]<span style=\"color: green;\">推理期</span></span><br />\n係古早既人工智能時期,科學家都覺得如果機器有邏輯推理能力,就可以好似人咁擁有智慧。呢樣野即係你學programming果陣的if else while for,但係好對唔住,雖然呢d logic係構成運算系統既核心,但係單係有邏輯推理,一部機器跟本無可能知道世界既原理,所以推理呢條路行唔通<br />\n<br />\n<span style=\"font-size: x-large;\"><span style=\"color: green;\">知識期</span></span><br />\n係科學家了解到單係識得推理跟本就應付唔到世界既原理之後,佢地就suggest,如果電腦放曬本百科全書入去,你再問佢問佢問題,佢咪乜都識囉<img src=\"/assets/faces/normal/hehe.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/hehe.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/hehe.gif\" class=\"hkgmoji\" /> <br />\n<br />\n事實唔係咁的,你睇下香港d填鴨式教育,就知呢種方式去令機器有智能,根本死路一條,於是係80年代,係電腦普及到大型公司同大學都已經配備, coding都已經容易左好多,數據既儲存量都比50年代上月球果陣d電腦多左唔知幾多倍。係咁既背景下,人類開始意識到數據可以用數學方法統計歸納,於是電腦學習呢個分支就慢慢建立出來<br />\n<br />\n<span style=\"font-size: x-large;\"><span style=\"color: green;\">學習期</span></span><br />\n經過90年代個人電腦同多媒體發展,去到00年代apple推出smartphone,到今年發表iphone X,人類係呢20年間既數據收集可以話係遠遠超愈左過去幾千年既資訊量,點樣更有效運用呢d data,成為左今日AI/ Big Data界既重要課題,電腦學習係現時最有效俾機械發展人工智能既方式<br />\n<br />\n以下將會介紹下唔同既學習方式,唔岩睇既,都係果句,等到人工神經網路果part先睇都ok"},{"pid":"7c40ebe2bff03b7350dbf961e6c65ef5e728d5d4","tid":483239,"uid":53122,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T03:21:18.000Z","msg":"it狗支持"},{"pid":"2c797ad050a782cff1d9de849ace4f48b894d475","tid":483239,"uid":112769,"like":2,"dislike":1,"score":1,"citedBy":0,"replyTime":"2017-11-29T03:28:42.000Z","msg":"咁岩我畢業論文寫 論機械人的道德考量<br />\n巴打有冇reference?<img src=\"/assets/faces/big/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/big/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/big/adore.gif\" class=\"hkgmoji\" />"},{"pid":"59c710c452a88ab1cd0d374678e12c223dee24a0","tid":483239,"uid":112769,"like":1,"dislike":10,"score":-9,"citedBy":0,"replyTime":"2017-11-29T03:33:25.000Z","msg":"<blockquote>細佬我做image processing打code打左4年左右, 一年前開始睇google同facebook d image processing 同拎佢地d code來自己做development<br />\n<br />\n其實香港真係好少人識當代最先進既AI,好多人仲以為係if else statement既年代,呢個post就簡單講解下AI係乜東西,其實AI唔止係alphago咁簡單架<br />\n<br />\n係睇呢個post前可以先重溫下兩個舊post:<br />\n<br />\n簡單了解AlphaGo 同今日既AI<br />\n<a href=\"https://lihkg.com/thread/257808/page/1\" target=\"_blank\">https://lihkg.com/thread/257808/page/1</a><br />\n<br />\n大數據深度學習 AI醫生診症快又準<br />\n<a href=\"https://lihkg.com/thread/404330/page/1\" target=\"_blank\">https://lihkg.com/thread/404330/page/1</a><br />\n<br />\n<span style=\"font-size: xx-large;\"><span style=\"color: red;\">定義</span></span><br />\n係初步了解咩係AI前,先睇下唔同的哲學家點定義人工智能呢一樣野,當中最出名就一定係電腦之父圖靈提出的圖靈測試<br />\n<br />\n<img src=\"http://pic.pimg.tw/uselesswood3/1425301678-2621973798_n.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fpic.pimg.tw%2Fuselesswood3%2F1425301678-2621973798_n.jpg&h=8301032d&s={SIZE}\" /><br />\n<del>電腦之父Alan Turing </del><br />\n<br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a1/Alan_Turing_Aged_16.jpg/220px-Alan_Turing_Aged_16.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fthumb%2Fa%2Fa1%2FAlan_Turing_Aged_16.jpg%2F220px-Alan_Turing_Aged_16.jpg&h=7c4cd302&s={SIZE}\" /><br />\nsorry 呢個先係 16歲的Alan Turing<br />\n<br />\n<span style=\"color: green;\"><span style=\"font-size: x-large;\">圖靈測試</span></span><br />\n呢個測試好簡單,你blind左咁去問一部機器,如果你無方法去分佢究竟係真人定係唔係,咁呢部機器就已經pass左圖靈測試,亦被認為係真正擁有人工智能<br />\n<br />\n而家係無一部機器係成功通過圖靈測試,最近睇左一套電影,對圖靈測試有更深入既探究,講緊係究竟只係語言上通過圖靈測試是否真係可以被證明為有人工智能:<br />\n<br />\n<a href=\"https://www.youtube.com/watch?v=15RZg4kp6qI\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D15RZg4kp6qI&d=tjzMqdWYAiG4enN%2Fcf3LS6NGKT0mDZcTD8WJHRevFdc%3D&h=f69da198\" target=\"_blank\">https://www.youtube.com/watch?v=15RZg4kp6qI</a><br />\nEx_Machina 智能叛侶, netflix同google play都有,劇情一般但係就對咩係AI有比較深入既探討<br />\n<br />\n係電腦岩岩發明既年代,所有同電算有關既基礎theory都好快速咁發展,當中最出名既要數人工智能之父約翰.麥卡錫1956年係第一次達特矛斯會議提出左「人工智能」一詞。佢既諗法同圖靈都係差唔多:<br />\n<br />\n<em><div style=\"text-align: center;\"><span style=\"color: blue;\">人工智能就是要讓機器的行為看起來就像是人所表現出的智能行為一樣。</span></div></em><br />\n<br />\n但咁樣係咪就係真正既人工智能呢? 係哲學上又有另一個思想實驗,令唯心論同唯物論者對人工智能有唔同既理解<br />\n<br />\n<span style=\"color: green;\"><span style=\"font-size: x-large;\">中文房間</span></span><br />\n如果麥卡錫同圖靈都係人工智能既唯物論派,咁約翰&middot;希爾勒就應該係唯心論既代表人物<br />\n<br />\n佢提出左中文房間呢個問題,呢個問題係關乎宇宙同人類既本質問題,不過呢樣野太複雜,呢度唔多講,介紹返個實驗先:<br />\n<br />\n<em>一個對漢語一竅不通，只說英語的人關在一間只有一個開口的封閉房間中。房間裏有一本用英文寫成的手冊，指示該如何處理收到的漢語訊息及如何以漢語相應地回覆。房外的人不斷向房間內遞進用中文寫成的問題。房內的人便按照手冊的說明，尋找到合適的指示，將相應的中文字元組合成對問題的解答，並將答案遞出房間。</em><br />\n<br />\n事實上,房入面既人係完全唔識中文,佢只係按一d規律去達成「智能」既假像,而家既google/ siri 之類軟件好大程度都係做緊類似既野,係人工智能定義上,呢兩派仍然係爭論不休。不過,哲學問題通常都係知道左都唔會影響世界運作既野,咁我地focus返去d solid d既AI題目上面<br />\n<br />\n<span style=\"color: red;\">[/size=6]強人工智能和弱人工智能</span>[/size=6]<br />\n<span style=\"color: green;\"><span style=\"font-size: x-large;\">強人工智能</span></span><br />\n呢樣野又可以叫做通用人工智能,講緊係一部機械可以完完全全做到人類先做到既野,亦即係通過到圖靈測試或其他類近既AI測試。呢樣野一度被認為不可能存在,但係近幾年既電腦運算力同人工神經網路理論有飛躍性進步,加上Big Data加持,令到呢一個topic又再heat起來。但事實上,強人工智能仍然未被發明<br />\n<br />\n<span style=\"color: green;\"><span style=\"font-size: x-large;\">弱人工智能</span></span><br />\n呢樣野已經存在,就係做特定任務的電腦軟件,都可以叫做弱人工智能。係2009年康乃爾大學俾左一d data入電腦入面,個program用左幾十個鐘就自己發掘出牛頓力學方程<br />\n<br />\n雖然睇上去已經非常地有智能,但由於一個program只能做到一件事,所以唔算係真正意義上既AI。而家聲音,圖像,情感之類既電腦處理已經好成熟,但係點樣完全整合仍然係一個大難題。<br />\n<br />\n==========================================================<br />\n大家可能睇左好多字too long didn't read, 但好可惜,之後1-2個chapter仍然係好多字好少圖 <br />\n<br />\n如果大家真係對現代AI有興趣可以等我寫到人工神經網絡先開始睇</blockquote><br />\n<br />\n可唔可以介紹幾本當今有權威性論AI有可能出現的書<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"49b48924893d35111dcc11ce0fb9e1f1573299b1","tid":483239,"uid":12599,"like":5,"dislike":0,"score":5,"citedBy":0,"replyTime":"2017-11-29T03:37:47.000Z","msg":"<span style=\"font-size: xx-large;\"><span style=\"color: red;\">機器學習</span></span><br />\n<br />\n<span style=\"color: green;\"><span style=\"font-size: x-large;\">分類</span></span><br />\n機器學習(Machine Learning, ML, 唔係make love 個ML), 主要分為3個大類<br />\n<br />\n<img src=\"http://www.dxai.com/wp-content/uploads/2017/02/types-of-machine-learning.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fwww.dxai.com%2Fwp-content%2Fuploads%2F2017%2F02%2Ftypes-of-machine-learning.jpg&h=ec0bfe16&s={SIZE}\" /><br />\n1. 監督學習 (Supervised Learning)<br />\n已知結果既訓練方式,再教返部機點樣做先係岩既方式,最常見就係回歸(regression), 支持向量機 (support vector machine, SVM), 同埋近期最hit的人工神經網路(artificaial neural network, ANN) <br />\n<br />\n2. 無監督學習(Unsupervised Learning)<br />\n唔需要知道結果,由數據本身特性做歸納。最出名就係聚類(clustering)。<br />\n<br />\n3. 強化學習(Reinforced Learning)<br />\n我地唔feed data俾電腦,淨係叫佢鳩行,行到岩就話俾佢知係岩,行錯左就屌鳩佢,由部機自由發展。呢程學習法人類由細到大都係咁做,你細個仆街仆得多去到大個左就識行路。呢個思路應用係機械人運動有非常之好既成本。<br />\n<br />\nReinforcement Learning近期最成功一定係Boston Dynamics隻機械人做後空翻啦<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <br />\n<a href=\"https://www.youtube.com/watch?v=WcbGRBPkrps\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DWcbGRBPkrps&d=tjzMqdWYAiG4enN%2Fcf3LS6NGKT0mDZcTD8WJHRevFdc%3D&h=614f19b5\" target=\"_blank\">https://www.youtube.com/watch?v=WcbGRBPkrps</a><br />\n<br />\n順手講下alphago, 佢最後versioni係1+3 一齊做,最新既alphago zero淨係用左3,係幾日內用google TPU已經贏曬人類4000年智慧<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\nMachine learning algo有好多隻,實際上係要按照data先再決定用邊隻更有效<br />\n<br />\nDecision tree learning<br />\nAssociation rule learning<br />\n<span style=\"color: red;\">Artificial neural networks<br />\nDeep learning</span><br />\nInductive logic programming<br />\nSupport vector machines<br />\nClustering<br />\nBayesian networks<br />\nReinforcement learning<br />\nRepresentation learning<br />\nSimilarity and metric learning<br />\nSparse dictionary learning<br />\nGenetic algorithms<br />\nRule-based machine learning<br />\n<br />\n如果隻隻都想知係咩來,我真係無能力講,我真係識唔曬咁多款machine learning方式,網上laymen講下ANN同Deep Learning就算,真係有性趣可以睇下呢本天書<br />\n<img src=\"http://aima.cs.berkeley.edu/cover2.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Faima.cs.berkeley.edu%2Fcover2.jpg&h=1a25d8bd&s={SIZE}\" /><br />\n<br />\n呢度有PDF:<br />\n<a href=\"https://dcs.abu.edu.ng/staff/abdulrahim-abdulrazaq/courses/cosc208/Artificial%20Intelligence%20A%20Modern%20Approach%20(3rd%20Edition).pdf\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fdcs.abu.edu.ng%2Fstaff%2Fabdulrahim-abdulrazaq%2Fcourses%2Fcosc208%2FArtificial%2520Intelligence%2520A%2520Modern%2520Approach%2520%283rd%2520Edition%29.pdf&d=tjzMqdWYAiG4enN%2Fcf3LS6NGKT0mDZcTD8WJHRevFdc%3D&h=3a557d75\" target=\"_blank\">https://dcs.abu.edu.ng/staff/abdulrahim-abdulrazaq/courses/cosc208/Artificial%20Intelligence%20A%20Modern%20Approach%20(3rd%20Edition).pdf</a>"},{"pid":"bf1b7cfb63c520ac2df47ec2179e5680c156c02e","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T03:39:09.000Z","msg":"<blockquote>咁岩我畢業論文寫 論機械人的道德考量<br />\n巴打有冇reference?<img src=\"/assets/faces/big/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/big/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/big/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\nPhilo朋友推薦,唔知岩唔岩你睇<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n<a href=\"https://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwirmKL27eLXAhVHjLwKHTVaANcQFgglMAA&amp;url=http%3A%2F%2Fmichaeljohnsonphilosophy.com%2Fwp-content%2Fuploads%2F2012%2F11%2FMechanical-Mind.pdf&amp;usg=AOvVaw08fsReaqnKcP_2a4Axkcvr\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.google.com.hk%2Furl%3Fsa%3Dt%26rct%3Dj%26q%3D%26esrc%3Ds%26source%3Dweb%26cd%3D1%26cad%3Drja%26uact%3D8%26ved%3D0ahUKEwirmKL27eLXAhVHjLwKHTVaANcQFgglMAA%26url%3Dhttp%253A%252F%252Fmichaeljohnsonphilosophy.com%252Fwp-content%252Fuploads%252F2012%252F11%252FMechanical-Mind.pdf%26usg%3DAOvVaw08fsReaqnKcP_2a4Axkcvr&d=tjzMqdWYAiG4enN%2Fcf3LS6NGKT0mDZcTD8WJHRevFdc%3D&h=69e466b4\" target=\"_blank\">https://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwirmKL27eLXAhVHjLwKHTVaANcQFgglMAA&amp;url=http%3A%2F%2Fmichaeljohnsonphilosophy.com%2Fwp-content%2Fuploads%2F2012%2F11%2FMechanical-Mind.pdf&amp;usg=AOvVaw08fsReaqnKcP_2a4Axkcvr</a>"},{"pid":"0adfaade1add2ca89d4ed53ab8a6f95d4cc702e9","tid":483239,"uid":12599,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-11-29T03:41:04.000Z","msg":"<blockquote><blockquote>咁岩我畢業論文寫 論機械人的道德考量<br />\n巴打有冇reference?<img src=\"/assets/faces/big/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/big/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/big/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\nPhilo朋友推薦,唔知岩唔岩你睇<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n<a href=\"https://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwirmKL27eLXAhVHjLwKHTVaANcQFgglMAA&amp;url=http%3A%2F%2Fmichaeljohnsonphilosophy.com%2Fwp-content%2Fuploads%2F2012%2F11%2FMechanical-Mind.pdf&amp;usg=AOvVaw08fsReaqnKcP_2a4Axkcvr\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.google.com.hk%2Furl%3Fsa%3Dt%26rct%3Dj%26q%3D%26esrc%3Ds%26source%3Dweb%26cd%3D1%26cad%3Drja%26uact%3D8%26ved%3D0ahUKEwirmKL27eLXAhVHjLwKHTVaANcQFgglMAA%26url%3Dhttp%253A%252F%252Fmichaeljohnsonphilosophy.com%252Fwp-content%252Fuploads%252F2012%252F11%252FMechanical-Mind.pdf%26usg%3DAOvVaw08fsReaqnKcP_2a4Axkcvr&d=tjzMqdWYAiG4enN%2Fcf3LS6NGKT0mDZcTD8WJHRevFdc%3D&h=69e466b4\" target=\"_blank\">https://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwirmKL27eLXAhVHjLwKHTVaANcQFgglMAA&amp;url=http%3A%2F%2Fmichaeljohnsonphilosophy.com%2Fwp-content%2Fuploads%2F2012%2F11%2FMechanical-Mind.pdf&amp;usg=AOvVaw08fsReaqnKcP_2a4Axkcvr</a></blockquote><br />\n工程書就有2-3本介紹下,github repository都有,講moral我真係唔識,我唔係哲學/道德撚<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"497ba68a5420af1868a9a074df8de5ba0eb4ba30","tid":483239,"uid":25055,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T03:51:38.000Z","msg":"巴打讀細電？<br />\nDsp 好多數同stat 又要識programming <img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" />"},{"pid":"f1b7a5100fd77efaf9ff7396a516b530131242ca","tid":483239,"uid":12599,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-11-29T03:56:26.000Z","msg":"<span style=\"font-size: xx-large;\"><span style=\"color: red;\">人工神經元</span></span><br />\n<br />\n<span style=\"color: green;\"><span style=\"font-size: x-large;\">生物神經元</span></span><br />\n<br />\n<img src=\"https://cdn.stemcellthailand.org/wp-content/uploads/2014/04/neurons-brain-injury-therapy.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fcdn.stemcellthailand.org%2Fwp-content%2Fuploads%2F2014%2F04%2Fneurons-brain-injury-therapy.jpg&h=2056ddeb&s={SIZE}\" /><br />\n<br />\n我地人類既神經系統,係由呢張相既神經元細胞組成,呢d cell去到某一個年紀唔會再有新既/就算有新既都會好慢地regenerate,所以如果我地d神經線/腦部炒左大獲,就會好難醫<br />\n<br />\n呢隻cell點樣運作呢?我唔係專攻neuroscience所以只可以簡介下<br />\n<br />\n基本原理係俾d 電化學信號入cell的一邊,隻cell會改變個電信號,由另一邊再放返d唔同的電化學物質出來,一直傳到成個神經系統<br />\n<br />\n再深少少講樹突(Dendrites) 食左d chemical,由cell body去決定點改變chemical 變化,再由軸突(Axon)放帶電chemical,整個機制好複雜下,複雜到醫院同大學要有一個專門department去醫同研究呢個area的問題<br />\n<br />\n由於神經網路就係呢d neural cell既連接同放電,係發現呢個生物機理既時候就有人問,可唔可以用機器模仿呢種行為?答案係肯定的,於是人工神經網路呢種革新既算法就出現了<br />\n<br />\n<img src=\"http://img.anyanother.com/data/62725.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fimg.anyanother.com%2Fdata%2F62725.png&h=ce862293&s={SIZE}\" />"},{"pid":"e7b4f7b3559e824d24fb52c344ba000020bc9579","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T04:13:39.000Z","msg":"<span style=\"font-size: x-large;\"><span style=\"color: green;\">人工神經元</span></span><br />\n點樣可以用數學表示一個神經元既行為?其實你中三左右就已經學左,就係用function呢個concept<br />\n<br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b6/Artificial_neural_network.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fb%2Fb6%2FArtificial_neural_network.png&h=8b6e4771&s={SIZE}\" /><br />\n<br />\n你有n個輸入值,經過hidden function f(x1,x2,...,xn)後,會出個數字y,呢個就係最簡單既人工神經元了<br />\n<br />\n問題係,用咩做人工神經元最好?最簡單,就係用linear function, 中二學生都識既y=mx+c去做,運算量最低,regression既方法亦都成熟<br />\n<br />\n不過我地而家改下y=mx+c個樣,變左咁<br />\n<img src=\"https://cdn-images-1.medium.com/max/1600/0*OHlzxsoDSEBXW0iI.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1600%2F0%2AOHlzxsoDSEBXW0iI.jpg&h=0df144e7&s={SIZE}\" /><br />\n<br />\n你個輸出係所有weight (slope)同輸入既product既相加(Sum of w_i* x_i), 後面加多個偏向值(bias),即係linear function個y intercept,成舊野就叫<del>夏天</del>summer<br />\n<br />\n但咁樣都唔係好夠,後面仲要加多個threshold function,講話最後個y輸出要大過某個數呢粒neuron先要被激活activate,咁先會傳送info去下一層既neuron,所以呢個function又叫activation function<br />\n<br />\n以前threshold function用sigmoid function<br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Gjl-t%28x%29.svg/320px-Gjl-t%28x%29.svg.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fthumb%2F6%2F6f%2FGjl-t%2528x%2529.svg%2F320px-Gjl-t%2528x%2529.svg.png&h=aec2c897&s={SIZE}\" /><br />\n<br />\n而家權衡左運算力後,發現Rectified Linear Unit(ReLU)呢條function更快更準,雖然唔知點解會work但都係用住先,我d friend話呢個function既功能係加強成個network既非線性,ReLU係最簡單做到呢件事,事實上仲有反ReLU, 負ReLU之類既變化,不過既然work又快就唔好理了<img src=\"/assets/faces/normal/ass.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/ass.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/ass.gif\" class=\"hkgmoji\" /> <br />\n<br />\n<img src=\"https://cdn-images-1.medium.com/max/1600/1*XxxiA0jJvPrHEJHD4z893g.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1600%2F1%2AXxxiA0jJvPrHEJHD4z893g.png&h=407a67ee&s={SIZE}\" />"},{"pid":"d4e686d17a0586a0184ce83adcfe73c4367d4fa5","tid":483239,"uid":60120,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T04:20:00.000Z","msg":"自從之前無聊玩excel撞見genetic algorithm<br />\n就對AI一見鍾情<br />\n留名學野<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"418c315e5ff350e4a2ec8d4a29a259e10e303e10","tid":483239,"uid":48679,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-11-29T04:23:22.000Z","msg":"<blockquote>咁岩我畢業論文寫 論機械人的道德考量<br />\n巴打有冇reference?<img src=\"/assets/faces/big/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/big/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/big/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n首先quote 番套Westworld 先"},{"pid":"925de4fd8a4e95a395702798fe43897b563ce9a9","tid":483239,"uid":100760,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T04:29:51.000Z","msg":"支持"},{"pid":"9a65fafa9ee092fda722b625b0e0969de09c383c","tid":483239,"uid":63491,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T04:35:04.000Z","msg":"香港做ai有咩好做?<br />\n<br />\n睇andrew ng coursera &amp; deep learning 睇左幾個月，都無咩呢d 工<br />\n<br />\n自己搞又無咩data"},{"pid":"3ae1aaa33000b7cde99781c6deac9f6a89ede086","tid":483239,"uid":63491,"like":0,"dislike":3,"score":-3,"citedBy":0,"replyTime":"2017-11-29T04:36:37.000Z","msg":"基本上你上面講個d 都睇過啦"},{"pid":"54ead39bc35f6fbe41600fd2016dfca5a18a7563","tid":483239,"uid":48679,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T04:38:59.000Z","msg":"<blockquote>香港做ai有咩好做?<br />\n<br />\n睇andrew ng coursera &amp; deep learning 睇左幾個月，都無咩呢d 工<br />\n<br />\n自己搞又無咩data</blockquote><br />\n香港無，過日本，好多唔洗識日文都請"},{"pid":"a96f9941ca85254f5aa5499a3d644aad2b24a5a7","tid":483239,"uid":71814,"like":0,"dislike":1,"score":-1,"citedBy":0,"replyTime":"2017-11-29T04:42:10.000Z","msg":"lm"},{"pid":"3648cf675117f768c2a6d4e04f0ffd01c1f4924c","tid":483239,"uid":7145,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-11-29T04:44:18.000Z","msg":"巴打咩background?<br />\n讀緊MSc data science有冇機做到AI related既野<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"eb5c85727cb147b1a231bfb126cbdd71b6efdeb5","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T04:48:58.000Z","msg":"雖然樓上講過下咩叫activation function,但係一個ANN算法呢條function唔係必需<br />\n<br />\nsummer得出來既結果,d人叫evidence(名來啫),咁要將呢個evidence轉成probability或者一d人可以理解既值,就會一用條叫softmax function既野<br />\n<br />\nsoftmax(x) = normalize(e^x)<br />\n<br />\n<img src=\"https://jamesmccaffrey.files.wordpress.com/2016/03/softmaxequation.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fjamesmccaffrey.files.wordpress.com%2F2016%2F03%2Fsoftmaxequation.jpg&h=b580fbb1&s={SIZE}\" /><br />\n<br />\n呢樣野就係將神經網路既輸出轉化成為概率值<br />\n<br />\n<img src=\"http://i.imgur.com/sqlI5oC.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fi.imgur.com%2FsqlI5oC.png&h=359abf4f&s={SIZE}\" /><br />\n<br />\n有左呢舊野我地就得到一個分類器(Classifier),有分類器就可以幫我地做好多決定了<br />\n<br />\n<img src=\"https://i.imgur.com/or6gDcnl.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fi.imgur.com%2For6gDcnl.jpg&h=796f447b&s={SIZE}\" /><br />\n<br />\n嘩個network咁多connect,寫得整齊d好唔好?得,而家轉做matrix form俾你睇下<br />\n<img src=\"http://i.imgur.com/ANwDWKW.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fi.imgur.com%2FANwDWKW.png&h=1bcb3b8a&s={SIZE}\" /><br />\n<br />\n唔好蝦我中學淨係讀左math core 唔識matrix,我識少少vector,改改佢我睇得明ok?<br />\n<br />\n<img src=\"http://i.imgur.com/f0c2kWB.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fi.imgur.com%2Ff0c2kWB.png&h=e6993b09&s={SIZE}\" /><br />\n<br />\n<img src=\"/assets/faces/lomoji/10.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/10.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/10.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/10.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/10.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/10.png\" class=\"hkgmoji\" /> <br />\n<br />\n由於呢堆野都係vector matrix,數學上就叫tensor,所以呢...google個AI development kit就係tensorflow,講緊呢d tensor數字點流動<br />\n<br />\n而我個人偏好facebook既ATen/ Torch,對於矩陣處理更為人性化,不過關於toolkit問題,我之後先再講<br />\n<br />\n淨係得個network,其實係無學習功能的,係成個network最後層,我地再加多個regression function,咁樣就令成個network可以進行學習了<br />\n<br />\n<img src=\"https://cdn-images-1.medium.com/max/1600/0*YRAt7P06fL7TObX-.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1600%2F0%2AYRAt7P06fL7TObX-.png&h=de4dd057&s={SIZE}\" /><br />\n<br />\n呢樣野有咩用?比如說我有個probability output 係[1,0,0], 而實際結果應該係[0, 1 ,0], 咁佢地之間既loss會好大,我地只要最少化佢地之間既loss,咁個預測應該就係最準<br />\n<br />\n常用既loss function有logit同cross entropy之類<br />\n<br />\n至於點樣對loss進行最小化,關乎一個叫做backward反向傳導既過程。呢個過程,就係要計下要點改weight同bias值,去令個network改變參數。<br />\n<br />\nbackward既運算法最出名係Stochastic Gradient Descent (SGD), 仲有d 咩Adam之類既最小化backward計算器,不過呢度唔再深入講<br />\n<br />\n成個流程變左:<br />\n1. 輸入input {x_i}<br />\n2. 係網路向前傳送(forward), 包括 weight bias activation, 計output probability<br />\n3. 計loss,我地想最小化個loss<br />\n4. 用SGD等計算器計算各neuron既參數修改值(backward)<br />\n5. repeat以上1-4流程,直到loss最小化<br />\n<br />\nANN大概都係咁樣,之後我試下用一個例子,整個ANN出來做手寫數字分析"},{"pid":"81c1bfa7ccd1686cca7c7f4e643cc877ecd841cc","tid":483239,"uid":37489,"like":0,"dislike":1,"score":-1,"citedBy":0,"replyTime":"2017-11-29T04:49:28.000Z","msg":"等睇比人屌唔識中文又離地<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"51d3e530eeef08a52f82c52e0f441f52d867cacb","tid":483239,"uid":12599,"like":5,"dislike":1,"score":4,"citedBy":0,"replyTime":"2017-11-29T04:55:22.000Z","msg":"<blockquote>等睇比人屌唔識中文又離地<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n已經寫到好貼地<img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" />"},{"pid":"e87daeb6ebd8a5f7d89a3df8f4b47005d96613f8","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T04:56:58.000Z","msg":"<blockquote>巴打咩background?<br />\n讀緊MSc data science有冇機做到AI related既野<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\nbachleor in physics, master唔講<br />\n<br />\n基本上咩displine都可以用到AI<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"7a086f9b3a2110e6068031097be64b1fda87dc98","tid":483239,"uid":12599,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-11-29T05:07:24.000Z","msg":"<blockquote><blockquote>香港做ai有咩好做?<br />\n<br />\n睇andrew ng coursera &amp; deep learning 睇左幾個月，都無咩呢d 工<br />\n<br />\n自己搞又無咩data</blockquote><br />\n香港無，過日本，好多唔洗識日文都請</blockquote><br />\n香港有,但9成都係中資background"},{"pid":"c5fbe2fffab8767cb36f20f9930b73786bec21ee","tid":483239,"uid":63491,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T05:25:44.000Z","msg":"<blockquote><blockquote><blockquote>香港做ai有咩好做?<br />\n<br />\n睇andrew ng coursera &amp; deep learning 睇左幾個月，都無咩呢d 工<br />\n<br />\n自己搞又無咩data</blockquote><br />\n香港無，過日本，好多唔洗識日文都請</blockquote><br />\n香港有,但9成都係中資background</blockquote><br />\n<br />\n有冇留意咩行業? 我去angelist / linkedin睇又唔覺喎"},{"pid":"6e9c89354426af72d182e7fb070ebf7a8ba09229","tid":483239,"uid":63491,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-11-29T05:29:54.000Z","msg":"來來去去都係forward prop, 計loss, back prop, 計dw, db, 最後update parameters<br />\n<br />\n最on9 既係hyper parameter全部都好似9撞，一開始都唔知點define no of hidden layers, layer units, learning rates, optimization algo, etc<br />\n<br />\n好似9質d data入去個model run 1 run"},{"pid":"528e0b330b03deb32d80a6aafeb822afb52effd5","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T05:37:10.000Z","msg":"<blockquote><blockquote><blockquote><blockquote>香港做ai有咩好做?<br />\n<br />\n睇andrew ng coursera &amp; deep learning 睇左幾個月，都無咩呢d 工<br />\n<br />\n自己搞又無咩data</blockquote><br />\n香港無，過日本，好多唔洗識日文都請</blockquote><br />\n香港有,但9成都係中資background</blockquote><br />\n<br />\n有冇留意咩行業? 我去angelist / linkedin睇又唔覺喎</blockquote><br />\n我做緊機械人, 其實而家medical, ecommerce都好需要人, 科學園有一間叫sensetime,算係香港最大既AI公司,但係中資來<br />\n<br />\n如果你想入外國公司,去外國啦,香港本土d人你同佢講樓上堆野好多都當你痴線佬又搵唔到錢"},{"pid":"669944cf6cad3ca231ed607c47616acd5b91f4e9","tid":483239,"uid":12599,"like":3,"dislike":0,"score":3,"citedBy":0,"replyTime":"2017-11-29T05:39:01.000Z","msg":"<blockquote>來來去去都係forward prop, 計loss, back prop, 計dw, db, 最後update parameters<br />\n<br />\n最on9 既係hyper parameter全部都好似9撞，一開始都唔知點define no of hidden layers, layer units, learning rates, optimization algo, etc<br />\n<br />\n好似9質d data入去個model run 1 run</blockquote><br />\n可能根本人腦一開波initialize果陣都係鳩set d parameters<img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <br />\n<br />\n而呢d鳩set就影響一個人究竟學習能力有幾強"},{"pid":"767359ce1c0f02c60f5c4b9fe2e0b9d1de679c36","tid":483239,"uid":102178,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T05:42:10.000Z","msg":"留名"},{"pid":"a8498986d7c7dc42b91bdeba311ea677e11a99c7","tid":483239,"uid":37489,"like":0,"dislike":1,"score":-1,"citedBy":0,"replyTime":"2017-11-29T05:43:02.000Z","msg":"<blockquote><blockquote>等睇比人屌唔識中文又離地<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n已經寫到好貼地<img src=\"/assets/faces/normal/dead.gif\" class=\"hkgmoji\" /></blockquote><br />\n仲有d人覺得你講得深 跟住唔自己睇又要屌"},{"pid":"ea40790575804ee1d979ca40b5fb756be5c72283","tid":483239,"uid":43173,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T05:43:33.000Z","msg":"高汁post 留名<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"7c28e8c99ee7c2455ab635c6dcd26f97c54da1ad","tid":483239,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T05:44:48.000Z","msg":"<blockquote><blockquote>來來去去都係forward prop, 計loss, back prop, 計dw, db, 最後update parameters<br />\n<br />\n最on9 既係hyper parameter全部都好似9撞，一開始都唔知點define no of hidden layers, layer units, learning rates, optimization algo, etc<br />\n<br />\n好似9質d data入去個model run 1 run</blockquote><br />\n可能根本人腦一開波initialize果陣都係鳩set d parameters<img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <br />\n<br />\n而呢d鳩set就影響一個人究竟學習能力有幾強</blockquote><br />\n但係hinton想推翻backprop<img src=\"/assets/faces/normal/hehe.gif\" class=\"hkgmoji\" /> <br />\n<br />\n如果作為model 大部分人接受唔到neural net太black box姐"},{"pid":"a8b35004c66626f2d44f21ffdd7f64921475e67b","tid":483239,"uid":21699,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T05:53:43.000Z","msg":"<blockquote><blockquote>來來去去都係forward prop, 計loss, back prop, 計dw, db, 最後update parameters<br />\n<br />\n最on9 既係hyper parameter全部都好似9撞，一開始都唔知點define no of hidden layers, layer units, learning rates, optimization algo, etc<br />\n<br />\n好似9質d data入去個model run 1 run</blockquote><br />\n可能根本人腦一開波initialize果陣都係鳩set d parameters<img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <br />\n<br />\n而呢d鳩set就影響一個人究竟學習能力有幾強</blockquote><br />\n<img src=\"/assets/faces/normal/@.gif\" class=\"hkgmoji\" />"},{"pid":"436c0cc49fd2415bdb6b3a5e7a0f8d630b726500","tid":483239,"uid":121087,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T05:56:50.000Z","msg":"睇哂 但最後有d 唔明 有無例子?<br />\n<br />\n例如, 我做一個決定有好多唔同既parameters <br />\n<br />\n當我lunch買飯食 影響既因素有價錢,好唔好食,距離等等<br />\n<br />\n而呢d 因素可以歸納做理性 or 感性<br />\n<br />\nAI 有咩方法去learn 一個感性既parameters 呢??<br />\n<br />\n捉棋就比較容易判斷一個決定係岩定錯 因為係純粹理性既判斷 最後輸嬴黎做reinforce就得<br />\n<br />\n但現實上好多決定都有感性既因素 結果亦唔係輸同嬴既區別 <br />\n<br />\n咁d AI 可以點learn 到?"},{"pid":"5015adfe43cfce7fbd1a2bdf238e2126d552a7ae","tid":483239,"uid":12599,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-11-29T05:59:49.000Z","msg":"<blockquote><blockquote><blockquote>來來去去都係forward prop, 計loss, back prop, 計dw, db, 最後update parameters<br />\n<br />\n最on9 既係hyper parameter全部都好似9撞，一開始都唔知點define no of hidden layers, layer units, learning rates, optimization algo, etc<br />\n<br />\n好似9質d data入去個model run 1 run</blockquote><br />\n可能根本人腦一開波initialize果陣都係鳩set d parameters<img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <br />\n<br />\n而呢d鳩set就影響一個人究竟學習能力有幾強</blockquote><br />\n<img src=\"/assets/faces/normal/@.gif\" class=\"hkgmoji\" /></blockquote><br />\n我唔係讀neuroscience所以真係唔多識<br />\n<br />\n就算有呢堆cells佢地一開波個初期值點究竟係由咩機制去控制<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"8ddae6a137ab8bc5c1eada8243d0b08fba6f8a5b","tid":483239,"uid":99978,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T06:00:57.000Z","msg":"如果ai可以取代人類，咁人類有咩存在的意義？"},{"pid":"5b73a27786284eab073c651fa6d3b461f17f7899","tid":483239,"uid":12599,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-11-29T06:05:06.000Z","msg":"<blockquote>睇哂 但最後有d 唔明 有無例子?<br />\n<br />\n例如, 我做一個決定有好多唔同既parameters <br />\n<br />\n當我lunch買飯食 影響既因素有價錢,好唔好食,距離等等<br />\n<br />\n而呢d 因素可以歸納做理性 or 感性<br />\n<br />\nAI 有咩方法去learn 一個感性既parameters 呢??<br />\n<br />\n捉棋就比較容易判斷一個決定係岩定錯 因為係純粹理性既判斷 最後輸嬴黎做reinforce就得<br />\n<br />\n但現實上好多決定都有感性既因素 結果亦唔係輸同嬴既區別 <br />\n<br />\n咁d AI 可以點learn 到?</blockquote><br />\n感性因素而家日本隻pepper有個情感機制<br />\n<br />\n人類情感有一part係由大腦既hormone chemicals控制,最簡單例子係多巴胺,你個腦多左呢d物質會令你覺得開心,仲有幾十隻唔同的chemcial去做一個平衡<br />\n<br />\nmachine learning就係將呢d chemical composition量化左,個output就係人類既情感,pepper對眼會sense其他人既面部表情去知道當時佢既digital hormone level應該有咩情感<br />\n<br />\n暫時情感AI呢樣野仲係起步階段,可能好快會有更新既算法出來"},{"pid":"60096f6c3af4cb7e177aabda4a8cff7fa7efe41e","tid":483239,"uid":12599,"like":5,"dislike":0,"score":5,"citedBy":0,"replyTime":"2017-11-29T06:08:02.000Z","msg":"<blockquote><blockquote>睇哂 但最後有d 唔明 有無例子?<br />\n<br />\n例如, 我做一個決定有好多唔同既parameters <br />\n<br />\n當我lunch買飯食 影響既因素有價錢,好唔好食,距離等等<br />\n<br />\n而呢d 因素可以歸納做理性 or 感性<br />\n<br />\nAI 有咩方法去learn 一個感性既parameters 呢??<br />\n<br />\n捉棋就比較容易判斷一個決定係岩定錯 因為係純粹理性既判斷 最後輸嬴黎做reinforce就得<br />\n<br />\n但現實上好多決定都有感性既因素 結果亦唔係輸同嬴既區別 <br />\n<br />\n咁d AI 可以點learn 到?</blockquote><br />\n感性因素而家日本隻pepper有個情感機制<br />\n<br />\n人類情感有一part係由大腦既hormone chemicals控制,最簡單例子係多巴胺,你個腦多左呢d物質會令你覺得開心,仲有幾十隻唔同的chemcial去做一個平衡<br />\n<br />\nmachine learning就係將呢d chemical composition量化左,個output就係人類既情感,pepper對眼會sense其他人既面部表情去知道當時佢既digital hormone level應該有咩情感<br />\n<br />\n暫時情感AI呢樣野仲係起步階段,可能好快會有更新既算法出來</blockquote><br />\n之前RTHK有條片拍pepper同人捉棋,pepper捉輸左,佢之前既algo preset左輸係會感到不快樂,但係佢見到d人就算輸左都好開心,佢就理解為雖然捉棋結果係輸左,但係捉棋既過程先係愉悅既所在點,之後佢就學識左enjoy捉棋呢個活動<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"3428aa74943bc2a1eae60c4cf845a9817c0595b8","tid":483239,"uid":48679,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-11-29T06:11:41.000Z","msg":"<blockquote>睇哂 但最後有d 唔明 有無例子?<br />\n<br />\n例如, 我做一個決定有好多唔同既parameters <br />\n<br />\n當我lunch買飯食 影響既因素有價錢,好唔好食,距離等等<br />\n<br />\n而呢d 因素可以歸納做理性 or 感性<br />\n<br />\nAI 有咩方法去learn 一個感性既parameters 呢??<br />\n<br />\n捉棋就比較容易判斷一個決定係岩定錯 因為係純粹理性既判斷 最後輸嬴黎做reinforce就得<br />\n<br />\n但現實上好多決定都有感性既因素 結果亦唔係輸同嬴既區別 <br />\n<br />\n咁d AI 可以點learn 到?</blockquote><br />\nanything can be, as long as you can quantify it"},{"pid":"52f65e488db5e3622de8dd3b5e518138bf47bfc8","tid":483239,"uid":21699,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T06:12:15.000Z","msg":"<blockquote><blockquote><blockquote><blockquote>來來去去都係forward prop, 計loss, back prop, 計dw, db, 最後update parameters<br />\n<br />\n最on9 既係hyper parameter全部都好似9撞，一開始都唔知點define no of hidden layers, layer units, learning rates, optimization algo, etc<br />\n<br />\n好似9質d data入去個model run 1 run</blockquote><br />\n可能根本人腦一開波initialize果陣都係鳩set d parameters<img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <br />\n<br />\n而呢d鳩set就影響一個人究竟學習能力有幾強</blockquote><br />\n<img src=\"/assets/faces/normal/wink.gif\" class=\"hkgmoji\" /></blockquote><br />\n我唔係讀neuroscience所以真係唔多識<br />\n<br />\n就算有呢堆cells佢地一開波個初期值點究竟係由咩機制去控制<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n<img src=\"/assets/faces/normal/shocking.gif\" class=\"hkgmoji\" />"},{"pid":"ad63ebba5b8a30add1e0e99cad5b8b1dbd0c4910","tid":483239,"uid":152451,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T06:16:24.000Z","msg":"<blockquote>咁岩我畢業論文寫 論機械人的道德考量<br />\n巴打有冇reference?<img src=\"/assets/faces/big/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/big/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/big/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n<a href=\"https://www.media.mit.edu/projects/moral-machine/overview/\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.media.mit.edu%2Fprojects%2Fmoral-machine%2Foverview%2F&d=tjzMqdWYAiG4enN%2Fcf3LS6NGKT0mDZcTD8WJHRevFdc%3D&h=f2ee6b93\" target=\"_blank\">https://www.media.mit.edu/projects/moral-machine/overview/</a>"},{"pid":"b05f0edbf915e4386b1a73071eee8fae28f08344","tid":483239,"uid":69125,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T06:22:09.000Z","msg":"ee仔睇唔明lm"},{"pid":"947b3d2240105e183a7f673b333a2d1b5b0a905f","tid":483239,"uid":152451,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T06:23:17.000Z","msg":"<blockquote><blockquote>巴打咩background?<br />\n讀緊MSc data science有冇機做到AI related既野<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\nbachleor in physics, master唔講<br />\n<br />\n基本上咩displine都可以用到AI<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\nTop AI 工最鍾意Physics grad <img src=\"/assets/faces/normal/like.gif\" class=\"hkgmoji\" />"},{"pid":"74d71973a2fefbfa0a7ed02c081c757973b07b94","tid":483239,"uid":69125,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T06:25:20.000Z","msg":"前幾個月睇過份doctoral thesis講4隻machine learning algo,linear regression,SVM,k-nearest integer仲有隻唔記得乜叉睇唔撚明<img src=\"/assets/faces/normal/fuck.gif\" class=\"hkgmoji\" />"},{"pid":"0160351fdf8469c3fd45c250a662b802200c6614","tid":483239,"uid":12599,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-11-29T06:28:12.000Z","msg":"<span style=\"font-size: xx-large;\"><span style=\"color: red;\">ANN小例子</span></span><br />\n<br />\n介紹返一set data叫MNIST,係AI界做圖像分類既入門data,內容係28*28 pixel的手寫0-9數字,好多時d人去做benchmarking呢套data容量小而且已經well established,所以所有AI image detection framework既hello world都係由呢set data開始<br />\n<br />\n<a href=\"http://yann.lecun.com/exdb/mnist/\" data-sr-url=\"https://r.lihkg.com/link?u=http%3A%2F%2Fyann.lecun.com%2Fexdb%2Fmnist%2F&d=tjzMqdWYAiG4enN%2Fcf3LS6NGKT0mDZcTD8WJHRevFdc%3D&h=ca9258af\" target=\"_blank\">http://yann.lecun.com/exdb/mnist/</a><br />\n<br />\ntensorflow, pytorch內置左dl link同reader,唔使自己再寫reader,非常好用的data set<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <br />\n<br />\n睇下neural net (ANN), 錯誤率0.35, CNN既錯誤率可以去到0.23<img src=\"/assets/faces/normal/@.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/@.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/@.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/@.gif\" class=\"hkgmoji\" /> <br />\n<br />\n我之後會再講CNN係乜東西,呢樣野係現代圖像處理既最先進算法,係ANN既變化來<br />\n<br />\n<img src=\"http://i.imgur.com/SQTMzsC.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fi.imgur.com%2FSQTMzsC.png&h=cc395252&s={SIZE}\" /><br />\nMNIST既dataset,一堆手寫數字<br />\n<br />\n呢堆數字放大來睇,其實係個28*28的矩陣來,入面就係0-1既數字(float image),點解要float呢?其實0-255的整數值圖都ok,但係tensorflow pytorch輸入都係要32bit float point number,我都唔知點解要咁set<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> <br />\n<br />\n<img src=\"http://i.imgur.com/QoBA21J.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fi.imgur.com%2FQoBA21J.png&h=5bb1da45&s={SIZE}\" /><br />\n放大張相睇下<br />\n<br />\n<img src=\"http://i.imgur.com/rnNXzOC.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fi.imgur.com%2FrnNXzOC.png&h=4dd0ad34&s={SIZE}\" /><br />\nMNIST係z 方向做堆疊,就變左一個28*28*256咁多個數字既matrix啦<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <br />\n<br />\n呢個可以拎來做輸入層(input layer)<br />\n<br />\n<img src=\"http://i.imgur.com/8X0IbcY.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fi.imgur.com%2F8X0IbcY.png&h=66066605&s={SIZE}\" /><br />\n呢個就係output vector, 10*55000既matrix array, 第一層[0 0 0 0 1 0 0 0 0 0]代表結果係5,呢個我地叫output layer<br />\n<br />\n咁我地就可以用ANN既方法去train部電腦去學個數字形狀。如果第一個如果係[1 0 0 0 0 0 0 0 0 0], 即係話你個network入面d weight同bias值唔岩,要改<img src=\"/assets/faces/normal/angry.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/angry.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/angry.gif\" class=\"hkgmoji\" /> <br />\n<br />\n呢個過程要不停做直到我地叫佢停為止<br />\n<br />\n<img src=\"http://i.imgur.com/Zff5Y7Q.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fi.imgur.com%2FZff5Y7Q.png&h=bd1c81c6&s={SIZE}\" /><br />\n中間d weight matrix我地又叫feature map,係可以拎出來plot來睇下,紅色係負值,藍色係正值,亦即係愈似果個數字既位,就愈藍,愈覺得唔係既,就會愈紅<br />\n<br />\n有呢個小例子希望大家更為了解咩係ANN<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" />"},{"pid":"294ab5bcce604b339e4beab1b342f00b6c7d00ee","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T06:40:37.000Z","msg":"去到呢步想講下data randomization既少量技巧,其實有N咁多種,呢度講下如果大家都用ANN,點樣排d data會有效少少<br />\n<br />\n拎返MNIST做例子,點樣先係最易入曬d data去train一次呢?<br />\n<br />\n無錯就係由頭到尾順序run一次啦<img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <br />\n<br />\n最少既工夫做最多既野,咁當然result未必會好,因為前後既次序(前面多d 1後面少d 1)會影響訓練精度<br />\n<br />\n又由於電腦ram有限(CPU又好GPU又好),所以呢55000個data係無可能一次放曬入去度train,所以就要分<span style=\"color: red;\">batch</span>, 變左d 28*28*100咁樣既小batch入去個ANN度訓練<br />\n<br />\n每loop完所有data一次就叫一個<span style=\"color: red;\">episode (epoch)</span><br />\n<br />\n而每個epoch既data前後排列,我地有會隨機化左佢,呢個過程叫<span style=\"color: red;\">shuffle</span><br />\n<br />\n仲有原生data仲可以分為training同evaluation/testing兩部份<br />\n<br />\n由於你用同一組data去train,你之後放返同一set的data入去evaluate/inference,結果一定係fit得好架<br />\n<br />\nML既重點,就係由有限既data set,對未知的data進行預測, 所以我地訓練之時,要保留一部份data唔可以俾個machine見過,之後再拎d未見過的data俾去佢試一下預估準確度,呢個先係valid的準確度<br />\n<br />\n至於點分呢兩set data,內裡大有學問<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <br />\n<br />\n實際上我去train的時間就係random揀training:evaluation 8:2既分類就算<img src=\"/assets/faces/lomoji/10.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/10.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/10.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/10.png\" class=\"hkgmoji\" />"},{"pid":"cd776244657df2d041b81c921200590d114dde60","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T06:44:41.000Z","msg":"之後會開始講咩係Deep Learning<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" />"},{"pid":"ca0a0fecd27a6c3ac9a7e5cd0460e4b750d0fb3e","tid":483239,"uid":1102,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T06:46:31.000Z","msg":"strong post<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"0b1457c1983dfd8c4e5c9de3804813dbebcde2ce","tid":483239,"uid":60967,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T06:48:36.000Z","msg":"淺白易明<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"46afafdc67938685d26ad1d4989a5ddba3432426","tid":483239,"uid":31575,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T06:50:41.000Z","msg":"學到野<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <br />\nlm"},{"pid":"bfc24eab2c3ee6dfb83de33e1c0c8faf5e3b6b71","tid":483239,"uid":159659,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T06:55:59.000Z","msg":"巴打點睇functional programming? 好多prof都話functional prog 先係皇道，甚至將來真正嘅AI 都係用functional programming寫。"},{"pid":"750eb353dd5c68f75703485cc7861155ea8971fd","tid":483239,"uid":28563,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T06:57:08.000Z","msg":"就快可以講埋overfit underfit<img src=\"/assets/faces/normal/hehe.gif\" class=\"hkgmoji\" />"},{"pid":"bab384e94e29e1a0d4061195664d9751a02f5066","tid":483239,"uid":97299,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T06:57:49.000Z","msg":"<blockquote><blockquote><blockquote>睇哂 但最後有d 唔明 有無例子?<br />\n<br />\n例如, 我做一個決定有好多唔同既parameters <br />\n<br />\n當我lunch買飯食 影響既因素有價錢,好唔好食,距離等等<br />\n<br />\n而呢d 因素可以歸納做理性 or 感性<br />\n<br />\nAI 有咩方法去learn 一個感性既parameters 呢??<br />\n<br />\n捉棋就比較容易判斷一個決定係岩定錯 因為係純粹理性既判斷 最後輸嬴黎做reinforce就得<br />\n<br />\n但現實上好多決定都有感性既因素 結果亦唔係輸同嬴既區別 <br />\n<br />\n咁d AI 可以點learn 到?</blockquote><br />\n感性因素而家日本隻pepper有個情感機制<br />\n<br />\n人類情感有一part係由大腦既hormone chemicals控制,最簡單例子係多巴胺,你個腦多左呢d物質會令你覺得開心,仲有幾十隻唔同的chemcial去做一個平衡<br />\n<br />\nmachine learning就係將呢d chemical composition量化左,個output就係人類既情感,pepper對眼會sense其他人既面部表情去知道當時佢既digital hormone level應該有咩情感<br />\n<br />\n暫時情感AI呢樣野仲係起步階段,可能好快會有更新既算法出來</blockquote><br />\n之前RTHK有條片拍pepper同人捉棋,pepper捉輸左,佢之前既algo preset左輸係會感到不快樂,但係佢見到d人就算輸左都好開心,佢就理解為雖然捉棋結果係輸左,但係捉棋既過程先係愉悅既所在點,之後佢就學識左enjoy捉棋呢個活動<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n<img src=\"/assets/faces/normal/@.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"52c2b851260c282e900ea791e2209bef0e55b3a5","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T07:03:01.000Z","msg":"<blockquote>巴打點睇functional programming? 好多prof都話functional prog 先係皇道，甚至將來真正嘅AI 都係用functional programming寫。</blockquote><br />\nfunctional programming係coding style來,同算法無關<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n咁high level的language行起上來其實好食computational resources"},{"pid":"fff27aa75e608a156322e0a395d72ebc32c488ab","tid":483239,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T07:05:26.000Z","msg":"<blockquote><blockquote>巴打點睇functional programming? 好多prof都話functional prog 先係皇道，甚至將來真正嘅AI 都係用functional programming寫。</blockquote><br />\nfunctional programming係coding style來,同算法無關<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n咁high level的language行起上來其實好食computational resources</blockquote><br />\n我個fd話其實functional都係勝在你腦裡面已經'compile'左某d code<br />\n<br />\n香港寫functional都曬氣 冇寫開難d睇得明"},{"pid":"ff34d88f56bc74481d41f033db3c49bef27d1cc2","tid":483239,"uid":46197,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T07:15:55.000Z","msg":"<blockquote><blockquote>來來去去都係forward prop, 計loss, back prop, 計dw, db, 最後update parameters<br />\n<br />\n最on9 既係hyper parameter全部都好似9撞，一開始都唔知點define no of hidden layers, layer units, learning rates, optimization algo, etc<br />\n<br />\n好似9質d data入去個model run 1 run</blockquote><br />\n可能根本人腦一開波initialize果陣都係鳩set d parameters<img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <br />\n<br />\n而呢d鳩set就影響一個人究竟學習能力有幾強</blockquote><br />\n<br />\n呢啲鳩set 叫gene"},{"pid":"03a0fdff5dc42e7fa2908708680f889e7fcbf6f7","tid":483239,"uid":28734,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-11-29T07:18:42.000Z","msg":"<blockquote>來來去去都係forward prop, 計loss, back prop, 計dw, db, 最後update parameters<br />\n<br />\n最on9 既係hyper parameter全部都好似9撞，一開始都唔知點define no of hidden layers, layer units, learning rates, optimization algo, etc<br />\n<br />\n好似9質d data入去個model run 1 run</blockquote><br />\n有同感<br />\nLayers點排<br />\n入面既parameters 點set<br />\n全部都冇咩既定既做法<br />\n只係可以參考人地既model然後大概跟<br />\n其他完全鳩撞睇下邊個model accuracy 最高<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"37fdc89d8b549a00d337952836dc2433919f8875","tid":483239,"uid":30478,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T08:20:24.000Z","msg":"請問想學AI應該由咩學起？<br />\n非IT底，岩岩先開始自學python <br />\n想學programming-&gt;data management/big data -&gt; AI<br />\n唔係真係要做到咁專業，始終唔係IT底<br />\n但係想自己可以明白到點運作，唔洗第時搵vendor做既時候完全唔明人講乜<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"415beb09048c0b40a1f8d9db3d39a980fab62cc7","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T08:22:04.000Z","msg":"<blockquote>請問想學AI應該由咩學起？<br />\n非IT底，岩岩先開始自學python <br />\n想學programming-&gt;data management/big data -&gt; AI<br />\n唔係真係要做到咁專業，始終唔係IT底<br />\n但係想自己可以明白到點運作，唔洗第時搵vendor做既時候完全唔明人講乜<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n學AI/打曲都係由應用開始學起<br />\n<br />\npure學theory一開波打d 底層野係coding界係好無聊既一件事<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"47e18240e75c85eb033071336a0ac0ddbe8cd8c7","tid":483239,"uid":59922,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T08:45:22.000Z","msg":"IT狗留名"},{"pid":"31fe83762a1a3a574f3e995e0d3024fd37372551","tid":483239,"uid":70989,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T08:56:04.000Z","msg":"留名，欣賞巴打<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"b896efbdedeac66ce74eca238bfb411bc9bc38c6","tid":483239,"uid":42260,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T08:56:07.000Z","msg":"lm好過睇joe哥9up"},{"pid":"febf2c099d6d510e74f1a79fbf6447416b4781a0","tid":483239,"uid":12599,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-11-29T09:04:25.000Z","msg":"<blockquote>lm好過睇joe哥9up</blockquote><br />\n無論連登定高登,叫得joe既都<img src=\"/assets/faces/lomoji/10.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/10.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/10.png\" class=\"hkgmoji\" />"},{"pid":"1ed79d5e1eb6d60dd15c3957684b4b88a9d52edb","tid":483239,"uid":139500,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T09:32:21.000Z","msg":"新手學緊machine learning<br />\n<br />\n想問下其實去做regression之前係咪應該要識幾款唔同既algorithm，之後visualise 完個data，做完EDA先決定用邊款algorithm<br />\n<br />\n而唔係用死某一隻「最勁」既algorithm，唔理個data係咩都照用死佢？"},{"pid":"2c46904694a068797d9f1e722cdb8a372553450d","tid":483239,"uid":74644,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T09:32:57.000Z","msg":"留名"},{"pid":"0b854958cf5b06b2653e37ee53dd6ede98f31a2d","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T09:38:47.000Z","msg":"<blockquote>新手學緊machine learning<br />\n<br />\n想問下其實去做regression之前係咪應該要識幾款唔同既algorithm，之後visualise 完個data，做完EDA先決定用邊款algorithm<br />\n<br />\n而唔係用死某一隻「最勁」既algorithm，唔理個data係咩都照用死佢？</blockquote><br />\n識幾隻大路野, kmean, svm, cnn, 再睇下咩場口咩應用<br />\n<br />\n唔通d data少你仲要走走train咩, ANN雖勁但係最大弱點係好睇data質量"},{"pid":"ce8f76cd2fe6dfaab06a46b0931eff11e500af8e","tid":483239,"uid":131589,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T09:43:43.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote>香港做ai有咩好做?<br />\n<br />\n睇andrew ng coursera &amp; deep learning 睇左幾個月，都無咩呢d 工<br />\n<br />\n自己搞又無咩data</blockquote><br />\n香港無，過日本，好多唔洗識日文都請</blockquote><br />\n香港有,但9成都係中資background</blockquote><br />\n<br />\n有冇留意咩行業? 我去angelist / linkedin睇又唔覺喎</blockquote><br />\n我做緊機械人, 其實而家medical, ecommerce都好需要人, 科學園有一間叫sensetime,算係香港最大既AI公司,但係中資來<br />\n<br />\n如果你想入外國公司,去外國啦,香港本土d人你同佢講樓上堆野好多都當你痴線佬又搵唔到錢</blockquote><br />\n<br />\n<br />\n高興搵到同路人。。<img src=\"/assets/faces/normal/smile.gif\" class=\"hkgmoji\" />"},{"pid":"6ee18fa8aade8ece1aab4b515e6b24761e707dfb","tid":483239,"uid":30478,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T10:39:31.000Z","msg":"<blockquote><blockquote>請問想學AI應該由咩學起？<br />\n非IT底，岩岩先開始自學python <br />\n想學programming-&gt;data management/big data -&gt; AI<br />\n唔係真係要做到咁專業，始終唔係IT底<br />\n但係想自己可以明白到點運作，唔洗第時搵vendor做既時候完全唔明人講乜<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n學AI/打曲都係由應用開始學起<br />\n<br />\npure學theory一開波打d 底層野係coding界係好無聊既一件事<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n應用即係點？搵d野出黎自己拆開佢睇下d code係點？"},{"pid":"e063304507bdb070c22188a14a77ba878de723e5","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T10:48:39.000Z","msg":"<blockquote><blockquote><blockquote>請問想學AI應該由咩學起？<br />\n非IT底，岩岩先開始自學python <br />\n想學programming-&gt;data management/big data -&gt; AI<br />\n唔係真係要做到咁專業，始終唔係IT底<br />\n但係想自己可以明白到點運作，唔洗第時搵vendor做既時候完全唔明人講乜<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n學AI/打曲都係由應用開始學起<br />\n<br />\npure學theory一開波打d 底層野係coding界係好無聊既一件事<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n應用即係點？搵d野出黎自己拆開佢睇下d code係點？</blockquote><br />\n你有實際應用先,大概了解下幾類大野既machine learning原理,再睇下自己手上有咩data, 再學一兩隻algo, dl人地寫好左d library/入門教學試玩下"},{"pid":"4ddbaaf29e7638617b944400a549de09a8596bed","tid":483239,"uid":100145,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-11-29T12:29:27.000Z","msg":"啱啱學校project先用完regression 同reinforcement learning做stock prices prediction <img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" />  (勿相認）<br />\n<br />\nBetween 強Post lm"},{"pid":"313e066325d58c746bcf09b10394748c03a911f5","tid":483239,"uid":63491,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T12:31:51.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>香港做ai有咩好做?<br />\n<br />\n睇andrew ng coursera &amp; deep learning 睇左幾個月，都無咩呢d 工<br />\n<br />\n自己搞又無咩data</blockquote><br />\n香港無，過日本，好多唔洗識日文都請</blockquote><br />\n香港有,但9成都係中資background</blockquote><br />\n<br />\n有冇留意咩行業? 我去angelist / linkedin睇又唔覺喎</blockquote><br />\n我做緊機械人, 其實而家medical, ecommerce都好需要人, 科學園有一間。叫sensetime,算係香港最大既AI公司,但係中資來<br />\n<br />\n如果你想入外國公司,去外國啦,香港本土d人你同佢講樓上堆野好多都當你痴線佬又搵唔到錢</blockquote><br />\n<br />\n<br />\n高興搵到同路人。。<img src=\"/assets/faces/normal/smile.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n我學左半年左右，都唔知有咩好做，香港又無咩人請，自己搞又未諗到有咩idea，幫香港客做就算9數把啦"},{"pid":"414de963959800377bd1f79a2dcc39e90545f224","tid":483239,"uid":12599,"like":1,"dislike":0,"score":1,"citedBy":1,"replyTime":"2017-11-29T12:36:34.000Z","msg":"<blockquote>啱啱學校project先用完regression 同reinforcement learning做stock prices prediction <img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" />  (勿相認）<br />\n<br />\nBetween 強Post lm</blockquote><br />\n我之後會講RNN同LSTM"},{"pid":"41d0bb6ca30b274cb7367938ff88d8f9a48f39cc","tid":483239,"uid":156615,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T12:39:25.000Z","msg":"lm"},{"pid":"35020c5bc0081732c1ada99c6e3f23779160abbd","tid":483239,"uid":149978,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T12:47:13.000Z","msg":"<blockquote>香港做ai有咩好做?<br />\n<br />\n睇andrew ng coursera &amp; deep learning 睇左幾個月，都無咩呢d 工<br />\n<br />\n自己搞又無咩data</blockquote><br />\n<br />\n香港係 financial centre<br />\n逢親同金融有關的東西，都應該有得撈<br />\n應該有工 develop ai for 預測金融行情<br />\n不過多數都比左外國或內地做<br />\n<br />\n大學有位同學，FYP就係寫program預測股票，佢後來自己玩，用幾萬蚊本，贏左幾百萬。<br />\n<br />\n另外MIT有人寫左個 AI 預測 bitcoin價格。<br />\n個algorithm係1秒預測未來十秒的價格，所以係玩高頻交易。"},{"pid":"b900292adad97a5ad4e999c035207c4b54adc5ce","tid":483239,"uid":63491,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T12:47:19.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote>香港做ai有咩好做?<br />\n<br />\n睇andrew ng coursera &amp; deep learning 睇左幾個月，都無咩呢d 工<br />\n<br />\n自己搞又無咩data</blockquote><br />\n香港無，過日本，好多唔洗識日文都請</blockquote><br />\n香港有,但9成都係中資background</blockquote><br />\n<br />\n有冇留意咩行業? 我去angelist / linkedin睇又唔覺喎</blockquote><br />\n我做緊機械人, 其實而家medical, ecommerce都好需要人, 科學園有一間叫sensetime,算係香港最大既AI公司,但係中資來<br />\n<br />\n如果你想入外國公司,去外國啦,香港本土d人你同佢講樓上堆野好多都當你痴線佬又搵唔到錢</blockquote><br />\n<br />\n想走都要走到先得架，美國要visa, 大陸又要接到地氣，唯一出路其實係自己造有用既攞去賣，build product仲實際<br />\n<br />\n我自己目標係想做saas收月費儲客，但係呢排又無咩idea，之前就做過唔同範疇"},{"pid":"ed01beb190aaf8b8ba2d90d50ad7a98a75d20285","tid":483239,"uid":63491,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T12:49:43.000Z","msg":"<blockquote>去到呢步想講下data randomization既少量技巧,其實有N咁多種,呢度講下如果大家都用ANN,點樣排d data會有效少少<br />\n<br />\n拎返MNIST做例子,點樣先係最易入曬d data去train一次呢?<br />\n<br />\n無錯就係由頭到尾順序run一次啦<img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/06.png\" class=\"hkgmoji\" /> <br />\n<br />\n最少既工夫做最多既野,咁當然result未必會好,因為前後既次序(前面多d 1後面少d 1)會影響訓練精度<br />\n<br />\n又由於電腦ram有限(CPU又好GPU又好),所以呢55000個data係無可能一次放曬入去度train,所以就要分<span style=\"color: red;\">batch</span>, 變左d 28*28*100咁樣既小batch入去個ANN度訓練<br />\n<br />\n每loop完所有data一次就叫一個<span style=\"color: red;\">episode (epoch)</span><br />\n<br />\n而每個epoch既data前後排列,我地有會隨機化左佢,呢個過程叫<span style=\"color: red;\">shuffle</span><br />\n<br />\n仲有原生data仲可以分為training同evaluation/testing兩部份<br />\n<br />\n由於你用同一組data去train,你之後放返同一set的data入去evaluate/inference,結果一定係fit得好架<br />\n<br />\nML既重點,就係由有限既data set,對未知的data進行預測, 所以我地訓練之時,要保留一部份data唔可以俾個machine見過,之後再拎d未見過的data俾去佢試一下預估準確度,呢個先係valid的準確度<br />\n<br />\n至於點分呢兩set data,內裡大有學問<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <br />\n<br />\n實際上我去train的時間就係random揀training:evaluation 8:2既分類就算<img src=\"/assets/faces/lomoji/10.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/10.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/10.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/10.png\" class=\"hkgmoji\" /></blockquote><br />\n<br />\n實際操作係分開training, dev, test set 三部分?"},{"pid":"2785f7a4513111f7e30f833015971f47a3e8c5b0","tid":483239,"uid":149978,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T12:51:07.000Z","msg":"<blockquote><blockquote>香港做ai有咩好做?<br />\n<br />\n睇andrew ng coursera &amp; deep learning 睇左幾個月，都無咩呢d 工<br />\n<br />\n自己搞又無咩data</blockquote><br />\n<br />\n香港係 financial centre<br />\n逢親同金融有關的東西，都應該有得撈<br />\n應該有工 develop ai for 預測金融行情<br />\n不過多數都比左外國或內地做<br />\n<br />\n大學有位同學，FYP就係寫program預測股票，佢後來自己玩，用幾萬蚊本，贏左幾百萬。<br />\n<br />\n另外MIT有人寫左個 AI 預測 bitcoin價格。<br />\n個algorithm係1秒預測未來十秒的價格，所以係玩高頻交易。</blockquote><br />\n<br />\n仲有d ai係搞customer relationship management<br />\n不過香港應該應用多，開發少"},{"pid":"4b401460e5527fe232ebfdca74649c0b81c3a64e","tid":483239,"uid":131589,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T12:59:22.000Z","msg":"<blockquote><blockquote>香港做ai有咩好做?<br />\n<br />\n睇andrew ng coursera &amp; deep learning 睇左幾個月，都無咩呢d 工<br />\n<br />\n自己搞又無咩data</blockquote><br />\n<br />\n香港係 financial centre<br />\n逢親同金融有關的東西，都應該有得撈<br />\n應該有工 develop ai for 預測金融行情<br />\n不過多數都比左外國或內地做<br />\n<br />\n大學有位同學，FYP就係寫program預測股票，佢後來自己玩，用幾萬蚊本，贏左幾百萬。<br />\n<br />\n另外MIT有人寫左個 AI 預測 bitcoin價格。<br />\n個algorithm係1秒預測未來十秒的價格，所以係玩高頻交易。</blockquote><br />\n<br />\n<br />\n幾萬去到幾百萬要幾耐？"},{"pid":"09409c70b99f233d74f0386715aacc7402341e2d","tid":483239,"uid":149978,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T13:03:14.000Z","msg":"<blockquote><blockquote><blockquote>香港做ai有咩好做?<br />\n<br />\n睇andrew ng coursera &amp; deep learning 睇左幾個月，都無咩呢d 工<br />\n<br />\n自己搞又無咩data</blockquote><br />\n<br />\n香港係 financial centre<br />\n逢親同金融有關的東西，都應該有得撈<br />\n應該有工 develop ai for 預測金融行情<br />\n不過多數都比左外國或內地做<br />\n<br />\n大學有位同學，FYP就係寫program預測股票，佢後來自己玩，用幾萬蚊本，贏左幾百萬。<br />\n<br />\n另外MIT有人寫左個 AI 預測 bitcoin價格。<br />\n個algorithm係1秒預測未來十秒的價格，所以係玩高頻交易。</blockquote><br />\n<br />\n<br />\n幾萬去到幾百萬要幾耐？</blockquote><br />\n<br />\n用左幾年，佢玩洐生工具先咁快<br />\n當時200x年，佢主要投資中資股"},{"pid":"68e517a937acd789579077af027ad8ac351521c1","tid":483239,"uid":131589,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T13:10:22.000Z","msg":"<blockquote><blockquote><blockquote><blockquote>香港做ai有咩好做?<br />\n<br />\n睇andrew ng coursera &amp; deep learning 睇左幾個月，都無咩呢d 工<br />\n<br />\n自己搞又無咩data</blockquote><br />\n<br />\n香港係 financial centre<br />\n逢親同金融有關的東西，都應該有得撈<br />\n應該有工 develop ai for 預測金融行情<br />\n不過多數都比左外國或內地做<br />\n<br />\n大學有位同學，FYP就係寫program預測股票，佢後來自己玩，用幾萬蚊本，贏左幾百萬。<br />\n<br />\n另外MIT有人寫左個 AI 預測 bitcoin價格。<br />\n個algorithm係1秒預測未來十秒的價格，所以係玩高頻交易。</blockquote><br />\n<br />\n<br />\n幾萬去到幾百萬要幾耐？</blockquote><br />\n<br />\n用左幾年，佢玩洐生工具先咁快<br />\n當時200x年，佢主要投資中資股</blockquote><br />\n高手. <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"11f22f4a3d5489b9d4f7f9ae51a5dbe130d99cd1","tid":483239,"uid":65859,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-11-29T14:06:47.000Z","msg":"用 relu 係因為可以有效解決vanishing gradient 既問題<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" />"},{"pid":"870f1e8b8a11b29f51477f6e0c84fc14acdb78f4","tid":483239,"uid":94813,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T14:12:37.000Z","msg":"强帖黎明"},{"pid":"498d6261bbf6e5214ffa72e0aa09608e65e05204","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T14:17:53.000Z","msg":"<blockquote>用 relu 係因為可以有效解決vanishing gradient 既問題<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /></blockquote><br />\n啱啱睇到vanshing gradient <br />\n<br />\nCNN無乜點講，RNN先有提到，但都係無講明ReLU係其中一個解決方法，淨係叫加residual同用LSTM<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" />"},{"pid":"1594559eb344a5cf349dfb12beeaaa9e8bf85165","tid":483239,"uid":108337,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T14:19:59.000Z","msg":"玩過下tensorflow, 留名學野"},{"pid":"edeab293f655495b612a5056f6cb93b94484028e","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T14:27:07.000Z","msg":"<blockquote><blockquote>用 relu 係因為可以有效解決vanishing gradient 既問題<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /></blockquote><br />\n啱啱睇到vanshing gradient <br />\n<br />\nCNN無乜點講，RNN先有提到，但都係無講明ReLU係其中一個解決方法，淨係叫加residual同用LSTM<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\nReLU 雖然可以減少vanish gardient 問題，係network一加大都係唔work, 2015年 propose嘅ResNet先有效增加network深度<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" />"},{"pid":"7b304bf42a50d4c7e4fdc8a8fcaf9739e72fd2cd","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T15:37:30.000Z","msg":"<span style=\"font-size: xx-large;\"><span style=\"color: red;\">深度人工網路</span></span><br />\n大家可能已經聽過deep learning深度學習呢隻字,其實呢樣野好簡單,就係多過一層的ANN架構,已經可以叫做深度人工網路<br />\n<br />\n<img src=\"https://i.stack.imgur.com/OH3gI.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fi.stack.imgur.com%2FOH3gI.png&h=3aacc6bc&s={SIZE}\" /><br />\n<br />\n好多人會覺得,如果你層數愈多,咁個network咪愈準?<br />\n<br />\n世上無免費的午餐,雖然大體上概念真係愈多層,預測真係會愈準,但係實際執行上,會有好多既問題出現<br />\n<br />\n單睇一層既ANN架構,如果input係m咁多個,有k咁多個人工神經元,輸出返n咁多個output,咁總連結數就係m*n*k咁多個組合,係咁既情況之下,就算ANN幾十年前已經被design出來,就算去到今日既大型電腦,都唔係好計算到咁大既量級data<br />\n<br />\n樓上呢個連接方式,又叫全連接(Fully Connect), 係現代既DL架構入面,一般只係classifier最後一層先會用。由於呢個過程相等於vector martix既inner product,所以又有人叫inner product layer。成個DL既架構又有人會叫<span style=\"color: red;\">Fully Connected Neural Network</span> (FNN)<br />\n<br />\n有另一隻neural network又係叫FNN, 全名係Feedforward Neural Network, 但係Convolution Neural Network都係後者之一, 所以FNN呢隻字都較為少見,又或者有人叫FC-ANN, fully connected artificial neural network,不過都只係名來的<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> <br />\n<br />\n呢種最簡單既ANN有個好大的問題,就係input layer既大小唔可以改變,咁樣對於唔同大小既影像/矩陣輸入就會好有侷限性<br />\n<br />\n<span style=\"font-size: x-large;\"><span style=\"color: green;\">網路深度增加時的問題</span></span><br />\n傳統ANN最大既問題,就係深度增加時,除左訓練時間會增加之外,仲有一個好大的問題就係梯度消失問題(Vanishing Gradient Problem)同Overfitting問題<br />\n<br />\n<span style=\"font-size: large;\"><span style=\"color: blue;\">梯度消失</span></span><br />\n<img src=\"https://kratzert.github.io/images/bn_backpass/chainrule_example.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fkratzert.github.io%2Fimages%2Fbn_backpass%2Fchainrule_example.PNG&h=e01c754f&s={SIZE}\" /><br />\n幾個chapter之前講過,ANN既訓練係靠SGD等反向傳遞去做optimization,而SGD既計算實際上係每一個neuron output得出誤差值後,用條activation function同hidden layer的微分值去得出修正值<br />\n<br />\n淺層既ANN還好, SGD後去返input layer個修正數字仲有返咁上下大細,但深度達到N層既ANN架構,做反向傳遞就要微分N次咁多,即係話,有可能去到一半果陣個gradient 就會變曬0,成個network根本就訓練唔到<br />\n<br />\nvanshing gradient可以用ReLU呢條activation function去改善,因為呢條function個微分值會大過sigmoid function係同一位置既微分值,不過深度增加後仍然有同樣問題出現<br />\n<br />\n<span style=\"font-size: large;\"><span style=\"color: blue;\">過擬合</span></span><br />\nOverfitting問題係指我地拎data訓練時,由於過度既訓練,導致訓練結果有偏差<br />\n<br />\n<img src=\"https://i.stack.imgur.com/t0zit.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fi.stack.imgur.com%2Ft0zit.png&h=af535654&s={SIZE}\" /><br />\n<br />\n利用過分深層既ANN做訓練,如果相應既training set 唔夠大的話,你好容易就會fit過龍<br />\n<br />\n簡單的例子:<br />\n如果你要電腦學一張紙既特徵,但你得10張相,你又係咁叫部電腦自己係呢10張相中搵出&quot;紙&quot;既特徵,佢好有可能連張紙旁邊既筆呀間尺呀人呀都學埋入去個Network度<br />\n<br />\n而點樣可以避免overfitting問題,之後講residual block果陣會講下新既NN有咩方式處理"},{"pid":"5b59fd64953c89b4a24d5f25b5fa4932a099ca05","tid":483239,"uid":5150,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T16:01:44.000Z","msg":"<blockquote><blockquote><blockquote>睇哂 但最後有d 唔明 有無例子?<br />\n<br />\n例如, 我做一個決定有好多唔同既parameters <br />\n<br />\n當我lunch買飯食 影響既因素有價錢,好唔好食,距離等等<br />\n<br />\n而呢d 因素可以歸納做理性 or 感性<br />\n<br />\nAI 有咩方法去learn 一個感性既parameters 呢??<br />\n<br />\n捉棋就比較容易判斷一個決定係岩定錯 因為係純粹理性既判斷 最後輸嬴黎做reinforce就得<br />\n<br />\n但現實上好多決定都有感性既因素 結果亦唔係輸同嬴既區別 <br />\n<br />\n咁d AI 可以點learn 到?</blockquote><br />\n感性因素而家日本隻pepper有個情感機制<br />\n<br />\n人類情感有一part係由大腦既hormone chemicals控制,最簡單例子係多巴胺,你個腦多左呢d物質會令你覺得開心,仲有幾十隻唔同的chemcial去做一個平衡<br />\n<br />\nmachine learning就係將呢d chemical composition量化左,個output就係人類既情感,pepper對眼會sense其他人既面部表情去知道當時佢既digital hormone level應該有咩情感<br />\n<br />\n暫時情感AI呢樣野仲係起步階段,可能好快會有更新既算法出來</blockquote><br />\n之前RTHK有條片拍pepper同人捉棋,pepper捉輸左,佢之前既algo preset左輸係會感到不快樂,但係佢見到d人就算輸左都好開心,佢就理解為雖然捉棋結果係輸左,但係捉棋既過程先係愉悅既所在點,之後佢就學識左enjoy捉棋呢個活動<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n但點解要enjoy捉棋嘅過程?<br />\nen唔enjoy捉棋過程唔係咁樣量化架下話?<br />\n(雖然唔enjoy捉棋嘅通常都唔會走去捉棋at the first place)"},{"pid":"9609f816b43dad004872e6a72be47cc363594607","tid":483239,"uid":63491,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T16:06:29.000Z","msg":"<blockquote><span style=\"font-size: xx-large;\"><span style=\"color: red;\">深度人工網路</span></span><br />\n大家可能已經聽過deep learning深度學習呢隻字,其實呢樣野好簡單,就係多過一層的ANN架構,已經可以叫做深度人工網路<br />\n<br />\n<img src=\"https://i.stack.imgur.com/OH3gI.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fi.stack.imgur.com%2FOH3gI.png&h=3aacc6bc&s={SIZE}\" /><br />\n<br />\n好多人會覺得,如果你層數愈多,咁個network咪愈準?<br />\n<br />\n世上無免費的午餐,雖然大體上概念真係愈多層,預測真係會愈準,但係實際執行上,會有好多既問題出現<br />\n<br />\n單睇一層既ANN架構,如果input係m咁多個,有k咁多個人工神經元,輸出返n咁多個output,咁總連結數就係m*n*k咁多個組合,係咁既情況之下,就算ANN幾十年前已經被design出來,就算去到今日既大型電腦,都唔係好計算到咁大既量級data<br />\n<br />\n樓上呢個連接方式,又叫全連接(Fully Connect), 係現代既DL架構入面,一般只係classifier最後一層先會用。由於呢個過程相等於vector martix既inner product,所以又有人叫inner product layer。成個DL既架構又有人會叫<span style=\"color: red;\">Fully Connected Neural Network</span> (FNN)<br />\n<br />\n有另一隻neural network又係叫FNN, 全名係Feedforward Neural Network, 但係Convolution Neural Network都係後者之一, 所以FNN呢隻字都較為少見,又或者有人叫FC-ANN, fully connected artificial neural network,不過都只係名來的<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> <br />\n<br />\n呢種最簡單既ANN有個好大的問題,就係input layer既大小唔可以改變,咁樣對於唔同大小既影像/矩陣輸入就會好有侷限性<br />\n<br />\n<span style=\"font-size: x-large;\"><span style=\"color: green;\">網路深度增加時的問題</span></span><br />\n傳統ANN最大既問題,就係深度增加時,除左訓練時間會增加之外,仲有一個好大的問題就係梯度消失問題(Vanishing Gradient Problem)同Overfitting問題<br />\n<br />\n<span style=\"font-size: large;\"><span style=\"color: blue;\">梯度消失</span></span><br />\n<img src=\"https://kratzert.github.io/images/bn_backpass/chainrule_example.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fkratzert.github.io%2Fimages%2Fbn_backpass%2Fchainrule_example.PNG&h=e01c754f&s={SIZE}\" /><br />\n幾個chapter之前講過,ANN既訓練係靠SGD等反向傳遞去做optimization,而SGD既計算實際上係每一個neuron output得出誤差值後,用條activation function同hidden layer的微分值去得出修正值<br />\n<br />\n淺層既ANN還好, SGD後去返input layer個修正數字仲有返咁上下大細,但深度達到N層既ANN架構,做反向傳遞就要微分N次咁多,即係話,有可能去到一半果陣個gradient 就會變曬0,成個network根本就訓練唔到<br />\n<br />\nvanshing gradient可以用ReLU呢條activation function去改善,因為呢條function個微分值會大過sigmoid function係同一位置既微分值,不過深度增加後仍然有同樣問題出現<br />\n<br />\n<span style=\"font-size: large;\"><span style=\"color: blue;\">過擬合</span></span><br />\nOverfitting問題係指我地拎data訓練時,由於過度既訓練,導致訓練結果有偏差<br />\n<br />\n<img src=\"https://i.stack.imgur.com/t0zit.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fi.stack.imgur.com%2Ft0zit.png&h=af535654&s={SIZE}\" /><br />\n<br />\n利用過分深層既ANN做訓練,如果相應既training set 唔夠大的話,你好容易就會fit過龍<br />\n<br />\n簡單的例子:<br />\n如果你要電腦學一張紙既特徵,但你得10張相,你又係咁叫部電腦自己係呢10張相中搵出&quot;紙&quot;既特徵,佢好有可能連張紙旁邊既筆呀間尺呀人呀都學埋入去個Network度<br />\n<br />\n而點樣可以避免overfitting問題,之後講residual block果陣會講下新既NN有咩方式處理</blockquote><br />\n<br />\n實際操作上，要幾多data起底，先叫有得開個頭玩下?"},{"pid":"534573f3e2521c93e8a84fc7bf35c8f277287e23","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T16:16:45.000Z","msg":"<blockquote><blockquote><span style=\"font-size: xx-large;\"><span style=\"color: red;\">深度人工網路</span></span><br />\n大家可能已經聽過deep learning深度學習呢隻字,其實呢樣野好簡單,就係多過一層的ANN架構,已經可以叫做深度人工網路<br />\n<br />\n<img src=\"https://i.stack.imgur.com/OH3gI.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fi.stack.imgur.com%2FOH3gI.png&h=3aacc6bc&s={SIZE}\" /><br />\n<br />\n好多人會覺得,如果你層數愈多,咁個network咪愈準?<br />\n<br />\n世上無免費的午餐,雖然大體上概念真係愈多層,預測真係會愈準,但係實際執行上,會有好多既問題出現<br />\n<br />\n單睇一層既ANN架構,如果input係m咁多個,有k咁多個人工神經元,輸出返n咁多個output,咁總連結數就係m*n*k咁多個組合,係咁既情況之下,就算ANN幾十年前已經被design出來,就算去到今日既大型電腦,都唔係好計算到咁大既量級data<br />\n<br />\n樓上呢個連接方式,又叫全連接(Fully Connect), 係現代既DL架構入面,一般只係classifier最後一層先會用。由於呢個過程相等於vector martix既inner product,所以又有人叫inner product layer。成個DL既架構又有人會叫<span style=\"color: red;\">Fully Connected Neural Network</span> (FNN)<br />\n<br />\n有另一隻neural network又係叫FNN, 全名係Feedforward Neural Network, 但係Convolution Neural Network都係後者之一, 所以FNN呢隻字都較為少見,又或者有人叫FC-ANN, fully connected artificial neural network,不過都只係名來的<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /> <br />\n<br />\n呢種最簡單既ANN有個好大的問題,就係input layer既大小唔可以改變,咁樣對於唔同大小既影像/矩陣輸入就會好有侷限性<br />\n<br />\n<span style=\"font-size: x-large;\"><span style=\"color: green;\">網路深度增加時的問題</span></span><br />\n傳統ANN最大既問題,就係深度增加時,除左訓練時間會增加之外,仲有一個好大的問題就係梯度消失問題(Vanishing Gradient Problem)同Overfitting問題<br />\n<br />\n<span style=\"font-size: large;\"><span style=\"color: blue;\">梯度消失</span></span><br />\n<img src=\"https://kratzert.github.io/images/bn_backpass/chainrule_example.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fkratzert.github.io%2Fimages%2Fbn_backpass%2Fchainrule_example.PNG&h=e01c754f&s={SIZE}\" /><br />\n幾個chapter之前講過,ANN既訓練係靠SGD等反向傳遞去做optimization,而SGD既計算實際上係每一個neuron output得出誤差值後,用條activation function同hidden layer的微分值去得出修正值<br />\n<br />\n淺層既ANN還好, SGD後去返input layer個修正數字仲有返咁上下大細,但深度達到N層既ANN架構,做反向傳遞就要微分N次咁多,即係話,有可能去到一半果陣個gradient 就會變曬0,成個network根本就訓練唔到<br />\n<br />\nvanshing gradient可以用ReLU呢條activation function去改善,因為呢條function個微分值會大過sigmoid function係同一位置既微分值,不過深度增加後仍然有同樣問題出現<br />\n<br />\n<span style=\"font-size: large;\"><span style=\"color: blue;\">過擬合</span></span><br />\nOverfitting問題係指我地拎data訓練時,由於過度既訓練,導致訓練結果有偏差<br />\n<br />\n<img src=\"https://i.stack.imgur.com/t0zit.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fi.stack.imgur.com%2Ft0zit.png&h=af535654&s={SIZE}\" /><br />\n<br />\n利用過分深層既ANN做訓練,如果相應既training set 唔夠大的話,你好容易就會fit過龍<br />\n<br />\n簡單的例子:<br />\n如果你要電腦學一張紙既特徵,但你得10張相,你又係咁叫部電腦自己係呢10張相中搵出&quot;紙&quot;既特徵,佢好有可能連張紙旁邊既筆呀間尺呀人呀都學埋入去個Network度<br />\n<br />\n而點樣可以避免overfitting問題,之後講residual block果陣會講下新既NN有咩方式處理</blockquote><br />\n<br />\n實際操作上，要幾多data起底，先叫有得開個頭玩下?</blockquote><br />\n真係睇application<br />\n<br />\n我有個network百幾個data都train到CNN好唔錯"},{"pid":"a114ff3237f844a7d278a149bff274d609a25e5a","tid":483239,"uid":63846,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T16:20:54.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>香港做ai有咩好做?<br />\n<br />\n睇andrew ng coursera &amp; deep learning 睇左幾個月，都無咩呢d 工<br />\n<br />\n自己搞又無咩data</blockquote><br />\n香港無，過日本，好多唔洗識日文都請</blockquote><br />\n香港有,但9成都係中資background</blockquote><br />\n<br />\n有冇留意咩行業? 我去angelist / linkedin睇又唔覺喎</blockquote><br />\n我做緊機械人, 其實而家medical, ecommerce都好需要人, 科學園有一間叫sensetime,算係香港最大既AI公司,但係中資來<br />\n<br />\n如果你想入外國公司,去外國啦,香港本土d人你同佢講樓上堆野好多都當你痴線佬又搵唔到錢</blockquote><br />\n<br />\n想走都要走到先得架，美國要visa, 大陸又要接到地氣，唯一出路其實係自己造有用既攞去賣，build product仲實際<br />\n<br />\n我自己目標係想做saas收月費儲客，但係呢排又無咩idea，之前就做過唔同範疇</blockquote><br />\n有PhD,visa好難咩，大不了做個post doc先揾工<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"4fceab9228591ee2a67188cf4a4da3ebca5727d0","tid":483239,"uid":53533,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T16:26:01.000Z","msg":"點睇 cozmo依隻野？<br />\n算唔算AI?"},{"pid":"b67a8d1a653a434f20c822ab89169d0f8c81648b","tid":483239,"uid":63846,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T16:30:31.000Z","msg":"<blockquote>點睇 cozmo依隻野？<br />\n算唔算AI?</blockquote><br />\nYr2 robotics coursework <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"8222f9be2d49a254b90649961b26ea6c94b4b585","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T16:41:52.000Z","msg":"<blockquote>點睇 cozmo依隻野？<br />\n算唔算AI?</blockquote><br />\n玩具來講都算好唔錯的ai了"},{"pid":"9b842ba9529fccd94b3730273f0c6d49640b2afc","tid":483239,"uid":115450,"like":4,"dislike":0,"score":4,"citedBy":0,"replyTime":"2017-11-29T17:08:04.000Z","msg":"<blockquote><span style=\"color: green;\"><span style=\"font-size: x-large;\">圖靈測試</span></span><br />\n呢個測試好簡單,你blind左咁去問一部機器,如果你無方法去分佢究竟係真人定係唔係,咁呢部機器就已經pass左圖靈測試,亦被認為係真正擁有人工智能<br />\n<br />\n而家係無一部機器係成功通過圖靈測試,最近睇左一套電影,對圖靈測試有更深入既探究,講緊係究竟只係語言上通過圖靈測試是否真係可以被證明為有人工智能:<br />\n<br />\n<a href=\"https://www.youtube.com/watch?v=15RZg4kp6qI\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D15RZg4kp6qI&d=tjzMqdWYAiG4enN%2Fcf3LS6NGKT0mDZcTD8WJHRevFdc%3D&h=f69da198\" target=\"_blank\">https://www.youtube.com/watch?v=15RZg4kp6qI</a><br />\nEx_Machina 智能叛侶, netflix同google play都有,劇情一般但係就對咩係AI有比較深入既探討<br />\n<br />\n係電腦岩岩發明既年代,所有同電算有關既基礎theory都好快速咁發展,當中最出名既要數人工智能之父約翰.麥卡錫1956年係第一次達特矛斯會議提出左「人工智能」一詞。佢既諗法同圖靈都係差唔多:<br />\n<br />\n<em><div style=\"text-align: center;\"><span style=\"color: blue;\">人工智能就是要讓機器的行為看起來就像是人所表現出的智能行為一樣。</span></div></em><br />\n<br />\n但咁樣係咪就係真正既人工智能呢? 係哲學上又有另一個思想實驗,令唯心論同唯物論者對人工智能有唔同既理解<br />\n<br />\n<span style=\"color: green;\"><span style=\"font-size: x-large;\">中文房間</span></span><br />\n如果麥卡錫同圖靈都係人工智能既唯物論派,咁約翰&middot;希爾勒就應該係唯心論既代表人物<br />\n<br />\n佢提出左中文房間呢個問題,呢個問題係關乎宇宙同人類既本質問題,不過呢樣野太複雜,呢度唔多講,介紹返個實驗先:<br />\n<br />\n<em>一個對漢語一竅不通，只說英語的人關在一間只有一個開口的封閉房間中。房間裏有一本用英文寫成的手冊，指示該如何處理收到的漢語訊息及如何以漢語相應地回覆。房外的人不斷向房間內遞進用中文寫成的問題。房內的人便按照手冊的說明，尋找到合適的指示，將相應的中文字元組合成對問題的解答，並將答案遞出房間。</em><br />\n<br />\n事實上,房入面既人係完全唔識中文,佢只係按一d規律去達成「智能」既假像,而家既google/ siri 之類軟件好大程度都係做緊類似既野,係人工智能定義上,呢兩派仍然係爭論不休。不過,哲學問題通常都係知道左都唔會影響世界運作既野,咁我地focus返去d solid d既AI題目上面</blockquote><br />\n<br />\n留名支持學術討論<br />\n樓主個Post好高質<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/hoho.gif\" class=\"hkgmoji\" /> <br />\n<br />\n其實Turing Test同Chinese Room Argument想講嘅嘢唔係咁簡單<br />\n<br />\nTuring test個Setting 係:<br />\n你坐係一間房入面, 另外一樣你唔知係咩嘅嘢坐咗係另外一間房入面<br />\n你係睇唔到對面做緊啲咩<br />\n唯一可以溝通嘅就係你問佢問題<br />\n佢會答返你一啲嘢<br />\n如果經過一段時間嘅對答<br />\n你分得出佢同你唔一樣<br />\n咁對面嘅物種就同你唔一樣<br />\n但係如果你完全分唔出有咩分別<br />\n咁姐係話<br />\n你同對面嘅物種係冇分別<br />\n<br />\n套用返係人工智能層面上<br />\n如果你分唔出佢係人工智能<br />\n<span style=\"color: red;\">咁你亦都冇辦法證明你唔係人工智能<br />\n因為事實上你同人工智能冇分別</span><br />\n<br />\n但係Chinese Room Argument反駁嘅地方係指<br />\n你之所以識中文<br />\n係因為你本身識中文<br />\n但係人工智能好似識中文<br />\n係因為其實佢只係根住本手冊對譯回覆<br />\n事實上人工智能本身並唔識得中文<br />\n<br />\nChinese Room Argument其實係某程度上係想嘗試區分人同人工智能嘅分別<br />\n就係<span style=\"color: red;\">人係有意識 (consciousness)</span> 嘅<br />\n<br />\n咁但係Chinese Room Argument其實有不足嘅地方<br />\n就係事實上如果你唔打開間房<br />\n你係永遠都唔會知道人工智能本身並唔識得中文<br />\n咁但係你唔打開間房就唔會知道人工智能本身並唔識得中文<br />\n咁你又點證明你本身識得中文<br />\n<br />\n意思姐係話<br />\n<span style=\"color: red;\">既然你打開個腦都唔會證明到你係本身識得中文<br />\n你又點證明你本身識得中文<br />\n而且<br />\n你又點證明你本身係有意識(consciousness)呢?</span><br />\n<br />\n之後我會嘗試幫樓主解釋ANN嘅原理<br />\n樓主加油<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /> <br />\n<br />\n利申: Psychology year 4 <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"f348b78b69b32663e2f63e2bfec1d81ecbd9ed71","tid":483239,"uid":135855,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-29T17:14:59.000Z","msg":"<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" />"},{"pid":"06d217310ab8a584e2d683cb6b19b38f390f8c4c","tid":483239,"uid":28279,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-30T00:04:15.000Z","msg":"<img src=\"/assets/faces/big/good.gif\" class=\"hkgmoji\" /> 好有心機"},{"pid":"d42dc6b3e6a2dea10b6236d933b3cc65010ee315","tid":483239,"uid":129364,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-30T00:28:22.000Z","msg":"<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> 高汁, 評已正<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" />"},{"pid":"e762c20a7cacc33f9c7298e5174553f2fd1def06","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-30T00:29:25.000Z","msg":"<blockquote><blockquote><span style=\"color: green;\"><span style=\"font-size: x-large;\">圖靈測試</span></span><br />\n呢個測試好簡單,你blind左咁去問一部機器,如果你無方法去分佢究竟係真人定係唔係,咁呢部機器就已經pass左圖靈測試,亦被認為係真正擁有人工智能<br />\n<br />\n而家係無一部機器係成功通過圖靈測試,最近睇左一套電影,對圖靈測試有更深入既探究,講緊係究竟只係語言上通過圖靈測試是否真係可以被證明為有人工智能:<br />\n<br />\n<a href=\"https://www.youtube.com/watch?v=15RZg4kp6qI\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D15RZg4kp6qI&d=tjzMqdWYAiG4enN%2Fcf3LS6NGKT0mDZcTD8WJHRevFdc%3D&h=f69da198\" target=\"_blank\">https://www.youtube.com/watch?v=15RZg4kp6qI</a><br />\nEx_Machina 智能叛侶, netflix同google play都有,劇情一般但係就對咩係AI有比較深入既探討<br />\n<br />\n係電腦岩岩發明既年代,所有同電算有關既基礎theory都好快速咁發展,當中最出名既要數人工智能之父約翰.麥卡錫1956年係第一次達特矛斯會議提出左「人工智能」一詞。佢既諗法同圖靈都係差唔多:<br />\n<br />\n<em><div style=\"text-align: center;\"><span style=\"color: blue;\">人工智能就是要讓機器的行為看起來就像是人所表現出的智能行為一樣。</span></div></em><br />\n<br />\n但咁樣係咪就係真正既人工智能呢? 係哲學上又有另一個思想實驗,令唯心論同唯物論者對人工智能有唔同既理解<br />\n<br />\n<span style=\"color: green;\"><span style=\"font-size: x-large;\">中文房間</span></span><br />\n如果麥卡錫同圖靈都係人工智能既唯物論派,咁約翰&middot;希爾勒就應該係唯心論既代表人物<br />\n<br />\n佢提出左中文房間呢個問題,呢個問題係關乎宇宙同人類既本質問題,不過呢樣野太複雜,呢度唔多講,介紹返個實驗先:<br />\n<br />\n<em>一個對漢語一竅不通，只說英語的人關在一間只有一個開口的封閉房間中。房間裏有一本用英文寫成的手冊，指示該如何處理收到的漢語訊息及如何以漢語相應地回覆。房外的人不斷向房間內遞進用中文寫成的問題。房內的人便按照手冊的說明，尋找到合適的指示，將相應的中文字元組合成對問題的解答，並將答案遞出房間。</em><br />\n<br />\n事實上,房入面既人係完全唔識中文,佢只係按一d規律去達成「智能」既假像,而家既google/ siri 之類軟件好大程度都係做緊類似既野,係人工智能定義上,呢兩派仍然係爭論不休。不過,哲學問題通常都係知道左都唔會影響世界運作既野,咁我地focus返去d solid d既AI題目上面</blockquote><br />\n<br />\n留名支持學術討論<br />\n樓主個Post好高質<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/hoho.gif\" class=\"hkgmoji\" /> <br />\n<br />\n其實Turing Test同Chinese Room Argument想講嘅嘢唔係咁簡單<br />\n<br />\nTuring test個Setting 係:<br />\n你坐係一間房入面, 另外一樣你唔知係咩嘅嘢坐咗係另外一間房入面<br />\n你係睇唔到對面做緊啲咩<br />\n唯一可以溝通嘅就係你問佢問題<br />\n佢會答返你一啲嘢<br />\n如果經過一段時間嘅對答<br />\n你分得出佢同你唔一樣<br />\n咁對面嘅物種就同你唔一樣<br />\n但係如果你完全分唔出有咩分別<br />\n咁姐係話<br />\n你同對面嘅物種係冇分別<br />\n<br />\n套用返係人工智能層面上<br />\n如果你分唔出佢係人工智能<br />\n<span style=\"color: red;\">咁你亦都冇辦法證明你唔係人工智能<br />\n因為事實上你同人工智能冇分別</span><br />\n<br />\n但係Chinese Room Argument反駁嘅地方係指<br />\n你之所以識中文<br />\n係因為你本身識中文<br />\n但係人工智能好似識中文<br />\n係因為其實佢只係根住本手冊對譯回覆<br />\n事實上人工智能本身並唔識得中文<br />\n<br />\nChinese Room Argument其實係某程度上係想嘗試區分人同人工智能嘅分別<br />\n就係<span style=\"color: red;\">人係有意識 (consciousness)</span> 嘅<br />\n<br />\n咁但係Chinese Room Argument其實有不足嘅地方<br />\n就係事實上如果你唔打開間房<br />\n你係永遠都唔會知道人工智能本身並唔識得中文<br />\n咁但係你唔打開間房就唔會知道人工智能本身並唔識得中文<br />\n咁你又點證明你本身識得中文<br />\n<br />\n意思姐係話<br />\n<span style=\"color: red;\">既然你打開個腦都唔會證明到你係本身識得中文<br />\n你又點證明你本身識得中文<br />\n而且<br />\n你又點證明你本身係有意識(consciousness)呢?</span><br />\n<br />\n之後我會嘗試幫樓主解釋ANN嘅原理<br />\n樓主加油<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /> <br />\n<br />\n利申: Psychology year 4 <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n呢個係好fundamental的問題<br />\n<br />\n既然智慧可以被製造，人類的智能係咪又係被製造出來？<br />\n<br />\n break down到細胞同分子層面，我地日常行為都係由生物/化學/物理過程支配，一條蟲都有同樣機制，要去到幾複雜的結構程度先可以算係有智能？<br />\n<br />\n好多年前人以為智能係人類獨有，但而家我地智好多動物都有，只係人類更高智能<br />\n<br />\n如果人腦活動能夠被數學化，咁仲存唔存在自由意志？係現行ANN機制下，你一但訓練好個network,佢就唔會再改變輸出結果，我地嘅意識究竟係當中有咩主導作用？<br />\n<br />\n當然可以加入隨機過程令網路輸出有變化，但係電腦係唔存在真正隨機，帶來更大的問題係，如果我地嘅自主意識包含隨機成分，咁宇宙係咪存在真正隨機就好大影響<br />\n<br />\n呢個問題諗左好耐，再推廣上去居然係有無做物主呢d哲學問題，所以我先係咁上網搵d 真正嘅AI 資料睇，睇睇下要去去搵d哲學朋友研究<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"98c62c71d9f9700cd23f65fe6ed453541be74f5f","tid":483239,"uid":73231,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-30T01:14:41.000Z","msg":"留名<br />\n想研究另類AI application<br />\n覺得廿年內會另一個shift 離開AI"},{"pid":"47f77ef18ae9e5d49a839e7eee4459af54ee425a","tid":483239,"uid":12599,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-11-30T03:50:42.000Z","msg":"<span style=\"font-size: xx-large;\"><span style=\"color: red;\">卷積神經網路</span></span><br />\n終於都去到現代AI其中一個最重要的算法,卷積神經網路(Convolution Neural Network, CNN)<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <br />\n<br />\nCNN主要應用場景係影像處理,對應於人類既視覺系統。相應聽覺同記憶系統就會用RNN呢種DL算法。<br />\n<br />\n係講CNN前,介紹返電腦視覺中最出名的OpenCV,相信唔少Engine出身既朋友都有玩過呢個toolkit<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <br />\n<br />\n<a href=\"https://opencv.org/\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fopencv.org%2F&d=tjzMqdWYAiG4enN%2Fcf3LS6NGKT0mDZcTD8WJHRevFdc%3D&h=43aaedcd\" target=\"_blank\">https://opencv.org/</a><br />\n<br />\n其實我自己主力都唔係打OpenCV而係其他3D影像處理library,不過OpenCV作為2D影像處理既頂尖軟件,集成左大量實用既影像算法同應用例字,有python有c++有java, ios android都support,作為影像算法入門係非常之好用既平台<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <br />\n<br />\n點解要拎OpenCV出來講?因為OpenCV絕大部分算法都係人為設計一d影像特徵(features),有時加上SVM等機械學習算法,去做影像運算。係呢個過程中,電腦影像工程師主要工作就係搵出呢d features<br />\n<br />\n<img src=\"https://docs.opencv.org/2.4/_images/Feature_Flann_Matcher_Tutorial_Cover.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fdocs.opencv.org%2F2.4%2F_images%2FFeature_Flann_Matcher_Tutorial_Cover.jpg&h=330eb876&s={SIZE}\" /><br />\n<br />\n當然,人腦既力量係有限的,所似呢種方法做出來的圖像分析,效果其實唔多好。好多人都知ANN係有效既圖像處理方法,但係ANN之前講左有好大侷限性,所以效果都唔比呢d人為feature extraction method分析要好<br />\n<br />\n去到1989年,終於有人propose左CNN既最經典架構 -- LeNet<br />\n<br />\n<img src=\"http://yann.lecun.com/exdb/lenet/gifs/asamples.gif\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fyann.lecun.com%2Fexdb%2Flenet%2Fgifs%2Fasamples.gif&h=7d287a13&s={SIZE}\" /><br />\n<br />\nLeNet作為CNN最先驅設計,網路深度一開始只有5層(LeNet-5),後來再有改良的7層LeNet-7。詳細既設計會之後再講,當時5層既CNN已經取得非常不錯既成果。但係受限於電腦運算力, CNN發展仍然好大限制。<br />\n<br />\n<span style=\"color: green;\"><span style=\"font-size: x-large;\">GPU的引入</span></span><br />\n時間轉到去2005年,大家果陣仲係用緊Pentium 4定Core 2 Duo的時代,PC經已普及,但應該唔多人屋企部機係有獨立GPU的。<br />\n<br />\n係呢個時候3D 遊戲既發展帶動左GPU市場,同時有人諗,可唔可以用GPU去做平行運算?於是就有左通用GPU(GPGPU) 算法的paper出現左<br />\n<br />\nGPU係乜東西?其實係就一個電腦元件集成左好多運算單元既一個平行計算器,單核時脈比CPU低好多,但係數量就多CPU幾百倍,拎來做計數就最岩啦<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <br />\n<br />\n平行運算一直都唔係容易入手既編程方法來,現代GPU編程會用到一重叫做核(Kernel)既概念,一個GPU運算單元就負責一個Kernel的運算。由於不同的運算單元運算能力都係極之接近,呢d我地叫同質性運算(Homogeneous Computing),係平行運算出算係比較容易處理而且高效既算法。當出最出名一定係Nvidia既CUDA<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <br />\n<br />\n<img src=\"http://geco.mines.edu/tesla/cuda_tutorial_mio/pic/Picture1.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fgeco.mines.edu%2Ftesla%2Fcuda_tutorial_mio%2Fpic%2FPicture1.png&h=055e339c&s={SIZE}\" /><br />\n<br />\n***關於GPU Kernel運算,正式名應該叫thread/block, Kernel係電腦有另一個意思係指軟硬件接合介面,不過講CNN,用樓上既概念去講Kernel會好d<br />\n<br />\n大家記得2007年iphone發佈嗎?大數據時代到來, GPU運算硬件ready, 只欠東風, 去到2012年,AlexNet發佈,證明左CNN係GPU上的運用有效性, Machine Learning自此進入新時代<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <br />\n<br />\n<span style=\"font-size: x-large;\"><span style=\"color: green;\">CNN is a black box</span></span><br />\n<img src=\"https://pic2.zhimg.com/50/v2-07b94113cd90697b17a3b149cff7f081_hd.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fpic2.zhimg.com%2F50%2Fv2-07b94113cd90697b17a3b149cff7f081_hd.jpg&h=88082790&s={SIZE}\" /><br />\n<br />\n一如ANN既目的,我地用CNN最初都只係想整一個Classifier, 去分唔同既圖像代表既意義,但係如果我地轉少少張相,成個ANN既network可能已經成個走左樣<br />\n<br />\n<img src=\"https://pic1.zhimg.com/50/v2-cc555b2db7b53159d41da4f6c9988610_hd.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fpic1.zhimg.com%2F50%2Fv2-cc555b2db7b53159d41da4f6c9988610_hd.jpg&h=184d50de&s={SIZE}\" /><br />\n<br />\nCNN提出左我地唔好著重圖像既整體,專注係不同既小區域去搵image feature,然後記住呢d feature,之後無論咩圖入來,我地都可以按feature既特性去分呢張圖究竟係咩東西<br />\n<br />\n<img src=\"https://pic1.zhimg.com/50/v2-389c7fe4b460d0c025c8413abfe5512c_hd.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fpic1.zhimg.com%2F50%2Fv2-389c7fe4b460d0c025c8413abfe5512c_hd.jpg&h=462cb57f&s={SIZE}\" /><br />\n<br />\n呢個諗法同人眼睇世界既模式好接近。其實我地既眼睛每次只可以focus係好細既區域入面,只係我地對眼係咁移動加大腦補完先會有全個畫面都係清晰既假像<br />\n<br />\n<img src=\"https://s-media-cache-ak0.pinimg.com/originals/ea/88/ff/ea88ff4e2244192a22908d539ee530c9.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fs-media-cache-ak0.pinimg.com%2Foriginals%2Fea%2F88%2Fff%2Fea88ff4e2244192a22908d539ee530c9.jpg&h=69037e4b&s={SIZE}\" /><br />\n<br />\n呢一個睇小部分區域既過程係電腦入面叫卷積Convolution"},{"pid":"c047414f32c1dc6326a87f87410f63b423f787d7","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-30T04:02:39.000Z","msg":"<span style=\"font-size: x-large;\"><span style=\"color: green;\">卷積層</span></span><br />\n呢樣野好簡單咁講,即係兩個矩陣element by element-wise地相乘<br />\n<br />\n<img src=\"https://pic4.zhimg.com/50/v2-170b72d851cbf324cc09c0d1730efbaf_hd.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fpic4.zhimg.com%2F50%2Fv2-170b72d851cbf324cc09c0d1730efbaf_hd.jpg&h=726b20ba&s={SIZE}\" /><br />\nConvolution單元叫kernel,一般最細用3*3, 極大部份係單數邊長, 1*1係特殊情況下會使用,3*3以上可以reduce返去n層3*3的convolution kernel,而且運算量會更低<br />\n<br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/4f/3D_Convolution_Animation.gif\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2F4%2F4f%2F3D_Convolution_Animation.gif&h=a3eabf9c&s={SIZE}\" /><br />\n呢張gif應該可以好易俾大家去知convolution 係點計<br />\n<br />\nCNN要做既,就係記住呢d kernel既形狀,同埋試多d不同形狀既kernel<br />\n<br />\n所以做完一個m*n既圖像, 會有3*3*k咁樣既matrix出返來,k就係我地設計network既每層測試既kernel數量。kernel愈多,運算愈準,但有機會overfit,而且時間亦都愈長。k又有d人叫深度,產生出來的3*3*k kernel matrix,可以再被視為一張大圖,不停再進行convolution/ pooling/ inner product等圖像運算<br />\n<br />\n<span style=\"font-size: x-large;\"><span style=\"color: green;\">池化層</span></span><br />\n池化(pooling)係convolutiona既一個特類,之前無講咩係步長(stride),其實就係解kernel係圖像中每次行既距離<br />\n<br />\n<img src=\"http://machinelearninguru.com/_images/topics/computer_vision/basics/convolutional_layer_1/stride2.gif\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fmachinelearninguru.com%2F_images%2Ftopics%2Fcomputer_vision%2Fbasics%2Fconvolutional_layer_1%2Fstride2.gif&h=4a5cda97&s={SIZE}\" /><br />\n<br />\npooling目標係縮細kernel image既大細,令運算量下降,做法同convolution layer差唔多,但唔係做相乘,而係拎最大值(max pooling)/平均值(average pooling)<br />\n<br />\n最常用係max pooling法,原因係運算量低而且已知有效 (CNN is black box, don't ask why, just try)<br />\n<br />\n<img src=\"http://cs231n.github.io/assets/cnn/maxpool.jpeg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fcs231n.github.io%2Fassets%2Fcnn%2Fmaxpool.jpeg&h=e0ca1371&s={SIZE}\" /><br />\n<br />\n<span style=\"font-size: x-large;\"><span style=\"color: green;\">全連接層/Softmax層</span></span><br />\n即係傳統既ANN架構,通常去到CNN結構尾部先會用到, 全名fully connected layer (FC layer), 又即係inner product layer<br />\n<br />\n<img src=\"https://pic2.zhimg.com/50/v2-9c967ec12fa4a969245de0ae7118bab9_hd.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fpic2.zhimg.com%2F50%2Fv2-9c967ec12fa4a969245de0ae7118bab9_hd.jpg&h=52823e7f&s={SIZE}\" /><br />\n<br />\n<img src=\"https://pic3.zhimg.com/50/v2-b833cf3280c2f12b6f50e6bf1ce48e3a_hd.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fpic3.zhimg.com%2F50%2Fv2-b833cf3280c2f12b6f50e6bf1ce48e3a_hd.jpg&h=6c409bd4&s={SIZE}\" /><br />\n樓上vectorization同fully connection數學上同inner product係一致的<br />\n<br />\n<img src=\"https://pic3.zhimg.com/50/v2-6840898a5893c31f77c89bcd539b4922_hd.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fpic3.zhimg.com%2F50%2Fv2-6840898a5893c31f77c89bcd539b4922_hd.jpg&h=29c2c608&s={SIZE}\" /><br />\n最後好似ANN咁,做返Softmax計classifier的probability,做量化估計,加上loss layer就可以做SGD反向訓練<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <br />\n<br />\n訓練好既network就可以拎來做prediction啦<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <br />\n<br />\n<br />\n之後會講下幾個經典CNN例子俾大家了解多d<img src=\"/assets/faces/normal/bye.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/bye.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/bye.gif\" class=\"hkgmoji\" />"},{"pid":"725df7bca10f335f7fb6bb40dbe819459b9fcba4","tid":483239,"uid":115450,"like":2,"dislike":0,"score":2,"citedBy":0,"replyTime":"2017-11-30T04:34:02.000Z","msg":"<blockquote><blockquote><br />\n<br />\n留名支持學術討論<br />\n樓主個Post好高質<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/hoho.gif\" class=\"hkgmoji\" /> <br />\n<br />\n其實Turing Test同Chinese Room Argument想講嘅嘢唔係咁簡單<br />\n<br />\nTuring test個Setting 係:<br />\n你坐係一間房入面, 另外一樣你唔知係咩嘅嘢坐咗係另外一間房入面<br />\n你係睇唔到對面做緊啲咩<br />\n唯一可以溝通嘅就係你問佢問題<br />\n佢會答返你一啲嘢<br />\n如果經過一段時間嘅對答<br />\n你分得出佢同你唔一樣<br />\n咁對面嘅物種就同你唔一樣<br />\n但係如果你完全分唔出有咩分別<br />\n咁姐係話<br />\n你同對面嘅物種係冇分別<br />\n<br />\n套用返係人工智能層面上<br />\n如果你分唔出佢係人工智能<br />\n<span style=\"color: red;\">咁你亦都冇辦法證明你唔係人工智能<br />\n因為事實上你同人工智能冇分別</span><br />\n<br />\n但係Chinese Room Argument反駁嘅地方係指<br />\n你之所以識中文<br />\n係因為你本身識中文<br />\n但係人工智能好似識中文<br />\n係因為其實佢只係根住本手冊對譯回覆<br />\n事實上人工智能本身並唔識得中文<br />\n<br />\nChinese Room Argument其實係某程度上係想嘗試區分人同人工智能嘅分別<br />\n就係<span style=\"color: red;\">人係有意識 (consciousness)</span> 嘅<br />\n<br />\n咁但係Chinese Room Argument其實有不足嘅地方<br />\n就係事實上如果你唔打開間房<br />\n你係永遠都唔會知道人工智能本身並唔識得中文<br />\n咁但係你唔打開間房就唔會知道人工智能本身並唔識得中文<br />\n咁你又點證明你本身識得中文<br />\n<br />\n意思姐係話<br />\n<span style=\"color: red;\">既然你打開個腦都唔會證明到你係本身識得中文<br />\n你又點證明你本身識得中文<br />\n而且<br />\n你又點證明你本身係有意識(consciousness)呢?</span><br />\n<br />\n之後我會嘗試幫樓主解釋ANN嘅原理<br />\n樓主加油<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /> <br />\n<br />\n利申: Psychology year 4 <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n呢個係好fundamental的問題<br />\n<br />\n既然智慧可以被製造，人類的智能係咪又係被製造出來？<br />\n<br />\n break down到細胞同分子層面，我地日常行為都係由生物/化學/物理過程支配，一條蟲都有同樣機制，要去到幾複雜的結構程度先可以算係有智能？<br />\n<br />\n好多年前人以為智能係人類獨有，但而家我地智好多動物都有，只係人類更高智能<br />\n<br />\n如果人腦活動能夠被數學化，咁仲存唔存在自由意志？係現行ANN機制下，你一但訓練好個network,佢就唔會再改變輸出結果，我地嘅意識究竟係當中有咩主導作用？<br />\n<br />\n當然可以加入隨機過程令網路輸出有變化，但係電腦係唔存在真正隨機，帶來更大的問題係，如果我地嘅自主意識包含隨機成分，咁宇宙係咪存在真正隨機就好大影響<br />\n<br />\n呢個問題諗左好耐，再推廣上去居然係有無做物主呢d哲學問題，所以我先係咁上網搵d 真正嘅AI 資料睇，睇睇下要去去搵d哲學朋友研究<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\n<br />\nANN嘅Machine Learning Method其實係演化自人類嘅Learning behavior<br />\n人類之所以有習慣會學識一樣新嘢, 其實都係因為其他人同佢做緊Conditioning (Reinforcement and Punishment), 而人係冇其他人幫佢嘅時候, 同樣會用邏輯推論(inductive and deductive reasoning) 黎幫我地Shape個Behavior出黎, 所以ANN唔單只係模擬緊人類點樣思考, 而且係一定程度上證明緊其實人係冇Free will (自由意志)<br />\n<br />\n所以我地Psychology入面都分開咗兩派學說, 一派係Functionalism, 認為如果果樣嘢可以做到我地同樣做到嘅嘢, 唔理佢點樣做出黎 (而事實上亦冇人會知道) 咁姐係果樣嘢同我地冇分別<br />\n<br />\n另一派就認為人係有Consciousness 所以先有Sub Conscious 同埋 Unconscious<br />\n<br />\n利申: Functionalism<img src=\"/assets/faces/normal/smile.gif\" class=\"hkgmoji\" />"},{"pid":"20be6b2cac8435bc95a6b4fade577e3a8e4d7ec5","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-30T05:41:48.000Z","msg":"<span style=\"font-size: x-large;\"><span style=\"color: green;\">LeNet</span></span><br />\n<br />\n<img src=\"https://cdn-images-1.medium.com/max/1600/1*BqkU9O6T8lwNbJJN-bS0yw.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1600%2F1%2ABqkU9O6T8lwNbJJN-bS0yw.png&h=83e72232&s={SIZE}\" /><br />\n<br />\n呢個係第一個propose的CNN結構,好簡單,基本上係conv -&gt; pool -&gt; conv -&gt; pool -&gt; fc -&gt;fc -&gt; softmax 既組合<br />\n<br />\nconv kernel 都係5*5既大小,不過後來發現, 2層3*3其實就等於5*5既kernel,<br />\n<br />\n3*3*2 = 18個connection<br />\n5*5 = 25 個connection<br />\n<br />\n咁樣,3*3既convolution kernel就變成左最基礎的unit<br />\n<br />\n<span style=\"font-size: x-large;\"><span style=\"color: green;\">AlexNet</span></span><br />\n<img src=\"https://sushscience.files.wordpress.com/2016/12/alexnet6.jpg?w=900\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fsushscience.files.wordpress.com%2F2016%2F12%2Falexnet6.jpg%3Fw%3D900&h=3be61d7a&s={SIZE}\" /><br />\n<br />\n呢個係第一個用左GPU加速的CNN結構,主要目的係想證明,CNN利用左GPU後,可以係有限既訓練時間內,得出令人滿意既結果<br />\n<br />\n但係由於alexnet仍然好大舊,當時既單張GPU (2012年),好似仲係GTX 980的年代,個internal memory仲係唔夠放入曬成個network, 折衷既法係convolution層將張相斬半,後來再組合返。但呢個唔係一個好的practice, 係多GPU運算出唔同既toolkit 開發商都投入左唔少心力先做到多卡平衡運算<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <br />\n<br />\n<span style=\"font-size: x-large;\"><span style=\"color: green;\">GoogleNet, VGG, ResNet</span></span><br />\nnetwork結構唔再詳講了,畢竟而家日日都有新network propose,不過都有幾個出名的CNN 結構介紹返<br />\n<br />\n<img src=\"http://img.mp.itc.cn/upload/20170416/fde6c43388a74e7cabf362ae3b0ac74a_th.jpeg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fimg.mp.itc.cn%2Fupload%2F20170416%2Ffde6c43388a74e7cabf362ae3b0ac74a_th.jpeg&h=35969da1&s={SIZE}\" /><br />\nGoogleNet, 2014年imagenet比賽冠軍,主要係想demonstrate大幅度增加network深度係可行的<br />\n<br />\nGoogleNet 主要組件(block)係Inception架構,基本上都係AlexNet的變化形<br />\n<img src=\"http://img.mp.itc.cn/upload/20170416/5adfebd4898b464fa9cea476401abbd5_th.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fimg.mp.itc.cn%2Fupload%2F20170416%2F5adfebd4898b464fa9cea476401abbd5_th.png&h=5b8d93e0&s={SIZE}\" /><br />\n<br />\n2014年同期有個好出名的network叫VGG,大量利用convolution,基本上都係幾個conv先pool一次, 一直去到得返1*1*kernel depth的形狀<br />\n<img src=\"https://www.pyimagesearch.com/wp-content/uploads/2017/03/imagenet_vgg16.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fwww.pyimagesearch.com%2Fwp-content%2Fuploads%2F2017%2F03%2Fimagenet_vgg16.png&h=94408e6d&s={SIZE}\" /><br />\n<br />\n去到舊年,有人提出layer同layer間可以有捷徑,保留返一d因為subsampling而損失的image features,呢d 捷徑正名叫殘差(Residual),所以就叫residual neural network<br />\n<br />\n<img src=\"http://img.mp.itc.cn/upload/20170416/0036945cc69f41d8b08b60a774ab2153.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fimg.mp.itc.cn%2Fupload%2F20170416%2F0036945cc69f41d8b08b60a774ab2153.png&h=bce824a5&s={SIZE}\" /><br />\n<br />\n利用殘差特性,gradient vanish problem有效解決,ANN深度最多達到152層,之前最多都係行到30層左右<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <br />\n<br />\n<img src=\"http://img.mp.itc.cn/upload/20170416/f58c4079344c4deea71f2f89a12f2116_th.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fimg.mp.itc.cn%2Fupload%2F20170416%2Ff58c4079344c4deea71f2f89a12f2116_th.png&h=1e1baffd&s={SIZE}\" /><br />\nResNet 的組成元件,原理就係VGG加分支<br />\n<br />\n而ResNet係而家image classification最有效既network之一"},{"pid":"02aa5f580fc10e1ec70282855dc8461ce4096194","tid":483239,"uid":12599,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-11-30T14:32:00.000Z","msg":"<span style=\"font-size: xx-large;\"><span style=\"color: red;\">應用場景</span></span><br />\n講左咁耐ANN, 其實係圖像上除左分類,仲有D咩可以應用?<br />\n<br />\n1. Semantic Segmentation 圖像分割<br />\n<a href=\"https://i.stack.imgur.com/mPFUo.jpg\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fi.stack.imgur.com%2FmPFUo.jpg&d=tjzMqdWYAiG4enN%2Fcf3LS6NGKT0mDZcTD8WJHRevFdc%3D&h=b970d759\" target=\"_blank\">https://i.stack.imgur.com/mPFUo.jpg</a><br />\n<br />\n2. Image Registration 圖像配準<br />\n<img src=\"https://www.mathworks.com/content/mathworks/www/en/discovery/image-registration/jcr:content/mainParsys/image_0.adapt.full.high.jpg/1505984374193.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fwww.mathworks.com%2Fcontent%2Fmathworks%2Fwww%2Fen%2Fdiscovery%2Fimage-registration%2Fjcr%3Acontent%2FmainParsys%2Fimage_0.adapt.full.high.jpg%2F1505984374193.jpg&h=2c29ae6e&s={SIZE}\" /><br />\n<br />\n3. Feature Extraction 提徵點提取<br />\n<img src=\"https://github.com/1adrianb/face-alignment/raw/master/docs/images/face-alignment-adrian.gif\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fgithub.com%2F1adrianb%2Fface-alignment%2Fraw%2Fmaster%2Fdocs%2Fimages%2Fface-alignment-adrian.gif&h=dc47952d&s={SIZE}\" /><br />\n<br />\n4. 3D Reconstruction 三維重建<br />\n<img src=\"http://aaronsplace.co.uk/papers/jackson2017recon/preview.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Faaronsplace.co.uk%2Fpapers%2Fjackson2017recon%2Fpreview.png&h=f61fa01f&s={SIZE}\" /><br />\n<br />\n5. Depth Sensing 深度測量<br />\n<img src=\"https://img.eservice-hk.net/upload/2017/11/30/223057_6f412a542647d967fbb3fad0fc40b872.jpeg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fimg.eservice-hk.net%2Fupload%2F2017%2F11%2F30%2F223057_6f412a542647d967fbb3fad0fc40b872.jpeg&h=5f50275e&s={SIZE}\" /><br />\n<br />\n每過幾日就有新既paper release, CNN係圖像處理方面既表現,仍然有好大既發揮空間<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" />"},{"pid":"da4aaff5dc44598d0019a2448a2106c8017a9efa","tid":483239,"uid":9683,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-30T14:37:43.000Z","msg":"樓主好撚勁，我剩係識matlab分析樣個D皮毛野<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" />"},{"pid":"09e10a952953b8a50bb607a82eccb437d51cbdcb","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-30T15:22:48.000Z","msg":"CNN既介紹到此為止,原本都想講埋RNN同LSTM(主要For有時間性既sequence,例如聲音,股票)<br />\n<br />\n不過似乎太過深入講技術野,如果真係有興趣既朋友,親身落手打係最好既學習方法<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <br />\n<br />\n<span style=\"font-size: xx-large;\"><span style=\"color: red;\">Machine Learning入門</span></span><br />\n其實每一隻programming language都有自己既machine learning toolkit,當中python既package特別優秀<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /> <br />\n<br />\n<span style=\"font-size: x-large;\"><span style=\"color: green;\">硬件</span></span><br />\n<span style=\"color: blue;\">CPU</span>: intel/amd都ok,如果用到Intel 的Math Kernel Library (MKL),用返intel會好d,不過有其他更好的linear algebra library,例如OpenBLAS同AtlasBLAS, 所以作為Ryzen fan我係推薦AMD多一d<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n<span style=\"color: blue;\">GPU</span>:<br />\n<img src=\"http://www.nvidia.com/docs/IO/151309/nvidia-logo.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fwww.nvidia.com%2Fdocs%2FIO%2F151309%2Fnvidia-logo.png&h=b49fee31&s={SIZE}\" /><br />\n 一定要用Nvidia卡,因為大部分package都係針對CUDA做左加速,以我所知所有大型server center都係用Nvida卡計DL, 個人建議入門都要買1060, 有錢就買1080ti, Titian Xp memory多1GB,但性價比低太多,又唔易買,所以都係望下就算<br />\n<br />\n<span style=\"color: blue;\">OS</span>: <br />\n<img src=\"https://assets.ubuntu.com/v1/c037fd75-ubuntu-logo.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fassets.ubuntu.com%2Fv1%2Fc037fd75-ubuntu-logo.png&h=1de1a5cf&s={SIZE}\" /><br />\nWindows, Linux, OSX都得,但記得太部份library都只係linux上有GPU acceleration, 係windows上需要用python 3先用到GPU, 呢樣野我個人係唔多鐘意的,同埋效能唔太好, OSX更加唔使講, 你根本裝唔到GPU入去,所以拎來打code就算, training你SSH返去linux server計啦<br />\n<br />\nLinux Distro以<span style=\"color: red;\">ubuntu</span>為最佳,唔好學軟件台d人用arch centos,你玩死你自己only,網上non-debian base既library build support係極少, ubuntu 以14同16兩個version最多人support"},{"pid":"d7c91a912f781ed51a7f90d7cbf0692a0c6135d4","tid":483239,"uid":12599,"like":2,"dislike":0,"score":2,"citedBy":0,"replyTime":"2017-11-30T15:23:00.000Z","msg":"<span style=\"font-size: x-large;\"><span style=\"color: green;\">軟件</span></span><br />\n如果你選擇以python作為入門,我強烈建議裝anaconda,集成左好多常用既python library,唔使逐隻裝,個人建議python 2.7會好d,不過3.6一般用都唔係大問題,但去到C++ embedding果陣3.6會出好多問題,唔打到咁深入既人唔使理, 留意返windows上面要用gpu加速好多都要用到python 3.6,主要係c++ compiler對parallelization處理的問題<br />\n<a href=\"https://anaconda.org/\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fanaconda.org%2F&d=tjzMqdWYAiG4enN%2Fcf3LS6NGKT0mDZcTD8WJHRevFdc%3D&h=b1c0149b\" target=\"_blank\">https://anaconda.org/</a><br />\n<br />\nIDE 我最鐘意用sublime text,如果你裝左python,按F7就可以run起段code<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <br />\n<a href=\"https://www.sublimetext.com/\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.sublimetext.com%2F&d=tjzMqdWYAiG4enN%2Fcf3LS6NGKT0mDZcTD8WJHRevFdc%3D&h=d54f4edb\" target=\"_blank\">https://www.sublimetext.com/</a><br />\n<br />\n現行最出名既machine learning toolkit有以下幾隻:<br />\n1. <span style=\"color: blue;\">Tensorflow</span> <br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a4/TensorFlowLogo.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fa%2Fa4%2FTensorFlowLogo.png&h=57b091a7&s={SIZE}\" /><br />\n(Python with C++ API, ios同android食到個save左既network)<br />\nGoogle出品,出名難用,原來Theano既developer都去曬TF條team, 做clustering computation 效率非常高,但單機效能唔多好。強大在於document超級多,使用者亦都超級多,所以你有問題好多時都會有solution, 而且有tensor board俾你用browser mon住個training進度,唔使自己寫,仲有TPU support(如果你申請到)。想向難度挑戰/通用型machine learning toolkit,請揀tensorflow<br />\n<br />\n2. <span style=\"color: blue;\">Torch</span> <br />\n<img src=\"https://cdn-images-1.medium.com/max/1200/1*aqNgmfyBIStLrf9k7d9cng.jpeg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1200%2F1%2AaqNgmfyBIStLrf9k7d9cng.jpeg&h=a1878f8b&s={SIZE}\" /><br />\n(原自Lua, 有Python 版, ios同android食到個save左既network)<br />\nFacebook出品,強烈建議使用,根本就係有GPU support的numpy來,非常之人性化。對於clustering computation, torch有nn.parallel package,雖然仍然開發中,但係已經有好多人揀左pytorch作為開發平台,因為實在太易上手<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <br />\n<br />\nC++ api我知有人寫緊,不過現階段真係唔好expect太多, 由於pytorch係1月發佈,doucmentation仍然不足<br />\n<br />\n3. <span style=\"color: blue;\">Caffe</span> <br />\n<img src=\"http://caffe2.ai/static/og_image_caffe2.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fcaffe2.ai%2Fstatic%2Fog_image_caffe2.png&h=12ad8d9f&s={SIZE}\" /><br />\n(Python with C++ API, ios同android食到個save左既network)<br />\nBerkeley出品,而家針對mobile平台開發左caffe2, 後來個developer俾Facebook請左,而家caffe同pytorch network係可以用converter轉換的, 如果你好keen on 圖像開發,caffe係一個好的選擇, caffe 2更好,但我只係linux 成功build到, windows得caffe 1代由Microsoft d 人出馬先改到build得到,如果你好想係windows上面玩machine learning, pytorch會更好<br />\n<br />\n4. <span style=\"color: blue;\">Keras</span><br />\n<img src=\"https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fs3.amazonaws.com%2Fkeras.io%2Fimg%2Fkeras-logo-2018-large-1200.png&h=ed0b3705&s={SIZE}\" /><br />\n(Python, R)<br />\n封裝左的tensorflow,唔建議用,新手玩下就好,得D fundamental function,你想加減野難過登天, 聽講拎來做RNN非常之不錯<br />\n<br />\n5. <span style=\"color: blue;\">Scikit-Learn</span><br />\n<img src=\"https://ih0.redbubble.net/image.193727600.0984/sticker,375x360-bg,ffffff.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fih0.redbubble.net%2Fimage.193727600.0984%2Fsticker%2C375x360-bg%2Cffffff.png&h=2d97fd42&s={SIZE}\" /><br />\n(Python)<br />\n最出名的machine learning package (deep learning以外), 如果你要做clustering, SVM, regression, 呢個package會好岩你<br />\n<br />\n6. <span style=\"color: blue;\">dlib</span><br />\n<img src=\"https://upload.wikimedia.org/wikipedia/en/d/d9/Dlib_c%2B%2B_library_logo.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fen%2Fd%2Fd9%2FDlib_c%252B%252B_library_logo.png&h=2bf55271&s={SIZE}\" /><br />\n(C++,有python接口)<br />\n以C++為本的的library,如果你要封裝你既software係local度run,又想係windows上面行,呢個有好多pre-built左既model,非常之易上手<br />\n<br />\n7. <span style=\"color: blue;\">其他</span><br />\n仲有MXnet, microsoft的CNTK,我自己無乜點用到,如果你真係要用到咁深入先再睇吧<br />\n<br />\n==========================================================<br />\n對AI既初步介紹到此結束,如果你真係對AI有興趣,記得要有住一分好奇心,同埋面對失敗唔好放棄,因為我淨係compile樓上堆library都失敗過不下50次<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n大家有咩對AI,不論係hardware software 算法定係點樣入手還是哲學野都可以繼續討論下<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" />"},{"pid":"8223049c8d88dba1ec41b4880fac6640ea9699b0","tid":483239,"uid":72909,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-30T15:26:21.000Z","msg":"好勁，留名。<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" />"},{"pid":"9717d999fe834cc8ba255bac08aaf42a8b6bffa2","tid":483239,"uid":70989,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-11-30T15:30:54.000Z","msg":"TensorFlow 1.4 已經直接加入左Keras，用Keras係好事，之後再慢慢上TensorFlow<br />\n<br />\n當然用PyTorch都好好"},{"pid":"a9e33121b833d3dca35444fa3eb020f155da80ae","tid":483239,"uid":12599,"like":1,"dislike":0,"score":1,"citedBy":0,"replyTime":"2017-11-30T15:49:24.000Z","msg":"<blockquote>TensorFlow 1.4 已經直接加入左Keras，用Keras係好事，之後再慢慢上TensorFlow<br />\n<br />\n當然用PyTorch都好好</blockquote><br />\nkeras入門ok,認真學AI就一定要轉走"},{"pid":"762e165106a136172fdf3b36289ec296d6ba595a","tid":483239,"uid":72909,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-30T15:49:29.000Z","msg":"btw，唔太明kernel點define、有咩用。"},{"pid":"f31a81fe31ff18f417377f3969d45921323ac010","tid":483239,"uid":63491,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-30T17:11:56.000Z","msg":"最有興趣諗下ml解決到咩商業問題，可以有咩product idea發展 <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" />"},{"pid":"2bc769eb318e4e56f753324242d7c8eb5f0f9454","tid":483239,"uid":58671,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-11-30T18:19:33.000Z","msg":"最緊玩緊GAN, 呢個貼應該係本地第一個有心用廣東話教既ml post, 大大力支持"},{"pid":"fae3318d0a959617c039cef4a825b2a549a9c18f","tid":483239,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T01:36:59.000Z","msg":"想實戰可以上kaggle"},{"pid":"6bf5814fdb39e3fa85b3458228a594532fbc2395","tid":483239,"uid":19632,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T01:39:14.000Z","msg":"大學讀緊垃圾<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> 支持"},{"pid":"f6bf99cb5b34ded419015274c750f4d0f574d6c1","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T03:11:15.000Z","msg":"<blockquote>btw，唔太明kernel點define、有咩用。</blockquote><br />\n<a href=\"https://devblogs.nvidia.com/parallelforall/easy-introduction-cuda-c-and-c/\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fdevblogs.nvidia.com%2Fparallelforall%2Feasy-introduction-cuda-c-and-c%2F&d=tjzMqdWYAiG4enN%2Fcf3LS6NGKT0mDZcTD8WJHRevFdc%3D&h=6ba2f837\" target=\"_blank\">https://devblogs.nvidia.com/parallelforall/easy-introduction-cuda-c-and-c/</a><br />\n<br />\n呢樣野真係要落場打GPU kernel先解釋到<br />\n<br />\n不論N記定A記都係用kernel function作為GPU運算單元<br />\n<br />\n係C/C++ side要做device memory allocation同memory transfer from host (CPU ram) to device (GPU ram),呢個過程我地叫畫田<br />\n<br />\n畫完田之後我地要話俾GPU知每個田入面既每個小區點樣運作,呢個運作就叫kernel<br />\n<br />\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/59/CUDA_processing_flow_%28En%29.PNG/300px-CUDA_processing_flow_%28En%29.PNG\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fthumb%2F5%2F59%2FCUDA_processing_flow_%2528En%2529.PNG%2F300px-CUDA_processing_flow_%2528En%2529.PNG&h=685bef98&s={SIZE}\" /><br />\n<br />\n呢個kernel既運作同convolution既kernel係極為相似,所以CNN先可以有效地利用GPU提速,nvidia有個特殊library計NN叫cudnn, 呢樣野係特別針對convolution同pooling減少memeory使用/同等memeory提速<br />\n<br />\n對於GPU device code,本質上係C來的,但kernel code要特別地去compile成GPU專用library先可以計到數<br />\n<br />\n呢個亦係點解而家d GPU ram可以爆炸性供加既原因,你一個network拆兩張卡計再合成效率其實唔夠單張卡計咁好,呢樣野alexnet果陣都有講過"},{"pid":"e907cb656494f259d30b4b52e139e25d1f535688","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T03:12:20.000Z","msg":"<blockquote>最有興趣諗下ml解決到咩商業問題，可以有咩product idea發展 <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /></blockquote><br />\nML淨係clustering同SVM都夠解決好多商業問題<br />\n<br />\nproduct你自己諗,我淨係可以講咩area都可以用ML去幫你手"},{"pid":"faddba9736a3286cea101ccac6f6b965ce225b9e","tid":483239,"uid":62610,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T09:39:07.000Z","msg":"做image classification黎講，有時如果training data唔夠<br />\n用SVM個performance會唔會好過Neural Net?<br />\n因為Neural Net咁大堆parameters如果太少data既話會overfit<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /> <br />\n利申: 淨係識好皮毛<img src=\"/assets/faces/fs/chicken.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/fs/chicken.gif\" class=\"hkgmoji\" />"},{"pid":"1204908ffcf7ef1ed25e213e5673f718c1eeb966","tid":483239,"uid":40684,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T10:08:06.000Z","msg":"識好多concept野, 但tensorflow認真難用<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n而家就好似學左好多內功<br />\n冇武功咁<br />\n都唔知做咩春....."},{"pid":"66291396abbebbeeadfdedc29caf09f69abc391e","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T10:22:08.000Z","msg":"<blockquote>做image classification黎講，有時如果training data唔夠<br />\n用SVM個performance會唔會好過Neural Net?<br />\n因為Neural Net咁大堆parameters如果太少data既話會overfit<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /> <br />\n利申: 淨係識好皮毛<img src=\"/assets/faces/fs/chicken.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/fs/chicken.gif\" class=\"hkgmoji\" /></blockquote><br />\n我以前會叫你用SVM,而家建議用淺層但新D的CNN<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"93723720fa3f8ea3c95f8f84bb71711253f51350","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T10:22:30.000Z","msg":"<blockquote>識好多concept野, 但tensorflow認真難用<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n而家就好似學左好多內功<br />\n冇武功咁<br />\n都唔知做咩春.....</blockquote><br />\npytorch<img src=\"/assets/faces/lomoji/42.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/42.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/42.png\" class=\"hkgmoji\" />"},{"pid":"c4da4683cf24260bdbe5fa7bde7178a5bc7f6cb5","tid":483239,"uid":40684,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T10:26:06.000Z","msg":"<blockquote><blockquote>識好多concept野, 但tensorflow認真難用<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n而家就好似學左好多內功<br />\n冇武功咁<br />\n都唔知做咩春.....</blockquote><br />\npytorch<img src=\"/assets/faces/lomoji/42.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/42.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/42.png\" class=\"hkgmoji\" /></blockquote><br />\n感覺tensorflow強大D<br />\n<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n橫掂都係"},{"pid":"6e1237fb14f9977da5f17c0ec4d51cd09638d34a","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T10:58:50.000Z","msg":"<blockquote><blockquote><blockquote>識好多concept野, 但tensorflow認真難用<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n而家就好似學左好多內功<br />\n冇武功咁<br />\n都唔知做咩春.....</blockquote><br />\npytorch<img src=\"/assets/faces/lomoji/42.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/42.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/42.png\" class=\"hkgmoji\" /></blockquote><br />\n感覺tensorflow強大D<br />\n<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n橫掂都係</blockquote><br />\n你玩得掂TF先得架<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n唔好聽到google個名就覺得好勁(雖然真係好勁),但真係elephant in a room<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\npytorch一出就好多人用,因為佢夠簡潔效率又高,development時間都係成本黎架"},{"pid":"611aac2c778ec8331908fa3320e180a4522a0c84","tid":483239,"uid":48679,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T11:09:55.000Z","msg":"有無非image processing 類既deep learning 講解下?<br />\n定係其實都只係當d data 係一幅n dimension 既graph?"},{"pid":"ea2fea838378a6b4d69a3567aee85a1a02b3e2e9","tid":483239,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T11:28:35.000Z","msg":"<blockquote><blockquote><blockquote><blockquote>識好多concept野, 但tensorflow認真難用<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n而家就好似學左好多內功<br />\n冇武功咁<br />\n都唔知做咩春.....</blockquote><br />\npytorch<img src=\"/assets/faces/lomoji/42.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/42.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/42.png\" class=\"hkgmoji\" /></blockquote><br />\n感覺tensorflow強大D<br />\n<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n橫掂都係</blockquote><br />\n你玩得掂TF先得架<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n唔好聽到google個名就覺得好勁(雖然真係好勁),但真係elephant in a room<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\npytorch一出就好多人用,因為佢夠簡潔效率又高,development時間都係成本黎架</blockquote><br />\n單計performance pytorch定tf好？"},{"pid":"e6d446f48b6ac03a51014a24ba2fbb87c9f0cd74","tid":483239,"uid":62610,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T11:43:14.000Z","msg":"<blockquote>有無非image processing 類既deep learning 講解下?<br />\n定係其實都只係當d data 係一幅n dimension 既graph?</blockquote><br />\n我諗應該係倒返轉，image classification本身就係將image睇成一個n-dimensional 既data <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n個model事實上你唔止可以fit個image落去，fit咩都冇問題架喎<br />\n反而我想知好似CNN呢d直接inspired by human vision system既model，係image processing以外有d咩應用<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" />"},{"pid":"520ef19f900c082b969a9e0948ef55b12bfb0898","tid":483239,"uid":70989,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T13:32:09.000Z","msg":"<blockquote>有無非image processing 類既deep learning 講解下?<br />\n定係其實都只係當d data 係一幅n dimension 既graph?</blockquote><br />\nBayesian deep learning<br />\n做probabilistic modeling"},{"pid":"ffe2a9a9e50360d27ac91efd954b2720da19206a","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T14:04:11.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote>識好多concept野, 但tensorflow認真難用<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n而家就好似學左好多內功<br />\n冇武功咁<br />\n都唔知做咩春.....</blockquote><br />\npytorch<img src=\"/assets/faces/lomoji/42.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/42.png\" class=\"hkgmoji\" /> <img src=\"/assets/faces/lomoji/42.png\" class=\"hkgmoji\" /></blockquote><br />\n感覺tensorflow強大D<br />\n<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n橫掂都係</blockquote><br />\n你玩得掂TF先得架<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n唔好聽到google個名就覺得好勁(雖然真係好勁),但真係elephant in a room<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\npytorch一出就好多人用,因為佢夠簡潔效率又高,development時間都係成本黎架</blockquote><br />\n單計performance pytorch定tf好？</blockquote><br />\npytorch"},{"pid":"baa188387a38a70771c4f146f971f598434b50c4","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T14:14:42.000Z","msg":"<blockquote><blockquote>有無非image processing 類既deep learning 講解下?<br />\n定係其實都只係當d data 係一幅n dimension 既graph?</blockquote><br />\n我諗應該係倒返轉，image classification本身就係將image睇成一個n-dimensional 既data <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n個model事實上你唔止可以fit個image落去，fit咩都冇問題架喎<br />\n反而我想知好似CNN呢d直接inspired by human vision system既model，係image processing以外有d咩應用<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\nCNN係專為影像設計的AI<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n其實仲有RNN for 聲音/natural language processing(NLP) 處理, 呢個係語言類的ai<br />\n<br />\n運動都係用RNN訓練<br />\n<br />\n人腦/動物腦強大之處就係集成曬以上所有系統仲有機制互相協調<br />\n<br />\n目前最強嘅AI CNN network只需用550萬個conncection就分到數字，大約同一隻烏蠅嘅腦效能相近，但明顯地烏蠅係唔識認數字<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n用最多connection的CNN而家大約一億五千萬個connection左右，但係呢個network係sequential, 樓上個550萬認字network升階左上n維感知，所以效果比一億個connection仲好好多<img src=\"/assets/faces/normal/yipes.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/yipes.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/yipes.gif\" class=\"hkgmoji\" /> <br />\n<br />\n人腦嘅neuron connection係呢個最先進的CNN 的2億倍左右<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" />"},{"pid":"9ae73f1217556052c8dadcaf56a53b7b120d362f","tid":483239,"uid":98493,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T14:26:25.000Z","msg":"樓主會唔會講LSTM同dropout rate? Time series 一般都會用LSTM"},{"pid":"2993e347b84335ef9c142a48c360ab1fbd7e8435","tid":483239,"uid":113467,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T14:41:49.000Z","msg":"Lm"},{"pid":"7963ccd8d16ecab20c50eac4650f3d2efcd1eaa9","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T14:59:15.000Z","msg":"<blockquote>樓主會唔會講LSTM同dropout rate? Time series 一般都會用LSTM</blockquote><br />\ndropout 新一代network唔會用了，原理係randomly停左某d neuron 唔做update去防止overfitting<br />\n<br />\n去到ResNet 改左用residual，無左dropout機制, 有人話shortcut呢樣野本質上就係0.5機率的dropout<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" />"},{"pid":"bf03475b9af4a235bd643d5f5dfd39049fecb5f6","tid":483239,"uid":55423,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T15:42:11.000Z","msg":"<img src=\"/assets/faces/normal/like.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/like.gif\" class=\"hkgmoji\" />"},{"pid":"067d521ca0011ea2ec525bfaabc4d0718608400c","tid":483239,"uid":63846,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T15:47:37.000Z","msg":"<blockquote><blockquote>樓主會唔會講LSTM同dropout rate? Time series 一般都會用LSTM</blockquote><br />\ndropout 新一代network唔會用了，原理係randomly停左某d neuron 唔做update去防止overfitting<br />\n<br />\n去到ResNet 改左用residual，無左dropout機制, 有人話shortcut呢樣野本質上就係0.5機率的dropout<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\nFc layer都唔dropout<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /> 定係冇FC Layer<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" />"},{"pid":"cb15a8ec9052d3617391d43332a064b3fd359b30","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T16:14:35.000Z","msg":"<blockquote><blockquote><blockquote>樓主會唔會講LSTM同dropout rate? Time series 一般都會用LSTM</blockquote><br />\ndropout 新一代network唔會用了，原理係randomly停左某d neuron 唔做update去防止overfitting<br />\n<br />\n去到ResNet 改左用residual，無左dropout機制, 有人話shortcut呢樣野本質上就係0.5機率的dropout<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\nFc layer都唔dropout<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /> 定係冇FC Layer<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /></blockquote><br />\n<img src=\"https://img.eservice-hk.net/upload/2017/12/02/000609_d0b3244693f743a542dc6a84b8f5ae22.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fimg.eservice-hk.net%2Fupload%2F2017%2F12%2F02%2F000609_d0b3244693f743a542dc6a84b8f5ae22.jpg&h=76b14a75&s={SIZE}\" /><br />\n應該dropout問題仲研究緊，FC layer係last果層做，去到呢個位dropout唔知仲有幾重要<br />\n<br />\n我睇左hinton最新(?)篇capsule net, 好似完全無講dropout,類似行為改左做dynamic routing，呢篇文好重要，出左3星期超多人discuss, 好有可能revolutionize成個CNN體系，我同個fd一邊研究一邊o 曬咀<br />\n<br />\n佢有篇2018年(?)的論文加深左caspule net同改左routing algorithm, 呢兩日得閒d會睇下又有無咩新insight<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" />"},{"pid":"a219428a6c8a3ba09885cdcd7bbd8abb79d3175d","tid":483239,"uid":98493,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-01T16:53:36.000Z","msg":"<blockquote><blockquote><blockquote><blockquote>樓主會唔會講LSTM同dropout rate? Time series 一般都會用LSTM</blockquote><br />\ndropout 新一代network唔會用了，原理係randomly停左某d neuron 唔做update去防止overfitting<br />\n<br />\n去到ResNet 改左用residual，無左dropout機制, 有人話shortcut呢樣野本質上就係0.5機率的dropout<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\nFc layer都唔dropout<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /> 定係冇FC Layer<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /></blockquote><br />\n<img src=\"https://img.eservice-hk.net/upload/2017/12/02/000609_d0b3244693f743a542dc6a84b8f5ae22.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fimg.eservice-hk.net%2Fupload%2F2017%2F12%2F02%2F000609_d0b3244693f743a542dc6a84b8f5ae22.jpg&h=76b14a75&s={SIZE}\" /><br />\n應該dropout問題仲研究緊，FC layer係last果層做，去到呢個位dropout唔知仲有幾重要<br />\n<br />\n我睇左hinton最新(?)篇capsule net, 好似完全無講dropout,類似行為改左做dynamic routing，呢篇文好重要，出左3星期超多人discuss, 好有可能revolutionize成個CNN體系，我同個fd一邊研究一邊o 曬咀<br />\n<br />\n佢有篇2018年(?)的論文加深左caspule net同改左routing algorithm, 呢兩日得閒d會睇下又有無咩新insight<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\n姐係CNN 可以做到time series ?"},{"pid":"4e368df2b6c9de89cac903233b60ae5647b3e091","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-02T00:12:15.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote>樓主會唔會講LSTM同dropout rate? Time series 一般都會用LSTM</blockquote><br />\ndropout 新一代network唔會用了，原理係randomly停左某d neuron 唔做update去防止overfitting<br />\n<br />\n去到ResNet 改左用residual，無左dropout機制, 有人話shortcut呢樣野本質上就係0.5機率的dropout<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\nFc layer都唔dropout<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /> 定係冇FC Layer<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /></blockquote><br />\n<img src=\"https://img.eservice-hk.net/upload/2017/12/02/000609_d0b3244693f743a542dc6a84b8f5ae22.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fimg.eservice-hk.net%2Fupload%2F2017%2F12%2F02%2F000609_d0b3244693f743a542dc6a84b8f5ae22.jpg&h=76b14a75&s={SIZE}\" /><br />\n應該dropout問題仲研究緊，FC layer係last果層做，去到呢個位dropout唔知仲有幾重要<br />\n<br />\n我睇左hinton最新(?)篇capsule net, 好似完全無講dropout,類似行為改左做dynamic routing，呢篇文好重要，出左3星期超多人discuss, 好有可能revolutionize成個CNN體系，我同個fd一邊研究一邊o 曬咀<br />\n<br />\n佢有篇2018年(?)的論文加深左caspule net同改左routing algorithm, 呢兩日得閒d會睇下又有無咩新insight<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\n姐係CNN 可以做到time series ?</blockquote><br />\nCNN做time series classification 係有的，有人拎來做鯨魚聲音分類，亦見有人拎淺層CNN來做dna prediction,但係唔太多人咁做,只係2D array轉返去1D<br />\n<br />\n單純CNN結構應用係algo trading可以拎來搵上升/下降信號，或者估交易信號/regession<br />\n<br />\n反而cnn+rnn hybrid model就理性一d<br />\n<br />\n當係股票market,你有當刻高位，低位，交易量之類，變n 個input x 1D time series的input, 其實就等價於一張圖片，一樣可以做返樓上講的野<br />\n<br />\n用CNN 單做估升跌/regression作用不大，因為time dimension extrapolation 一定效果唔好。有人拎CNN來做signal filter, 過走曬d noise先放入RNN/LSTM, 好似話會準d<br />\n<br />\n做time series 用LSTM為主幹啦，而家做stock prediction好似得60%準確度，對比做lung cancer detection準確度去到99.97%果d強大的classifer, 股票應用真係對AI 無乜貢獻，有d finance professor夠膽死出paper淨係寫用左NN去train唔講個training model係乜野樣，淨係output d kernel俾你睇，然後話自己好準，估都估到係d淺層NN<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" />"},{"pid":"962b11c7f19a23ff2082035f21a7ae4c543cadba","tid":483239,"uid":40684,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-02T04:29:04.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>樓主會唔會講LSTM同dropout rate? Time series 一般都會用LSTM</blockquote><br />\ndropout 新一代network唔會用了，原理係randomly停左某d neuron 唔做update去防止overfitting<br />\n<br />\n去到ResNet 改左用residual，無左dropout機制, 有人話shortcut呢樣野本質上就係0.5機率的dropout<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\nFc layer都唔dropout<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /> 定係冇FC Layer<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /></blockquote><br />\n<img src=\"https://img.eservice-hk.net/upload/2017/12/02/000609_d0b3244693f743a542dc6a84b8f5ae22.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fimg.eservice-hk.net%2Fupload%2F2017%2F12%2F02%2F000609_d0b3244693f743a542dc6a84b8f5ae22.jpg&h=76b14a75&s={SIZE}\" /><br />\n應該dropout問題仲研究緊，FC layer係last果層做，去到呢個位dropout唔知仲有幾重要<br />\n<br />\n我睇左hinton最新(?)篇capsule net, 好似完全無講dropout,類似行為改左做dynamic routing，呢篇文好重要，出左3星期超多人discuss, 好有可能revolutionize成個CNN體系，我同個fd一邊研究一邊o 曬咀<br />\n<br />\n佢有篇2018年(?)的論文加深左caspule net同改左routing algorithm, 呢兩日得閒d會睇下又有無咩新insight<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\n姐係CNN 可以做到time series ?</blockquote><br />\nCNN做time series classification 係有的，有人拎來做鯨魚聲音分類，亦見有人拎淺層CNN來做dna prediction,但係唔太多人咁做,只係2D array轉返去1D<br />\n<br />\n單純CNN結構應用係algo trading可以拎來搵上升/下降信號，或者估交易信號/regession<br />\n<br />\n反而cnn+rnn hybrid model就理性一d<br />\n<br />\n當係股票market,你有當刻高位，低位，交易量之類，變n 個input x 1D time series的input, 其實就等價於一張圖片，一樣可以做返樓上講的野<br />\n<br />\n用CNN 單做估升跌/regression作用不大，因為time dimension extrapolation 一定效果唔好。有人拎CNN來做signal filter, 過走曬d noise先放入RNN/LSTM, 好似話會準d<br />\n<br />\n做time series 用LSTM為主幹啦，而家做stock prediction好似得60%準確度，對比做lung cancer detection準確度去到99.97%果d強大的classifer, 股票應用真係對AI 無乜貢獻，有d finance professor夠膽死出paper淨係寫用左NN去train唔講個training model係乜野樣，淨係output d kernel俾你睇，然後話自己好準，估都估到係d淺層NN<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /></blockquote><br />\n我好似睇過你講果篇paper<br />\n係咪stanford既？"},{"pid":"ae80a7ecaebec34970274507b852a3beff024c9f","tid":483239,"uid":37489,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-02T04:34:03.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>樓主會唔會講LSTM同dropout rate? Time series 一般都會用LSTM</blockquote><br />\ndropout 新一代network唔會用了，原理係randomly停左某d neuron 唔做update去防止overfitting<br />\n<br />\n去到ResNet 改左用residual，無左dropout機制, 有人話shortcut呢樣野本質上就係0.5機率的dropout<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\nFc layer都唔dropout<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /> 定係冇FC Layer<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /></blockquote><br />\n<img src=\"https://img.eservice-hk.net/upload/2017/12/02/000609_d0b3244693f743a542dc6a84b8f5ae22.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fimg.eservice-hk.net%2Fupload%2F2017%2F12%2F02%2F000609_d0b3244693f743a542dc6a84b8f5ae22.jpg&h=76b14a75&s={SIZE}\" /><br />\n應該dropout問題仲研究緊，FC layer係last果層做，去到呢個位dropout唔知仲有幾重要<br />\n<br />\n我睇左hinton最新(?)篇capsule net, 好似完全無講dropout,類似行為改左做dynamic routing，呢篇文好重要，出左3星期超多人discuss, 好有可能revolutionize成個CNN體系，我同個fd一邊研究一邊o 曬咀<br />\n<br />\n佢有篇2018年(?)的論文加深左caspule net同改左routing algorithm, 呢兩日得閒d會睇下又有無咩新insight<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\n姐係CNN 可以做到time series ?</blockquote><br />\nCNN做time series classification 係有的，有人拎來做鯨魚聲音分類，亦見有人拎淺層CNN來做dna prediction,但係唔太多人咁做,只係2D array轉返去1D<br />\n<br />\n單純CNN結構應用係algo trading可以拎來搵上升/下降信號，或者估交易信號/regession<br />\n<br />\n反而cnn+rnn hybrid model就理性一d<br />\n<br />\n當係股票market,你有當刻高位，低位，交易量之類，變n 個input x 1D time series的input, 其實就等價於一張圖片，一樣可以做返樓上講的野<br />\n<br />\n用CNN 單做估升跌/regression作用不大，因為time dimension extrapolation 一定效果唔好。有人拎CNN來做signal filter, 過走曬d noise先放入RNN/LSTM, 好似話會準d<br />\n<br />\n做time series 用LSTM為主幹啦，而家做stock prediction好似得60%準確度，對比做lung cancer detection準確度去到99.97%果d強大的classifer, 股票應用真係對AI 無乜貢獻，有d finance professor夠膽死出paper淨係寫用左NN去train唔講個training model係乜野樣，淨係output d kernel俾你睇，然後話自己好準，估都估到係d淺層NN<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /></blockquote><br />\nGenomic多數d人想知mechanism多 所以少用nn"},{"pid":"4c5199c4638eebcafdbda590a516332cfe618def","tid":483239,"uid":112769,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-02T04:39:57.000Z","msg":"<blockquote><blockquote>咁岩我畢業論文寫 論機械人的道德考量<br />\n巴打有冇reference?<img src=\"/assets/faces/big/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/big/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/big/adore.gif\" class=\"hkgmoji\" /></blockquote><br />\nPhilo朋友推薦,唔知岩唔岩你睇<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\n<a href=\"https://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwirmKL27eLXAhVHjLwKHTVaANcQFgglMAA&amp;url=http%3A%2F%2Fmichaeljohnsonphilosophy.com%2Fwp-content%2Fuploads%2F2012%2F11%2FMechanical-Mind.pdf&amp;usg=AOvVaw08fsReaqnKcP_2a4Axkcvr\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Fwww.google.com.hk%2Furl%3Fsa%3Dt%26rct%3Dj%26q%3D%26esrc%3Ds%26source%3Dweb%26cd%3D1%26cad%3Drja%26uact%3D8%26ved%3D0ahUKEwirmKL27eLXAhVHjLwKHTVaANcQFgglMAA%26url%3Dhttp%253A%252F%252Fmichaeljohnsonphilosophy.com%252Fwp-content%252Fuploads%252F2012%252F11%252FMechanical-Mind.pdf%26usg%3DAOvVaw08fsReaqnKcP_2a4Axkcvr&d=tjzMqdWYAiG4enN%2Fcf3LS6NGKT0mDZcTD8WJHRevFdc%3D&h=69e466b4\" target=\"_blank\">https://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwirmKL27eLXAhVHjLwKHTVaANcQFgglMAA&amp;url=http%3A%2F%2Fmichaeljohnsonphilosophy.com%2Fwp-content%2Fuploads%2F2012%2F11%2FMechanical-Mind.pdf&amp;usg=AOvVaw08fsReaqnKcP_2a4Axkcvr</a></blockquote><br />\n<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> 感謝主"},{"pid":"314c033e4dd4d0b407b9a9ecd70949754be8f057","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-02T04:58:58.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>樓主會唔會講LSTM同dropout rate? Time series 一般都會用LSTM</blockquote><br />\ndropout 新一代network唔會用了，原理係randomly停左某d neuron 唔做update去防止overfitting<br />\n<br />\n去到ResNet 改左用residual，無左dropout機制, 有人話shortcut呢樣野本質上就係0.5機率的dropout<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\nFc layer都唔dropout<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /> 定係冇FC Layer<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /></blockquote><br />\n<img src=\"https://img.eservice-hk.net/upload/2017/12/02/000609_d0b3244693f743a542dc6a84b8f5ae22.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fimg.eservice-hk.net%2Fupload%2F2017%2F12%2F02%2F000609_d0b3244693f743a542dc6a84b8f5ae22.jpg&h=76b14a75&s={SIZE}\" /><br />\n應該dropout問題仲研究緊，FC layer係last果層做，去到呢個位dropout唔知仲有幾重要<br />\n<br />\n我睇左hinton最新(?)篇capsule net, 好似完全無講dropout,類似行為改左做dynamic routing，呢篇文好重要，出左3星期超多人discuss, 好有可能revolutionize成個CNN體系，我同個fd一邊研究一邊o 曬咀<br />\n<br />\n佢有篇2018年(?)的論文加深左caspule net同改左routing algorithm, 呢兩日得閒d會睇下又有無咩新insight<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\n姐係CNN 可以做到time series ?</blockquote><br />\nCNN做time series classification 係有的，有人拎來做鯨魚聲音分類，亦見有人拎淺層CNN來做dna prediction,但係唔太多人咁做,只係2D array轉返去1D<br />\n<br />\n單純CNN結構應用係algo trading可以拎來搵上升/下降信號，或者估交易信號/regession<br />\n<br />\n反而cnn+rnn hybrid model就理性一d<br />\n<br />\n當係股票market,你有當刻高位，低位，交易量之類，變n 個input x 1D time series的input, 其實就等價於一張圖片，一樣可以做返樓上講的野<br />\n<br />\n用CNN 單做估升跌/regression作用不大，因為time dimension extrapolation 一定效果唔好。有人拎CNN來做signal filter, 過走曬d noise先放入RNN/LSTM, 好似話會準d<br />\n<br />\n做time series 用LSTM為主幹啦，而家做stock prediction好似得60%準確度，對比做lung cancer detection準確度去到99.97%果d強大的classifer, 股票應用真係對AI 無乜貢獻，有d finance professor夠膽死出paper淨係寫用左NN去train唔講個training model係乜野樣，淨係output d kernel俾你睇，然後話自己好準，估都估到係d淺層NN<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /></blockquote><br />\n我好似睇過你講果篇paper<br />\n係咪stanford既？</blockquote><br />\n係standford果篇<br />\n<br />\n似係undergrad既文來,CS人寫<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" />"},{"pid":"3f7829f1c287232186f69424ee4c5625a6effa9b","tid":483239,"uid":93084,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-02T05:19:45.000Z","msg":"堅係要留一留名<img src=\"/assets/faces/normal/tongue.gif\" class=\"hkgmoji\" />"},{"pid":"821c11f3805725b44931e7f146f7358065a20a59","tid":483239,"uid":40684,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-02T05:32:29.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><br />\ndropout 新一代network唔會用了，原理係randomly停左某d neuron 唔做update去防止overfitting<br />\n<br />\n去到ResNet 改左用residual，無左dropout機制, 有人話shortcut呢樣野本質上就係0.5機率的dropout<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\nFc layer都唔dropout<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /> 定係冇FC Layer<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /></blockquote><br />\n<img src=\"https://img.eservice-hk.net/upload/2017/12/02/000609_d0b3244693f743a542dc6a84b8f5ae22.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fimg.eservice-hk.net%2Fupload%2F2017%2F12%2F02%2F000609_d0b3244693f743a542dc6a84b8f5ae22.jpg&h=76b14a75&s={SIZE}\" /><br />\n應該dropout問題仲研究緊，FC layer係last果層做，去到呢個位dropout唔知仲有幾重要<br />\n<br />\n我睇左hinton最新(?)篇capsule net, 好似完全無講dropout,類似行為改左做dynamic routing，呢篇文好重要，出左3星期超多人discuss, 好有可能revolutionize成個CNN體系，我同個fd一邊研究一邊o 曬咀<br />\n<br />\n佢有篇2018年(?)的論文加深左caspule net同改左routing algorithm, 呢兩日得閒d會睇下又有無咩新insight<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\n姐係CNN 可以做到time series ?</blockquote><br />\nCNN做time series classification 係有的，有人拎來做鯨魚聲音分類，亦見有人拎淺層CNN來做dna prediction,但係唔太多人咁做,只係2D array轉返去1D<br />\n<br />\n單純CNN結構應用係algo trading可以拎來搵上升/下降信號，或者估交易信號/regession<br />\n<br />\n反而cnn+rnn hybrid model就理性一d<br />\n<br />\n當係股票market,你有當刻高位，低位，交易量之類，變n 個input x 1D time series的input, 其實就等價於一張圖片，一樣可以做返樓上講的野<br />\n<br />\n用CNN 單做估升跌/regression作用不大，因為time dimension extrapolation 一定效果唔好。有人拎CNN來做signal filter, 過走曬d noise先放入RNN/LSTM, 好似話會準d<br />\n<br />\n做time series 用LSTM為主幹啦，而家做stock prediction好似得60%準確度，對比做lung cancer detection準確度去到99.97%果d強大的classifer, 股票應用真係對AI 無乜貢獻，有d finance professor夠膽死出paper淨係寫用左NN去train唔講個training model係乜野樣，淨係output d kernel俾你睇，然後話自己好準，估都估到係d淺層NN<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /></blockquote><br />\n我好似睇過你講果篇paper<br />\n係咪stanford既？</blockquote><br />\n係standford果篇<br />\n<br />\n似係undergrad既文來,CS人寫<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\n篇文打哂J<br />\n實際點做又冇咩點講<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"f99d79fcc7f380d6338efa92f0046f6d1f303f38","tid":483239,"uid":37722,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-02T06:51:02.000Z","msg":"最近學緊cnn, 發現有篇文講到fcn, 全部只用convolutional layer, 做image segmentation更準, 比cnn compute更快, 請問樓主點睇？"},{"pid":"80d088f5b0795be4f89816a197102fdb684a7b31","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-02T07:32:01.000Z","msg":"<blockquote>最近學緊cnn, 發現有篇文講到fcn, 全部只用convolutional layer, 做image segmentation更準, 比cnn compute更快, 請問樓主點睇？</blockquote><br />\nresnet係fcnn的一種(唔計最後一層FC layer做classification)<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\nfcnn之前都覺得幾ok, 但hinton最新個capsule network一出，樓上堆野都好似好小兒科<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"49c5e646c2cf6a2c969fa1abd0a53f571b55c016","tid":483239,"uid":98493,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-02T20:18:11.000Z","msg":"<blockquote><blockquote><blockquote><blockquote><blockquote><blockquote>樓主會唔會講LSTM同dropout rate? Time series 一般都會用LSTM</blockquote><br />\ndropout 新一代network唔會用了，原理係randomly停左某d neuron 唔做update去防止overfitting<br />\n<br />\n去到ResNet 改左用residual，無左dropout機制, 有人話shortcut呢樣野本質上就係0.5機率的dropout<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\nFc layer都唔dropout<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /> 定係冇FC Layer<img src=\"/assets/faces/normal/wonder2.gif\" class=\"hkgmoji\" /></blockquote><br />\n<img src=\"https://img.eservice-hk.net/upload/2017/12/02/000609_d0b3244693f743a542dc6a84b8f5ae22.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fimg.eservice-hk.net%2Fupload%2F2017%2F12%2F02%2F000609_d0b3244693f743a542dc6a84b8f5ae22.jpg&h=76b14a75&s={SIZE}\" /><br />\n應該dropout問題仲研究緊，FC layer係last果層做，去到呢個位dropout唔知仲有幾重要<br />\n<br />\n我睇左hinton最新(?)篇capsule net, 好似完全無講dropout,類似行為改左做dynamic routing，呢篇文好重要，出左3星期超多人discuss, 好有可能revolutionize成個CNN體系，我同個fd一邊研究一邊o 曬咀<br />\n<br />\n佢有篇2018年(?)的論文加深左caspule net同改左routing algorithm, 呢兩日得閒d會睇下又有無咩新insight<img src=\"/assets/faces/normal/banghead.gif\" class=\"hkgmoji\" /></blockquote><br />\n姐係CNN 可以做到time series ?</blockquote><br />\nCNN做time series classification 係有的，有人拎來做鯨魚聲音分類，亦見有人拎淺層CNN來做dna prediction,但係唔太多人咁做,只係2D array轉返去1D<br />\n<br />\n單純CNN結構應用係algo trading可以拎來搵上升/下降信號，或者估交易信號/regession<br />\n<br />\n反而cnn+rnn hybrid model就理性一d<br />\n<br />\n當係股票market,你有當刻高位，低位，交易量之類，變n 個input x 1D time series的input, 其實就等價於一張圖片，一樣可以做返樓上講的野<br />\n<br />\n用CNN 單做估升跌/regression作用不大，因為time dimension extrapolation 一定效果唔好。有人拎CNN來做signal filter, 過走曬d noise先放入RNN/LSTM, 好似話會準d<br />\n<br />\n做time series 用LSTM為主幹啦，而家做stock prediction好似得60%準確度，對比做lung cancer detection準確度去到99.97%果d強大的classifer, 股票應用真係對AI 無乜貢獻，有d finance professor夠膽死出paper淨係寫用左NN去train唔講個training model係乜野樣，淨係output d kernel俾你睇，然後話自己好準，估都估到係d淺層NN<img src=\"/assets/faces/normal/clown.gif\" class=\"hkgmoji\" /></blockquote><br />\n我就係用lstm 做prediction ,都想睇下可以點樣做好dd"},{"pid":"39f56937f487a019967968b2109ecae02b0eaa16","tid":483239,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-04T03:59:30.000Z","msg":"<a href=\"https://t.me/HKAIG\" data-sr-url=\"https://r.lihkg.com/link?u=https%3A%2F%2Ft.me%2FHKAIG&d=tjzMqdWYAiG4enN%2Fcf3LS6NGKT0mDZcTD8WJHRevFdc%3D&h=3fdac1d2\" target=\"_blank\">https://t.me/HKAIG</a><br />\n<br />\n如果有人想識多d香港玩緊AI既朋友,有個tg group可以俾大家一齊再深入share下<img src=\"/assets/faces/normal/chicken.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/chicken.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/chicken.gif\" class=\"hkgmoji\" />"},{"pid":"edd71ba610ecd11ad0ca19f10abdc2dd1259e635","tid":483239,"uid":37869,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-12-04T13:54:54.000Z","msg":"<img src=\"/assets/faces/normal/smile.gif\" class=\"hkgmoji\" />"}]}