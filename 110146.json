{"tid":110146,"cid":26,"subCid":0,"title":"Machine Learning 自學日記","createTime":"2017-02-06T13:05:48.000Z","updateTime":"2017-03-29T16:54:38.000Z","uid":53524,"like":8,"dislike":0,"uniUserReply":0,"replies":[{"pid":"a54054c125a56c655db1e8f3100ffdac8d290f4b","tid":110146,"uid":53524,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-06T13:05:48.000Z","msg":"[06/02]<br />\n<br />\nSupport Vector Machines : 畫條Hyperplance出黎做Binary Classification<br />\n<br />\n今日講住咁多先<br />\n<br />\n新手, 岩岩學 夠prerequisite 都要睇好耐先明<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" />"},{"pid":"01fe4d72a74254a463985b5f0df2b38f99329f3a","tid":110146,"uid":24006,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-06T13:25:18.000Z","msg":"Lm"},{"pid":"71a1b7a0402ac7f8cce6934e45a74840491216d7","tid":110146,"uid":52024,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-06T14:00:33.000Z","msg":"巴打<img src=\"/assets/faces/normal/bye.gif\" class=\"hkgmoji\" /> <br />\n我都自學緊, 依家先睇到Multivariate Linear Regression<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"a838bcf149d9cf3bb33f4512b400af505d7e8e19","tid":110146,"uid":53524,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-06T14:24:39.000Z","msg":"<blockquote>巴打<img src=\"/assets/faces/normal/bye.gif\" class=\"hkgmoji\" /> <br />\n我都自學緊, 依家先睇到Multivariate Linear Regression<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /></blockquote><br />\n呢個stat黎俾我就skip啦<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"5e718744a46927ed35081d7130a510613d54c4ab","tid":110146,"uid":33883,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-06T14:38:30.000Z","msg":"果陣係U學Stat最深都只係去到玩下Two-Way ANOVA, 對ML嚟講, 呢D好似只係幼兒級數, 所以我覺得呢世都冇本事學識/好ML架喇, 唉! <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\nAnyway, 祝你學ML冇乜阻滯, 一帆風順! <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" />"},{"pid":"8ec2ba4a705561dde2cf50210d1e4b8938f15758","tid":110146,"uid":53524,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-06T14:47:20.000Z","msg":"<blockquote>果陣係U學Stat最深都只係去到玩下Two-Way ANOVA, 對ML嚟講, 呢D好似只係幼兒級數, 所以我覺得呢世都冇本事學識/好ML架喇, 唉! <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\nAnyway, 祝你學ML冇乜阻滯, 一帆風順! <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /></blockquote><br />\n其實唔深, 傻瓜版用Program 就搞掂<br />\n如果好似我試過咁用手計TWO-WAY ANOVA就真係ON9<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\nML我覺得只係電腦版既STAT<br />\n好似STAT咁搬上電腦就好易<img src=\"/assets/faces/normal/like.gif\" class=\"hkgmoji\" /> <br />\n依家ML既PLATFORM已經好成熟, 真係好似EXCEL咁用<br />\n<br />\n<img src=\"https://pythonprogramming.net/static/images/svm/machineLearning.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fpythonprogramming.net%2Fstatic%2Fimages%2Fsvm%2FmachineLearning.png&h=0a65bf0b&s={SIZE}\" />"},{"pid":"280617d46304e86d17c9c506b8b89dc05409d6df","tid":110146,"uid":17402,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-06T14:51:15.000Z","msg":"<blockquote><blockquote>果陣係U學Stat最深都只係去到玩下Two-Way ANOVA, 對ML嚟講, 呢D好似只係幼兒級數, 所以我覺得呢世都冇本事學識/好ML架喇, 唉! <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\nAnyway, 祝你學ML冇乜阻滯, 一帆風順! <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /></blockquote><br />\n其實唔深, 傻瓜版用Program 就搞掂<br />\n如果好似我試過咁用手計TWO-WAY ANOVA就真係ON9<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\nML我覺得只係電腦版既STAT<br />\n好似STAT咁搬上電腦就好易<img src=\"/assets/faces/normal/like.gif\" class=\"hkgmoji\" /> <br />\n依家ML既PLATFORM已經好成熟, 真係好似EXCEL咁用<br />\n<br />\n<img src=\"https://pythonprogramming.net/static/images/svm/machineLearning.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fpythonprogramming.net%2Fstatic%2Fimages%2Fsvm%2FmachineLearning.png&h=0a65bf0b&s={SIZE}\" /></blockquote><br />\nPython 一個package 就搞店哂"},{"pid":"aadb16d2bca102119f7e75bf0ea083b160bb7eba","tid":110146,"uid":35216,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-06T17:26:30.000Z","msg":"巴打我都係由呢幾日開始學起<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n新手黎講裝個scikitlearn真係好萬能<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" />"},{"pid":"49a18b6a65d0a95bbe9e3692f87c4fc1a5c37f22","tid":110146,"uid":53524,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-07T14:49:08.000Z","msg":"[07/02] SVM (2)<br />\n1. Maximal Margin classifier<br />\n2. Support vector classifier (1+Slack Variable)<br />\n3. Support vector machine (2+Kernel Trick)<br />\n<br />\nA. How to find the separating hyperplane?<br />\nConstruct 條function 如下:<br />\n搵所有support vector, which is the minimal distance to the hyperplane.<br />\n然後再Maximize 所有Support Vector 既distance. (多維版Least Square Solution?)<br />\n即係將條分隔線放得靚仔D <br />\n<br />\n<img src=\"http://docs.opencv.org/2.4/_images/optimal-hyperplane.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fdocs.opencv.org%2F2.4%2F_images%2Foptimal-hyperplane.png&h=969496df&s={SIZE}\" /><br />\n<br />\nBy practice, 用Lagrange multiplier將條formula 轉換, 用constrained optimization: quadratic solver, SMO [Platt, 1999]搵條separating hyperplane 既function parameter<br />\n<br />\nB. How about non-separable case (or overfitting)?<br />\n加slack variable, allow some outliners on violate the margin and hyperplane, make the margin classifier more robust<br />\nwhen slack variable(i) = 0, 呢個observation放岩位<br />\n<br />\nC. Support Vector Machine<br />\n係Support Vector Classifier 加入Kernel Trick, 用inner product 黎做classifier, map 去higher dimension space, 將observation 同hyperplane 既間隔最大化, 最常用radial kernel function(好似係用Gaussian distribution)"},{"pid":"8213d3449bd963c8777cf8ff6ea34938459fe7f7","tid":110146,"uid":12599,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-07T17:45:02.000Z","msg":"<blockquote><blockquote>果陣係U學Stat最深都只係去到玩下Two-Way ANOVA, 對ML嚟講, 呢D好似只係幼兒級數, 所以我覺得呢世都冇本事學識/好ML架喇, 唉! <img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\nAnyway, 祝你學ML冇乜阻滯, 一帆風順! <img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /></blockquote><br />\n其實唔深, 傻瓜版用Program 就搞掂<br />\n如果好似我試過咁用手計TWO-WAY ANOVA就真係ON9<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /> <br />\n<br />\nML我覺得只係電腦版既STAT<br />\n好似STAT咁搬上電腦就好易<img src=\"/assets/faces/normal/like.gif\" class=\"hkgmoji\" /> <br />\n依家ML既PLATFORM已經好成熟, 真係好似EXCEL咁用<br />\n<br />\n<img src=\"https://pythonprogramming.net/static/images/svm/machineLearning.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fpythonprogramming.net%2Fstatic%2Fimages%2Fsvm%2FmachineLearning.png&h=0a65bf0b&s={SIZE}\" /></blockquote><br />\n屌SVM<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" /><br />\n<br />\n以為你講CNN<img src=\"/assets/faces/normal/sosad.gif\" class=\"hkgmoji\" />"},{"pid":"66a080f02002cdb94c42a8fac6c61a72260bd5ef","tid":110146,"uid":53524,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-08T15:34:51.000Z","msg":"[08/02] Logistic Regression<br />\n- Binary Classification<br />\n<br />\n<img src=\"https://www.biomedware.com/files/documentation/spacestat/Statistics/Multivariate_Modeling/Regression/logistic_gen2.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=https%3A%2F%2Fwww.biomedware.com%2Ffiles%2Fdocumentation%2Fspacestat%2FStatistics%2FMultivariate_Modeling%2FRegression%2Flogistic_gen2.jpg&h=8d7cbf33&s={SIZE}\" /><br />\n<br />\n只會output y^{i} = {0,1}<br />\ninput &gt;.5 出1, vice versa<br />\n<br />\nA. 同Linear Regression 分別<br />\nLinear Regression 用 linear equation<br />\nLogistic Regression 用 sigmoid function<br />\n<br />\n1/e^beta,x<br />\n<br />\n要minimize 所有training set 既 Total Sum of Error<br />\n首先要將objective function 用MLE 轉做log likelihood function<br />\n然後用optimization method 搵出最好既beta (quasi-newton, sgd)<br />\n<br />\n搵完beta就得出model, 完成"},{"pid":"12b01338a1f9ed0daecd46457516e0455c1c4359","tid":110146,"uid":53524,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-09T14:18:28.000Z","msg":"[09/02] Regularization<br />\n<br />\n<img src=\"http://docplayer.net/docs-images/40/14087928/images/page_8.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fdocplayer.net%2Fdocs-images%2F40%2F14087928%2Fimages%2Fpage_8.jpg&h=bf78443b&s={SIZE}\" /><br />\n<br />\nWhat is regularization?<br />\nAssign shrinkage penalty to resolve overfitting.<br />\n<br />\nTypes<br />\nRidge regression &ndash; L2 norms<br />\nLasso &ndash; L1 norms<br />\nElastic net &ndash; Combination of both<br />\n<br />\nEffect on &lambda;<br />\nWhen &lambda;&rarr;&infin; too generalize and cause under fitting<br />\nWhen &lambda;&rarr;0 no penalty<br />\n<br />\nHow to tune &lambda;? <br />\nUse cross-validation to assess the variance and the bias.<br />\n<br />\nProblem of Ridge regression<br />\nWhen the penalty is large, all coefficient in the model will be shrink to zero.<br />\n<br />\nFeature of Lasso<br />\nPerform variable selection, where it yields sparse model. Only shrink a const value, whereas ridge regression do shrinkage proportionally to all coefficients.<br />\n<br />\nApplication<br />\nLinear regression, Logistic regression, neural network"},{"pid":"2c4b8b9701cc19ec894cb4824f48d7b861fb4338","tid":110146,"uid":43649,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-09T14:54:44.000Z","msg":"巴打加油，我都自學緊<img src=\"/assets/faces/normal/hoho.gif\" class=\"hkgmoji\" />"},{"pid":"5442448eb96a75160b2ccdd38c1c39cb9d7e7cf0","tid":110146,"uid":53524,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-09T15:58:02.000Z","msg":"<blockquote>巴打加油，我都自學緊<img src=\"/assets/faces/normal/hoho.gif\" class=\"hkgmoji\" /></blockquote><br />\n<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /> <img src=\"/assets/faces/normal/hoho.gif\" class=\"hkgmoji\" /> 加油"},{"pid":"eccffc615d96d2c0c1a0e52f3ee88d2480d56c2a","tid":110146,"uid":53524,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-10T14:56:09.000Z","msg":"[10/02] MCMC Method<br />\n<br />\nConstruct a Markov chain with desired stationary distribution.<br />\n<br />\n<img src=\"http://images.slideplayer.com/28/9361524/slides/slide_45.jpg\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fimages.slideplayer.com%2F28%2F9361524%2Fslides%2Fslide_45.jpg&h=190087f8&s={SIZE}\" /><br />\n<img src=\"http://marple.eeb.uconn.edu/mcmcrobot/wp-content/uploads/2012/09/robotsrules1.png\" data-thumbnail-src=\"https://i.lih.kg/thumbnail?u=http%3A%2F%2Fmarple.eeb.uconn.edu%2Fmcmcrobot%2Fwp-content%2Fuploads%2F2012%2F09%2Frobotsrules1.png&h=0963265a&s={SIZE}\" /><br />\n<br />\nMajor types<br />\n\tMetropolis-Hasting<br />\n\tGibbs Sampling<br />\n<br />\n\tMetropolis-Hasting<br />\n# must be irreducible <br />\nGiven a state, propose a new state and accept it with a defined probability (自己SET). It is about an idea of propose and accept on the new state. <br />\n<br />\nWhat affect a new state to be accepted?<br />\nIt depends on the transition probability to new state, and the acceptance criterion.<br />\n<br />\n\tGibbs Sampling<br />\nFor n-dimensional states, sample from conditional distribution with one variable each time. N-dimension stationary distribution = representation of joint PMF (應該無錯)<br />\n<br />\n\tSystematic Scan: Scan and set the state from S_n^1 to S_(n+1)^k  of component from component 1 to k, where K{1&hellip; k} is the component of joint distribution.<br />\n\tRandom Scan: Component update in new state are picked in random order<br />\n<br />\n\tComparison<br />\nMetropolis-Hasting require a good choice of proposed stationary distribution, If acceptance rate is high, it converges too quick and cannot result a good solution, vice versa (i.e. 可參考Simulated Annealing).<br />\nGibbs is a simpler method which always accept a new state. It relies on conditional distribution.<br />\n<br />\n\tSame as Monte Carlo, an approximation algorithm for Markov Chain."},{"pid":"b08d16d3cc81964e5616237cfcff060a4333b38d","tid":110146,"uid":52024,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-12T15:24:43.000Z","msg":"巴打, 你係邊度學, 好似講得咁深入既<img src=\"/assets/faces/normal/adore.gif\" class=\"hkgmoji\" /> <br />\n我睇andrew ng, 佢冇講咁多maths野<img src=\"/assets/faces/normal/frown.gif\" class=\"hkgmoji\" /> , 有啲野唔係幾明"},{"pid":"65d49b9ef5ddc4cfe3831410ec43fc411cee5461","tid":110146,"uid":13879,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-13T03:17:02.000Z","msg":"留名"},{"pid":"1ab575e503ae15c74cca0bd4c13f802fbe8005ed","tid":110146,"uid":85524,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-17T14:57:13.000Z","msg":"其實用scikit-learn同tensorflow<br />\n係咪對數學要求大減好多？<br />\n如果新手要點開始？ 讀數先定用python狗衝左先？<br />\n<br />\n預備fyp以machine learning為題。<br />\n我見youtube search &quot;sentdex&quot; 佢教ml教得幾好咁。"},{"pid":"5318c997fa59efcf202c1fc57cb9e57b0d6cfbff","tid":110146,"uid":65859,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-19T15:04:40.000Z","msg":"巴打，我都有用 SVM 做 2 class classification　<img src=\"/assets/faces/normal/angel.gif\" class=\"hkgmoji\" /> <br />\n你知唔知點樣可以visualize 條 hyperplane 出黎，事關我用黎train 既 feature vectors 個dimensionality 都好大下，用linear kernal，想知佢到底係點樣界條 hyperplane 出黎<img src=\"/assets/faces/normal/kill.gif\" class=\"hkgmoji\" />"},{"pid":"8bb2d79e8d93684a7819643e72c0d18454f925bb","tid":110146,"uid":35216,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-19T15:08:26.000Z","msg":"<blockquote>其實用scikit-learn同tensorflow<br />\n係咪對數學要求大減好多？<br />\n如果新手要點開始？ 讀數先定用python狗衝左先？<br />\n<br />\n預備fyp以machine learning為題。<br />\n我見youtube search &quot;sentdex&quot; 佢教ml教得幾好咁。</blockquote><br />\n新手表示why not both<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /><br />\n數呢d用到先學<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /><br />\nbtw如果你入岩位 sklearn可以幾乎有理無理照用"},{"pid":"686781a4a39683057f618cc7ed200ee797989889","tid":110146,"uid":85524,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-19T17:04:54.000Z","msg":"<blockquote><blockquote>其實用scikit-learn同tensorflow<br />\n係咪對數學要求大減好多？<br />\n如果新手要點開始？ 讀數先定用python狗衝左先？<br />\n<br />\n預備fyp以machine learning為題。<br />\n我見youtube search &quot;sentdex&quot; 佢教ml教得幾好咁。</blockquote><br />\n新手表示why not both<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /><br />\n數呢d用到先學<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /><br />\nbtw如果你入岩位 sklearn可以幾乎有理無理照用</blockquote><br />\n<br />\n<img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/good.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/hehe.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/hehe.gif\" class=\"hkgmoji\" /><br />\n如果要將ml放入web到<br />\n例如好似airbnb難唔難搞？<br />\n<img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" /><img src=\"/assets/faces/normal/agree.gif\" class=\"hkgmoji\" />"},{"pid":"4af3124ec592a77a8c4c4b03aadbfa325fd66417","tid":110146,"uid":40250,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-19T17:17:56.000Z","msg":"無program底<br />\n幾年前d c++都無晒記憶<br />\n睇完andrew ng d教學<br />\n去到寫multivariable linear regression已經stuck住<br />\n<br />\n有無巴打可以補下習？ 我唔介意出錢學下野"},{"pid":"bbecf2c2bddc00e9c1aacef85a00b4329e9acb89","tid":110146,"uid":35792,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-19T17:42:00.000Z","msg":"LM"},{"pid":"5ed95f7bc7af89acb96ff122e1d71baa470a1ca2","tid":110146,"uid":65833,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-28T16:46:12.000Z","msg":"pish"},{"pid":"65465b660d60cd31963ef9af58c3ee2c497992cb","tid":110146,"uid":17021,"like":0,"dislike":0,"score":0,"citedBy":0,"replyTime":"2017-02-28T17:20:45.000Z","msg":"冇人用Azure ML or R?"}]}